% !TeX spellcheck = en_UK
% \let\accentvec\vec              
\documentclass[runningheads,11pt]{llncs}
\let\spvec\vec
\let\vec\accentvec

\newcommand\hmmax{0}
\newcommand\bmmax{0}

\DeclareFontFamily{U}{mathx}{\hyphenchar\font45}
\DeclareFontShape{U}{mathx}{m}{n}{<-> mathx10}{}
\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}
\DeclareMathAccent{\widebar}{0}{mathx}{"73}

\let\spvec\vec
\usepackage{amssymb,amsmath}
\let\vec\spvec
%\usepackage{newtxmath,newtxtext}
\usepackage[T1]{fontenc}

  \def\vec#1{\mathchoice{\mbox{\boldmath$\displaystyle#1$}}
  {\mbox{\boldmath$\textstyle#1$}} {\mbox{\boldmath$\scriptstyle#1$}}
  {\mbox{\boldmath$\scriptscriptstyle#1$}}}

% lncs size (as printed in books, with small margins):
 % \usepackage[paperheight=23.5cm,paperwidth=15.5cm,text={13.2cm,20.3cm},centering]{geometry}
 %\usepackage{fullpage}
\usepackage{soulutf8} \soulregister\cite7 \soulregister\ref7
\soulregister\pageref7
\usepackage{hyperref}
\usepackage[color=yellow]{todonotes} \hypersetup{final}
\usepackage{mathrsfs}
\usepackage[advantage,asymptotics,adversary,sets,keys,ff,lambda,primitives,events,operators,probability,logic,mm,complexity]{cryptocode}

\usepackage[capitalise]{cleveref}
%\crefname{appendix}{Supp.~Mat.}{Sup.~Mat.}
%\Crefname{appendix}{Supp.~Mat.}{Sup.~Mat.}
\usepackage{cite} 
\usepackage{booktabs}
\usepackage{paralist}
\usepackage[innerleftmargin=5pt,innerrightmargin=5pt]{mdframed}
\usepackage{caption}
\captionsetup{belowskip=0pt}
\usepackage{bm}
\usepackage{url}
\usepackage{dirtytalk}
\usepackage[margin=0.7in,a4paper]{geometry}
\usepackage[normalem]{ulem}
\usepackage{dashbox}
\newcommand\dboxed[1]{\dbox{\ensuremath{#1}}}
\include{macros}

%\title{On Simulation-Extractability of Universal zkSNARKs}
\title{Non-Malleability of the Fiat--Shamir Transform Revisited}
%\author{Anonymous submission to PKC}
\author{Markulf Kohlweiss\inst{1,2} \and Michał Zając\inst{3}} 
%\iflncs{
\institute{University of Edinburgh, Edinburgh, UK \and IOHK \\
\email{mkohlwei@inf.ed.ac.uk} \and Clearmatics, London, UK \\
\email{m.p.zajac@gmail.com}} 

\allowdisplaybreaks

\begin{document} \sloppy \maketitle

\begin{abstract}
  Faust, Kohlweiss, Marson, and Venturi (INDOCRYPT 2012) showed that  non-interactive zero knowledge (NIZK) proof systems obtained by applying the
  Fiat--Shamir transformation to a public-coin sigma protocol are
  simulation sound and simulation extractable under lenient conditions. In this
  paper, we extend this work and formally define
  simulation extractability for protocols in the random oracle model (ROM) which also use a structured reference string (SRS). Furthermore, we show that NIZK
  proof systems obtained by applying the Fiat--Shamir transformation to a
  public-coin multi-round interactive protocol with SRS is
  simulation-extractable under lenient conditions. A consequence of our result
  is that, in the ROM, we obtain non-malleable NIZKs essentially for free from a
  much wider class of protocols than Faust et al. Importantly, we show that two
  popular zero knowledge SNARKs --- \plonk{}~\cite{EPRINT:GabWilCio19} and
  \sonic{}~\cite{CCS:MBKM19} --- are simulation extractable out-of-the-box.
\end{abstract}

\section{Introduction}
% \paragraph{On the popularity and power of Fiat--Shamir.}
The Fiat--Shamir (FS) transform takes a public-coin interactive protocol and makes it
interactive by computing the verifier's public coins by hashing the current
protocol transcript. While in principle justifiable in the random oracle model
(ROM)~\cite{CCS:BelRog93}, it is theoretically unsound~\cite{FOCS:GolKal03} and
so only a heuristic that should be used with care.

Nevertheless, the FS transform is a popular design tool when it comes
to constructing \emph{zero knowledge succinct arguments of knowledge}
(zkSNARKs). In recent years, zkSNARKs have seen indisputable progress~\cite{AC:Groth10a,TCC:Lipmaa12,EC:GGPR13,SP:PHGR13,AC:Lipmaa13,AC:DFGK14,EC:Groth16,SP:BBBPWM18}
Their succinctness make them especially useful for deployment in
real systems~\cite{REPO:Zcash20,ARXIV:RonZaj19,REPO:Zeth20,REPO:Celo20,REPO:Aztec20}. However,
many works, including \sonic{}~\cite{CCS:MBKM19}
\plonk{}~\cite{EPRINT:GabWilCio19}, and \marlin~\cite{EC:CHMMVW20} are designed and proven secure as multi-round interactive protocols. Security is then only
\emph{conjectured} for their non-interactive variants by employing
the FS transform. Properly analyzing this heuristic security of zkSNARKs is of great importance not the least because they are frequently
used to secure crypto-currency assets of immense value. Moreover, the aforementioned
zkSNARKs employ not only a FS transform, but also a structured
reference string (SRS). To the best of our knowledge the security of the FS
transform for multi-round SRS-based zero knowledge proof systems has not yet been
shown, not even heuristically in the random oracle model. \michals{22.05}{What about some works by Tibor Jeager?}\markulf{Not sure which work you refer to, but I agree. Likely someone did look at this. We can also add the multi-round conjunct.}

\hl{Write about FS analysis for e.g. Bulletproofs}
Security conjectures via the Fiat--Shamir transform are not the only Achilles heel of zkSNARKs. While the security of the FS transform is of interest primarily to cryptographers other concerns are more practical: The most efficient zkSNARKs
require a trusted third party generated SRS and provide no security if the
SRS is subverted. This issue has been tackled by, e.g.~\plonk{}
and \sonic{} by employing the updatable SRS framework~\cite{C:GKMMM18}. Instead of
using a single SRS-generating entity, this framework allows the SRS to be generated by a set of
different parties and assures that it remains secure if at least one of these
parties is honest.

A further problem with existing security models is proof malleability.  Arguably,
in the real world one simply cannot assume that the adversary who tries to break
security of a system does not have access to any proofs provided by other
parties using the same zero-knowledge scheme. On the contrary, in the most
popular applications of zkSNARKs, like privacy-preserving blockchains, proofs
made by all blockchain-participants are (usually) public. Thus, it is only
reasonable to require a zero-knowledge proof system to be resilient to attacks
that utilise proofs generated by different parties.  Nevertheless, most zkSNARKs
are only shown to satisfy a (standard) knowledge soundness definition. We
argue that simulation-extractability (SE) is the property that should be
required from zkSNARKs deployed in practice.

Unfortunately, to the best of our knowledge, there are zkSNARKs that are
simulation-extractable~\cite{C:GroMal17,EPRINT:BowGab18,EPRINT:AtaBag19,EC:Groth16}
and zkSNARKs that are universal or
updatable~\cite{C:GKMMM18,CCS:MBKM19,EPRINT:GabWilCio19,EC:CHMMVW20}. However, no zkSNARKs are known to enjoy all of these properties out-of-the-box
(even for a weaker notion of simulation extractability). Obviously, given a
universal and updateable zkSNARK one could lift it to be simulation-extractable using
techniques described, e.g.,~in \cite{EPRINT:KZMQCP15,CCS:AbdRamSla20}, but such a
lift comes with inevitable efficiency loss.  %The same applies for updatable
%zkSNARKs. No out-of-the-box simulation-extractable updatable zkSNARKs are
%currently known and there is no transformation that could take a SE zkSNARK and
%make it updatable (even though transformations like \cite{CCS:AbdRamSla20}
%preserves updatability).

\subsection{Our contribution}
We show that a class of computationally special-sound interactive proofs of
knowledge that are (honest-verifier) zero-knowledge in the standard model and
have a unique response property \emph{are simulation-extractable out-of-the box}
in the random oracle model when the Fiat--Shamir transformation is applied to
them. In contract to InFaust et al.~\cite{INDOCRYPT:FKMV12} who focused on
special-sound $3$-message $\Sigma$-protocols, our result can be applied to a
wider class of protocols and is applicable to zkSNARKs.

We follow Faust et al.~in their definition of simulation extractability. They
called their simulation-extractability \emph{weak} which relates to the fact
that the extractor's probability of returning a witness depends on the
adversary's probability $\accProb$ of producing an acceptable proof. More
precisely, the extractor is not guaranteed to succeed if the adversary outputs
an acceptable proof with probability $\accProb$ smaller than knowledge error
$\nu$. On the other hand, we show that this $\nu$ can be arbitrarily small, even
negligible. However, for $\accProb$ close to $\nu$ the extractor becomes fairly
inefficient. Since the extractor utilizes special soundness of the protocol, we
call our flavour of simulation extractability \emph{special}. This is also to
avoid confusion with weakly simulation extractable protocols, which provide
simulation extractability given that the adversary cannot output a proof for a
statement it saw a simulated proof for. Importantly, weakly simulation
extractable proof systems are (often) re-randomizable. (Since we do not consider other notions
of simulation extractability than special, we sometimes omit that adjective and
write ``simulation extractability'')

Furthermore, Faust et al.'s extractor is not fully black-box as it needs to run
the adversary multiple times on different random oracles. On the other hand, the
extractor is not white-box either as it only requires oracle access to the
adversary and does not depend on the adversary's code. Moreover, it does not
depend on any knowledge assumption.  (However, it also depends on a number of
properties required from the protocol. In case of $\plonk$ and $\sonic$, these
properties are often proven in the AGM.) We also note that when an extractor
depends on rewinding the adversary, it is expected to have extractor's success
probability dependent on the adversary's probability of returning a valid proof.
To show that our result is useful and practical we prove that two of the most
efficient updatable and universal zkSNARKs---$\plonk$ and $\sonic$---are
simulation-extractable. To obtain this result, without having to change anything
at all in these protocols, we define new intermediary properties satisfied by
these multi-round computationally sound protocols and their building blocks:
computational special soundness, generalized unique response, and a generalized
forking lemma. These conceptual insights into interactive SNARK systems help us
overcome a number of challenges, as we explain below.

Unfortunately, the extraction technique we use is quite inefficient as the
security loss is exponential in the number of transcripts required to extract
the witness. We use that result as a warning -- one should pay more attention
when conjecturing security relying on the Fiat--Shamir transformation as it may
introduce huge security loss.  On the up side, we show that even that fairly
inefficient extraction gives us tight simulation soundness.

\subsection{Our techniques}

Before we continue, we note that \plonk{} and \sonic{}---as originally presented
in \cite{EPRINT:GabWilCio19} and \cite{CCS:MBKM19}---are interactive proofs of
knowledge made non-interactive by the Fiat--Shamir transform. In the following,
we denote the underlying interactive protocols by $\plonkprot$ (for $\plonk$)
and $\sonicprot$ (for $\sonic$) and the resulting non-interactive proof systems by
$\plonkprotfs$ and $\sonicprotfs$, respectively.

\subsubsection{Special soundness.} 
First, following \cite{INDOCRYPT:FKMV12}, we need to show that $\plonkprot$ and
$\sonicprot$ are special-sound. However the standard definition of special
soundness could not be met. First of all, the definition requires extraction of
a witness from any two transcripts, each containing three messages and sharing
the first message. For $\plonkprot$ and $\sonicprot$ that is not enough. The
definition needs to be adapted to cover protocols with more than
three messages. Furthermore, the number of transcripts required is much
greater. Concretely, $(\numberofconstrains + 3)$---where $\numberofconstrains$ is the
number of constraints in the proven circuit---for $\plonkprot$ and $(\multconstr
+ \linconstr + 1)$---where $\multconstr$ and $\linconstr$ are the numbers of
multiplicative and linear constraints---for $\sonicprot$. Hence, we do not have
a \emph{pair of transcripts}, but a \emph{tree of transcripts}.

Second, both protocols rely on structured reference strings which come with
trapdoors. If in possession of the trapdoor, an adversary can produce multiple valid
proof transcripts without knowing the witness and potentially for false statements. Recall that the
standard special soundness definition requires witness extraction from
\emph{any} tree of acceptable transcripts that share a common root. This means that there are no such trees for incorrect statements. In this paper we
define a weaker computational version of special soundness. That is, we show that it is possible to extract a witness from all
but negligibly many trees of acceptable transcripts produced by probabilistic
polynomial time (PPT) adversaries. Said otherwise, an adversary that produced a
tree of acceptable transcripts from which one cannot extract can be used to
break some underlying computational assumption.

\subsubsection{Unique response property.} Another property that has to be proven
is the unique response property which states, as expressed in
\cite{C:Fischlin05}, that except for the first message, all messages sent by the
prover are deterministic (intuitively,~the prover can only employ fresh
randomness in the first message of the protocol). Again, we can not use this
definition right out of the box. $\plonkprot$ does not satisfy it---both the
first and the second prover's messages are randomised. We thus propose a
generalisation of the definition which states that a protocol is $\ur{i}$ if the
prover is deterministic starting from its $i$-th message. For our proof it is
sufficient that this property is met by $\plonkprot$ for $i = 3$. \markulf{What about \sonic?}

To be able to show the unique response property (for both of the protocols) we
also had to show that the modified KZG polynomial commitment schemes
\cite{AC:KatZavGol10} proposed in \cite{EPRINT:GabWilCio19} and
\cite{CCS:MBKM19} have a \emph{unique opening property} which states that for a
polynomial $\p{f}(X)$ evaluated at some point $z$ it should be infeasible for
any PPT adversary to provide two different but acceptable openings
of the commitment.

\subsubsection{HVZK.}%
In order to show our result we also show that (interactive) $\plonkprot$ and
$\sonicprot$ are honest verifier zero-knowledge in the standard model, i.e.~the
simulator is able to produce a transcript indistinguishable from a transcript
produced by an honest prover and verifier without any additional knowledge,
esp.~without knowing the SRS trapdoor. Although both $\sonic$ and $\plonk$ are
shown to be zero-knowledge, the proofs provided by their authors utilise
trapdoors. For our reduction to work, we need simulators that provide
indistinguishable proofs relying only on reordering the messages
and picking suitable verifier challenges. That is, any PPT party should be
able to produce a simulated proof by its own. (Note that this property does not
necessary break soundness of the protocol as the simulator is required only to
produce a transcript and is not involved in a real conversation with a real
verifier). This property allows us to build simulators for $\plonkprotfs$ and
$\sonicprotfs$ that rely only on the programmability of the random oracle.

\subsubsection{Generalisation of the general forking lemma.}
Consider an interactive $3$-message special-sound protocol $\ps$ and its
non-interactive version $\ps_\fs$ obtained by the Fiat--Shamir transform. The
general forking lemma provides an instrumental lower bound for the probability of
extracting a witness from an adversary who provides two proofs for the same
statement that share the first message. Since $\plonkprot$ and $\sonicprot$ have
more than $3$ messages and are not special-sound, the forking lemma of Bellare and Neven~\cite{CCS:BelNev06}, cannot be used directly. We thus propose a modification
that covers multi-message protocols where witness extraction requires more
transcripts than merely two.  Unfortunately, we also observe that the security
gap grows with the number of transcripts and the probability that the extractor
succeeds diminishes significantly. (That said, we have to note that the security
loss is polynomial, albeit big.)

We note that some modern zkSNARKs, like \cite{SP:BBBPWM18,CCS:MBKM19}, heavily rely on
the Fiat--Shamir transform and the forking lemma. First, an interactive
protocol is proposed and its security and (a variant of) special-soundness analysed. Second,
one uses an argument that the Fiat--Shamir transform can be used to get a
protocol that is non-interactive and shares the same security properties.

  
We see our generalized forking lemma as contributing to a critical assessment of
this approach. The analysis of the interactive protocol is not enough and one
has to consider the security loss implied by the generalisation of the forking
lemma or disclose a transformation that does not suffer this loss. We note that the security loss may also apply when knowledge
soundness is proven. That is the case for $\sonic$, whose security proof relies
on so-called witness-extended emulation. The authors of $\plonk$ worked around this
problem by proving knowledge soundness directly in the algebraic group model.

\subsubsection{Special simulation-extractability.} Given our modified, less
restrictive, definition for special-soundness and the unique response property,
and our generalised forking lemma we obtain our main
result---special simulation extractability of $\plonkprotfs$ and $\sonicprotfs$. The
proof is inspired by simulation-extractability and simulation-soundness proofs
from \cite{INDOCRYPT:FKMV12}, with major modifications, which were required as
\cite{INDOCRYPT:FKMV12} considers only $\Sigma$-protocols
that are undoubtedly simpler protocols than the considered proof systems.

\subsubsection{Generalising Boneh-Boyen-Goh~\cite{EC:BonBoyGoh05} uber assumption.}
To show that $\plonk$ is zero-knowledge we rely on a variant of
BBG's \emph{uber assumption}. In its original version, the
assumption assures that some polynomial evaluation (on a random, unknown point)
represented in a bilinear pairing target group $\GRP_T$ is indistinguishable
from a random element. In our variant, we modify two things.
\begin{inparaenum}[(1)]
\item Firstly, the polynomial evaluation can be represented in other groups
  than $\GRP_T$; here we use $\GRP_1$;
\item Secondly, the distinguisher is given not only a single polynomial evaluation,
  but a number of polynomials  evaluations. That is, it either gets the result of $k$ polynomial
  evaluations or $k$ random numbers.
\end{inparaenum}
We show security of the generalized uber assumption directly in the generic
group model.

% Since the proof highly relies on the (generalised) forking lemma, the security
% lost it introduces is considerable.

% \subsubsection{Efficient simulation-soundness.}
% Given that the security reduction for simulation-extractability introduces a
% security gap we also present a proof for $\plonkprot_\fs$ simulation soundness
% which utilises the algebraic group model and is tight. It remains an open
% question how to show simulation extractability tightly, e.g.~using AGM.

% \subsection{Comparison with Bootle et al.~\cite{EC:BCCGP16}}
% \newcommand{\treefinder}{\pcadvstyle{T}}
% \newcommand{\provers}{\prover^*}
% Bootle et al.~\cite{EC:BCCGP16} provides a ``forking lemma'' showing that a
% protocol which has an extractor $\ext$ able to extract a witness from a tree of
% acceptable transcripts follows also notion of witness-extended emulation.
% An interestin question is whether their version of the forking lemma could be
% used to show security of protocols which were made non-interactive by the
% Fiat--Shamir transformation. We provide a simple adversary which shows that the
% answer is---not directly. \hl{In the following part of the paper we show that
%   allowing for some efficiency loss the result of Bootle et al.~\emph{can} be
%   used for non-interactive protocols.}


% \begin{lemma}
%   Let $\proofsystem$ be an $(2\mu + 1)$ public-coin interactive proofsystem and
%   $\proofsystem_\fs$ be Fiat--Shamir transformed $\proofsystem$, $\ext$ be a
%   witness extraction algorithm which always succeeds to extract a witness from a
%   $(n_1, \ldots, n_\mu)$-tree of accepting transcripts and $\provers$ be a
%   (possibly malicious) prover for $\proofsystem_\fs$.
%   Then the tree finding algorithm $\treefinder$ presented at \cite[Lemma
%   1]{EC:BCCGP16} builds a tree of accepting transcript with probability $0$.
% \end{lemma}
% \begin{proof}
% Let $\provers$ be an prover who proceeds as follows. On input $(\inp, \wit)
% \in \REL$ it computes the first $i$ rounds of the protocol honestly, then it breaks
% and start the protocol over. With the second run, it finishes and outputs an
% honest proof for $\inp$ using $\wit$.

% Next, we analyse the probability that the tree-finder $\treefinder$ outputs a
% tree of accepting transcripts by rewinding $\provers$ and providing it with
% fresh challenges.

%   \qed
% \end{proof}


\subsection{Structure of the paper}
In the next section we present necessary preliminaries. \cref{sec:se_definitions} gives definitions and lemmas for instantiating our framework. In \cref{sec:general}, we
present our main result, a proof of simulation-soundness for a class of
zero-knowledge proofs of knowledge. In \cref{sec:plonk} and \cref{sec:sonic} we show
that $\plonk$ and $\sonic$ fulfill the requirements of our framework and are in fact special simulation extractable.

\subsection{Related Work}
There are many results on simulation extractability for non-interactive
zero-knowledge proofs (NIZKs). First, Groth \cite{AC:Groth07} noticed that a
(black-box) simulation extractable NIZK is universally-composable (UC)
\cite{EPRINT:Canetti00}. Then Dodis et al.~\cite{AC:DHLW10} introduced a notion
of (black-box) \emph{true simulation extractability} and showed that no NIZK can
be UC-secure if it does not satisfy this property. In the context of zkSNARKs it is
important to mention such works as the first simulation-extractable zkSNARK by
Groth and Maller \cite{C:GroMal17} and SE zkSNARK for QAP by Lipmaa
\cite{EPRINT:Lipmaa19b}. Kosba's et al.~\cite{EPRINT:KZMQCP15} give a general
transformation from a NIZK to a black-box SE NIZK. Although their transformation
works for zkSNARKs as well, succinctness of the proof system is not preserved as
the statement's witness is encrypted. Recently, Abdolmaleki et
al.~\cite{CCS:AbdRamSla20} showed another transformation that obtains
non-black-box simulation extractability but also preserves succinctness of the
argument.

Independently, some authors focused on obtaining simulation extractability of
known zkSNARKs, like $\groth$ \cite{EC:Groth16}, by introducing minor
modifications and using stronger assumptions
\cite{EPRINT:BowGab18,EPRINT:AtaBag19}. Interestingly, although such
modifications hurt performance of the proof system, the resulting zkSNARKs are
still more efficient than the first SE zkSNARK \cite{C:GroMal17}, see
\cite{EPRINT:AtaBag19}. Recently, \cite{EPRINT:BKSV20} showed that the original
Groth's proof system from \cite{EC:Groth16} is weakly SE and randomisable.

\paragraph{Forking lemma generalizations.}
In \cite{EC:BCCGP16} Bootle et al.~proposed a novel inner-product argument which
security relies on, so-called, witness-extended emulation. To show that
property, the authors proposed a new version of forking lemma, which gives a
lower bound on probability that a tree finding algorithm is able to produce a 
tree of acceptable transcripts by rewinding a conversation between a
(potentially malicious) prover and verifier.

Although the result of presented in that paper is dubbed ``forking lemma'' it
differs from forking lemmas known from e.g.~\cite{JC:PoiSte00,CCS:BelNev06}.
First of all, the forking lemmas in these papers analyse probability of building
a tree of acceptable transcripts for Fiat--Shamir based non-interactive proof
systems, while the protocol presented by Bootle et al.~is intended to work for
interactive proof systems.

Importantly, it is not obvious how the result of Bootle et al.~can be used to
show security of non-interactive protocols as it relies on interactive provers
whose proving strategies are more limited than proving strategies of
non-interactive ones. For example, if a challenge given by the verifier does not
suit an interactive prover, it can only try to finish a proof with it or
abort. On the other hand, a non-interactive prover has far wider scope of
possible actions---when the protocol is non-interactive the prover may
adapt its strategy respectively to the random oracle outputs. For example,
seeing a response $h$ after sending $k$-th message it may decide to ``go back''
a number of steps, e.g.~prior sending $k'$-th message ($k' \leq k$), provide
different messages hoping for better suited response $h'$ on its $k$-th
message. Furthermore, the adversary may even interrupt the proof and try again
with another instance. All of these actions would make the proof unacceptable
for an interactive verifier, however they may give a perfectly fine proof for a
non-interactive one.

\section{Preliminaries}
\label{sec:preliminaries}
Let $\ppt$ denote probabilistic polynomial-time and $\secpar \in \NN$ be the
security parameter. All adversaries are stateful. For an algorithm $\adv$, let
$\image (\adv)$ be the image of $\adv$ (the set of valid outputs of $\adv$), let
$\RND{\adv}$ denote the set of random tapes of correct length for $\adv$
(assuming the given value of $\secpar$), and let $r \sample \RND{\adv}$ denote
the random choice of the randomiser $r$ from $\RND{\adv}$. We denote by $\negl$
($\poly$) an arbitrary negligible (resp.~polynomial) function.

Probability ensembles $X = \smallset{X_\secpar}_\secpar$ and $Y =
\smallset{Y_\secpar}_\secpar$, for distributions $X_\secpar, Y_\secpar$, have
\emph{statistical distance} $\SD$ equal $\epsilon(\secpar)$ if $\sum_{a \in
  \supp{X_\secpar \cup Y_\secpar}} \abs{\prob{X_\secpar = a} - \prob{Y_\secpar =
    a}} = \epsilon(\secpar)$. We write $X \approx_\secpar Y$ if $\SD(X_\secpar,
Y_\secpar) \leq \negl$. For values $a(\secpar)$ and $b(\secpar)$ we write
$a(\secpar) \approx_\secpar b(\secpar)$ if $\abs{a(\secpar) - b(\secpar)} \leq
\negl$.

\newcommand{\samplespace}{\Omega}
\newcommand{\eventspace}{\mathcal{F}}
\newcommand{\probfunction}{\mu}

For a probability space $(\samplespace, \eventspace, \probfunction)$ and event
$\event{E} \in \eventspace$ we denote by $\nevent{E}$ an event that is
complementary to $\event{E}$,
i.e.~$\nevent{E} = \samplespace \setminus \event{E}$.

Denote by $\RELGEN = \smallset{\REL}$ a family of relations. We assume that if
$\REL$ comes with any auxiliary input, it is benign. Directly from the
description of $\REL$ one learns security parameter $\secpar$ and other
necessary information like public parameters $\pp$ containing description of a
group $\GRP$, if the relation is a relation of group elements (as it usually is
in case of zkSNARKs).

\paragraph{Bilinear groups.}
A bilinear group generator $\pgen (\secparam)$ returns public parameters $ \pp =
(p, \GRP_1, \GRP_2, \GRP_T, \pair, \gone{1}, \gtwo{1})$, where $\GRP_1$,
$\GRP_2$, and $\GRP_T$ are additive cyclic groups of prime order $p = 2^{\Omega
  (\secpar)}$, $\gone{1}, \gtwo{1}$ are generators of $\GRP_1$, $\GRP_2$, resp.,
and $\pair: \GRP_1 \times \GRP_2 \to \GRP_T$ is a non-degenerate
$\ppt$-computable bilinear pairing. We assume the bilinear pairing to be Type-3,
i.e., that there is no efficient isomorphism from $\GRP_1$ to $\GRP_2$ or from
$\GRP_2$ to $\GRP_1$. We use the by now standard bracket notation, i.e., we
write $\bmap{a}{\gi}$ to denote $a g_{\gi}$ where $g_{\gi}$ is a fixed generator
of $\GRP_{\gi}$. We denote $\pair (\gone{a}, \gtwo{b})$ as $\gone{a} \bullet
\gtwo{b}$. Thus, $\gone{a} \bullet \gtwo{b} = \gtar{a b}$. We freely use the
bracket notation with matrices, e.g., if $\vec{A} \vec{B} = \vec{C}$ then
$\vec{A} \grpgi{\vec{B}} = \grpgi{\vec{C}}$ and $\gone{\vec{A}}\bullet
\gtwo{\vec{B}} = \gtar{\vec{C}}$. Since every algorithm $\adv$ takes as input
the public parameters we skip them when describing $\adv$'s input. Similarly, we
do not explicitly state that each protocol starts with generating these
parameters by $\pgen$.

\subsection{Computational assumptions.}

\paragraph{Discrete-log assumptions.}
Security of $\plonk$ and $\sonic$ relies on two discrete-log based security
assumptions---$(q_1, q_2)$-$\dlog$ assumption and its extended version with negative
exponents $(q_1, q_2)$-$\ldlog$ assumption\footnote{Note that
  \cite{CCS:MBKM19} dubs their assumption \emph{a dlog assumption}. We changed
  that name to distinguish it from the more standard dlog assumption used in
  \cite{EPRINT:GabWilCio19}. ``l'' in \emph{ldlog} relates to use of Laurent
  polynomials in the assumption.}.

\begin{definition}[$(q_1, q_2)\mhyph\dlog$ assumption]
	Let $\adv$ be a $\ppt$ adversary that gets as input
  $\gone{1, \chi, \ldots, \chi^{q_1}}, \gtwo{1, \chi, \ldots, \chi^{q_2}}$, for
  some randomly picked $\chi \in \FF_p$, then
	\[
		\condprob{\chi \gets \adv(\gone{1, \chi, \ldots, \chi^{q_1}}, \gtwo{1, \chi,
        \ldots, \chi^{q_2} })}{\chi \sample \FF_p} \leq \negl.
	\]
\end{definition}

\begin{definition}[$(q_1, q_2)\mhyph\ldlog$ assumption]
  Let $\adv$ be a $\ppt$ adversary that gets as input
  $\gone{\chi^{-q_1}, \ldots, 1, \chi, \ldots, \chi^{q_1}}, \gtwo{\chi^{-q_2},
    \ldots, 1, \chi, \ldots, \chi^{q_2}}$, for some randomly picked
  $\chi \in \FF_p$, then
	\[
    \condprob{\chi \gets \adv(\gone{\chi^{-q_1}, \ldots, 1, \chi, \ldots,
        \chi^{q_1}}, \gtwo{\chi^{-q_2}, \ldots, 1, \chi, \ldots, \chi^{q_2}
      })}{\chi \sample \FF_p} \leq \negl.
	\]
\end{definition}

\paragraph{BBG uber assumption.}
Also, to be able to show computational honest verifier zero knowledge of
$\plonk$ in the standard model, what is required by our reduction, we rely on the
\emph{uber assumption} introduced by Boneh et
al.~\cite{EC:BonBoyGoh05} as presented by Boyen in \cite{PAIRING:Boyen08}.
% Here we present not in its whole generality but rather fitted to our purpose.

Let $r, s, t, c \in \NN \setminus \smallset{0}$, Consider vectors of polynomials
$\pR \in \FF_p[X_1, \ldots, X_c]^r$, $\pS \in \FF_p[X_1, \ldots, X_c]^s$ and
$\pT \in \FF_p[X_1, \ldots, X_c]^t$. Write $\pR = \left( \p{r}_1, \ldots,
  \p{r}_r \right)$, $\pS = \left( \p{s}_1, \ldots, \p{s}_s \right)$ and $\pT =
\left( \p{t}_1, \ldots, \p{t}_t \right)$ for polynomials $\p{r}_i, \p{s}_j,
\p{t}_k$.

For a function $f$ and vector $(x_1, \ldots, x_c)$ we write $f(\pR)$ to
denote application of $f$ to each element of $\pR$, i.e.
\[
	f(\pR) = \left( f(\p{r}_1 (x_1, \ldots, x_c), \ldots, f(\p{r}_r
	(x_1, \ldots, x_c) \right).
\]
Similarly for applying $f$ to $\pS$ and $\pT$.

\begin{definition}[Independence of $\pR, \pS, \pT$]
	\label{def:independence}
	Let $\pR, \pS, \pT$ be defined as above. We say that polynomial $\p{f} \in
  \FF_p[X_1, \ldots, X_c]$ is \emph{dependent} on $\pR, \pS, \pT$ if there
  exists $rs + t$ constants $a_{i, j}, b_k$ such that $ \p{f} = \sum_{i = 1}^{r}
  \sum_{j = 1}^{s} a_{i, j} \p{r}_i \p{s}_j + \sum_{k = 1}^{t} b_k \p{t}_k. $ We
  say that $\p{f}$ is \emph{independent} if it is not dependent.
\end{definition}

To show (standard-model) zero knowledge of $\plonk$ we utilize a generalization
of Boneh-Boyen-Goh's \emph{uber assumption} \cite{EC:BonBoyGoh05} stated as
follows (the changed element has been put into a \dbox{dashbox})
% \begin{definition}[$(\pR, \pS, \pT, \p{f}, T)$-uber assumption
% 	\cite{EC:BonBoyGoh05}]
% 	\label{def:uber_assumption_orig}
% 	Let $\pR, \pS, \pT$ be defined as above, $(x', x_1, \ldots, x_c) \sample
%   \FF_p^{c + 1}$ and let $\p{f}$ be independent on $(\pR, \pS, \pT)$,
%   cf.~\cref{def:independence}. Then, for any $\ppt$ adversary $\adv$
% 	\begin{multline*}
% 		\prob{\adv(\gone{\pR(x_1, \ldots x_c)}, \gtwo{\pS(x_1, \ldots, x_c)},
% 		\gtar{\pT(x_1, \ldots, x_c)}, \gtar{\p{f}(x_1, \ldots, x_c)}) = 1} \approx_\secpar \\ 
% 		\prob{\adv(\gone{\pR(x_1, \ldots x_c)}, \gtwo{\pS(x_1, \ldots, x_c)},
% 		\gtar{\pT(x_1, \ldots, x_c)}, \gtar{x'}) = 1}.  
% 	\end{multline*}
% \end{definition}
\begin{definition}[$(\pR, \pS, \pT, \p{F}, 1)$-uber assumption]
	\label{def:uber_assumption}
	Let $\pR, \pS, \pT$ be defined as above,
    $(x_1, \ldots, x_c, y_1, \ldots, y_{d}) \sample \FF_p^{c + d}$ and let
    $\p{F}$ be a cardinality-$d$ set of pair-wise independent polynomials which are also
    independent of $(\pR, \pS, \pT)$, cf.~\cref{def:independence}.  Then, for
    any $\ppt$ adversary $\adv$
	\begin{multline*}
      \Pr\left[\adv(\gone{\pR(x_1, \ldots x_c)}, \gtwo{\pS(x_1, \ldots, x_c)},
		\gtar{\pT(x_1, \ldots, x_c)}, \dboxed{\gone{\p{F}(x_1, \ldots, x_c)}}) = 1\right] \approx_\secpar \\
      \Pr\left[\adv(\gone{\pR(x_1, \ldots x_c)}, \gtwo{\pS(x_1, \ldots, x_c)},
        \gtar{\pT(x_1, \ldots, x_c)}, \dboxed{\gone{y_1, \ldots, y_{d}}}) =
        1\right].
	\end{multline*}
  \end{definition}

  Compared to the original uber assumptions, there are two major changes. First,
  we require not target group $\GRP_T$ elements to be indistinguishable, but
  elements of $\GRP_1$. Second, Boneh et al.'s assumption works for
  distinguishers who are given only one challenge polynomial $\p{f}$,
  i.e.~$\abs{\p{F}} = 1$.  We prove our variant of the uber assumption in the
  generic group model, see \cref{thm:uber_assumption}.

%   \begin{lemma}
%     Let $\pS$ contain a constant polynomial $\p{s}(X) = 1$, then
%     $(\pR, \pS, \pT, \p{f}, T)$-uber assumption implies
%     $(\pR, \pS, \pT, \p{f}, 1)$-uber assumption.
%   \end{lemma}
%   \begin{proof}
%     Let $\adv_T$ be an adversary that breaks the
%     $(\pR, \pS, \pT, \p{f}, T)$-uber assumption with some non-negligible
%     advantage $\eps$, we build $\adv_{1}$ that breaks
%     $(\pR, \pS, \pT, \p{f}, 1)$-uber assumption with the same advantage. Let
%     $\adv_1$ get as its input
%     $(\gone{\pR(x_1, \ldots x_c)}, \gtwo{\pS(x_1, \ldots, x_c)}, \gtar{\pT(x_1,
%       \ldots, x_c)}, \gone{x'})$, for some random $x_1, \ldots, x_c$, and $x'$
%     being either $\p{f}(x_1, \ldots, x_c)$ or a random element. Then $\adv_1$
%     obtains $\gtar{x'}$ by computing $\gone{x'} \bullet \gtwo{1}$ and runs
%     $\adv_T (\gone{\pR(x_1, \ldots x_c)}, \gtwo{\pS(x_1, \ldots, x_c)},
%     \gtar{\pT(x_1, \ldots, x_c)}, \gtar{x'})$ which outputs its guess
%     $b$. Adversary $\adv_1$ returns it and wins with probability $1/2 + \eps$.
% \end{proof}

\paragraph{Proofs by Game-Hoping.}
Proofs by \emph{game hoping} is a method of writing proofs popularised by
e.g.~Shoup \cite{EPRINT:Shoup04} and Dent \cite{EPRINT:Dent06c}. The method
relies on the following lemma.

\begin{lemma}[Difference lemma,~{\cite[Lemma 1]{EPRINT:Shoup04}}]
	\label{lem:difference_lemma}
	Let $\event{A}, \event{B}, \event{F}$ be events defined in some probability
	space, and suppose that $\event{A} \land \nevent{F} \iff \event{B}
		\land \nevent{F}$.  Then 
	$
		\abs{\prob{\event{A}} - \prob{\event{B}}} \leq \prob{\event{F}}\,.
	$
\end{lemma}
\subsection{Algebraic Group Model}
The algebraic group model (AGM) introduced in \cite{C:FucKilLos18} lies between
the standard model and generic bilinear group model. In the AGM it is assumed
that an adversary $\adv$ can output a group element $\gnone{y} \in \GRP$ if
$\gnone{y}$ has been computed by applying group operations to group elements
given to $\adv$ as input. It is further assumed, that $\adv$ knows how to
``build'' $\gnone{y}$ from that elements. More precisely, the AGM requires that
whenever $\adv(\gnone{\vec{x}})$ outputs a group element $\gnone{y}$ then it
also outputs $\vec{c}$ such that $\gnone{y} = \vec{c}^\top \cdot
\gnone{\vec{x}}$. Both $\plonk$ and $\sonic$ have been shown secure using the
AGM. An adversary that works in the AGM is called \emph{algebraic}.

\subsection{Polynomial commitment.}
\label{sec:poly_com}
In the polynomial commitment scheme $\PCOM = (\kgen, \com, \open, \verify)$ the
committer $\committer$ can convince the receiver $\receiver$ that some polynomial
$\p{f}$ which $\committer$ committed to evaluates to $s$ at some point $z$
chosen by $\receiver$. $\plonk$ and $\sonic$ use variants of the KZG polynomial
commitment scheme \cite{AC:KatZavGol10}. We denote the first by $\PCOMp$, presented in
\cref{fig:pcomp}, and the latter by $\PCOMs$, presented in \cref{fig:pcoms}. The
key generation algorithm $\kgen$ takes as input a security parameter $\secparam$
and a parameter $\maxdeg$ which determines the maximal degree of the committed
polynomial. We assume that $\maxdeg$ can be read from the output SRS.
%
% We require $\PCOM$ to have the following properties:
\begin{figure}[t!]
\centering
	\begin{pcvstack}[center,boxed]
		\begin{pchstack}
			\procedure{$\kgen(\secparam, \maxdeg)$}
			{
			\chi \sample \FF^2_p \\ [\myskip]
			\pcreturn \gone{1, \ldots, \chi^{\numberofconstrains + 2}}, \gtwo{\chi}\\ [\myskip]
				\hphantom{\hspace*{5.5cm}}	
        %\hphantom{\pcind \p{o}_i(X) \gets \sum_{j = 1}^{t_i} \gamma_i^{j - 1}
        %\frac{\p{f}_{i,j}(X) - \p{f}_{i, j}(z_i)}{X - z_i}}
      }
			
			\pchspace
			
			\procedure{$\com(\srs, \vec{\p{f}}(X))$}
			{ 
				\pcreturn \gone{\vec{c}} = \gone{\vec{\p{f}}(\chi)}\\ [\myskip]
				\hphantom{\pcind \pcif 
					\sum_{i = 1}^{\abs{\vec{z}}} r_i \cdot \gone{\sum_{j = 1}^{t_j}
					\gamma_i^{j - 1} c_{i, j} - \sum{j = 1}^{t_j} s_{i, j}} \bullet
				\gtwo{1} + }
			}
		\end{pchstack}
		% \pcvspace
    
		\begin{pchstack}
			\procedure{$\open(\srs, \vec{\gamma}, \vec{z}, \vec{s}, \vec{\p{f}}(X))$}
			{
			\pcfor i \in \range{1}{\abs{\vec{z}}} \pcdo\\ [\myskip]
      \pcind \p{o}_i(X) \gets \sum_{j = 1}^{t_i} \gamma_i^{j - 1}
      \frac{\p{f}_{i,j}(X) - \p{f}_{i, j}(z_i)}{X - z_i}\\ [\myskip] \pcreturn
      \vec{o} = \gone{\vec{\p{o}}(\chi)}\\ [\myskip]
				\hphantom{\hspace*{5.5cm}}	
			}
			
			\pchspace
			
			\procedure{$\verify(\srs, \gone{c}, \vec{z}, \vec{s}, \gone{\p{o}(\chi)})$}
			{
				\vec{r} \gets \FF_p^{\abs{\vec{z}}}\\ [\myskip]
				\pcfor i \in \range{1}{\abs{\vec{z}}} \pcdo \\ [\myskip]
				\pcind \pcif 
          \sum_{i = 1}^{\abs{\vec{z}}} r_i \cdot \gone{\sum_{j = 1}^{t_j}
          \gamma_i^{j - 1} c_{i, j} - \sum{j = 1}^{t_j} s_{i, j}} \bullet
          \gtwo{1} + \\ [\myskip] \pcind \sum_{i = 1}^{\abs{\vec{z}}} r_i z_i
          o_i
          \bullet \gtwo{1} \neq \gone{- \sum_{i = 1}^{\abs{\vec{z}}} r_i o_i }
          \bullet \gtwo{\chi} \pcthen  \\
					\pcind \pcreturn 0\\ [\myskip]
					\pcreturn 1.
			}
		\end{pchstack}
	\end{pcvstack}
	\caption{$\PCOMp$ polynomial commitment scheme.}
	\label{fig:pcomp}
  \end{figure}

\begin{figure}[t!]
\centering
	\begin{pcvstack}[center,boxed]
		\begin{pchstack}
			\procedure{$\kgen(\secparam, \maxdeg)$} {
				\alpha, \chi \sample \FF^2_p \\ [\myskip]
				\pcreturn \gone{\smallset{\chi^i}_{i = -\multconstr}^{\multconstr},
          \smallset{\alpha \chi^i}_{i = -\multconstr, i \neq
            0}^{\multconstr}},\\
        \pcind \gtwo{\smallset{\chi^i, \alpha \chi^i}_{i =
            -\multconstr}^{\multconstr}}, \gtar{\alpha}\\
				%\markulf{03.11.2020}{} \\
			%	\hphantom{\pcind \p{o}_i(X) \gets \sum_{j = 1}^{t_i} \gamma_i^{j - 1} \frac{\p{f}_{i,j}(X) - \p{f}_{i, j}(z_i)}{X - z_i}}
				\hphantom{\hspace*{5.5cm}}	
		}
			
			\pchspace
			
			\procedure{$\com(\srs, \maxconst, \p{f}(X))$} {
				\p{c}(X) \gets \alpha \cdot X^{\dconst - \maxconst} \p{f}(X) \\ [\myskip]
				\pcreturn \gone{c} = \gone{\p{c}(\chi)}\\ [\myskip]
				\hphantom{\pcind \pcif \sum_{i = 1}^{\abs{\vec{z}}} r_i \cdot
          \gone{\sum_{j = 1}^{t_j} \gamma_i^{j - 1} c_{i, j} - \sum_{j = 1}^{t_j}
            s_{i, j}} \bullet \gtwo{1} + } }
		\end{pchstack}
		% \pcvspace
    
		\begin{pchstack}
			\procedure{$\open(\srs, z, s, f(X))$}
			{
				\p{o}(X) \gets \frac{\p{f}(X) - \p{f}(z)}{X - z}\\ [\myskip]
				\pcreturn \gone{\p{o}(\chi)}\\ [\myskip]
				\hphantom{\hspace*{5.5cm}}	
			}
			
			\pchspace
			
			\procedure{$\verify(\srs, \maxconst, \gone{c}, z, s, \gone{\p{o}(\chi)})$}
      {
        \pcif \gone{\p{o}(\chi)} \bullet \gtwo{\alpha \chi} + \gone{s - z
        \p{o}(\chi)} \bullet \gtwo{\alpha} = \\ [\myskip] \pcind \gone{c}
        \bullet \gtwo{\chi^{- \dconst + \maxconst}} \pcthen  \pcreturn 1\\
        [\myskip]
        \rlap{\pcelse \pcreturn 0.} \hphantom{\pcind \pcif \sum_{i =
            1}^{\abs{\vec{z}}} r_i \cdot \gone{\sum_{j = 1}^{t_j} \gamma_i^{j -
              1} c_{i, j} - \sum{j = 1}^{t_j} s_{i, j}} \bullet \gtwo{1} + } }
		\end{pchstack}
	\end{pcvstack}
	
	\caption{$\PCOMs$ polynomial commitment scheme.}
	\label{fig:pcoms}
\end{figure}
  
We emphasize the following properties of a secure polynomial commitment
$\PCOM$:
\begin{description}
\item[Evaluation binding:] A $\ppt$ adversary $\adv$ which outputs a commitment
  $\vec{c}$ and evaluation points $\vec{z}$ has at most negligible chances to open
  the commitment to two different evaluations $\vec{s}, \vec{s'}$. That is, let
  $k \in \NN$ be the number of committed polynomials, $l \in \NN$ number of
  evaluation points, $\vec{c} \in \GRP^k$ be the commitments, $\vec{z} \in
  \FF_p^l$ be the arguments the polynomials are evaluated at, $\vec{s},\vec{s}'
  \in \FF_p^k$ the evaluations, and $\vec{o},\vec{o}' \in \FF_p^l$ be the
  commitment openings. Then for every $\ppt$ adversary $\adv$
	\[
		\Pr
			\left[
			\begin{aligned}
				& \verify(\srs, \vec{c}, \vec{z}, \vec{s}, \vec{o}) = 1,  \\ 
				& \verify(\srs, \vec{c}, \vec{z}, \vec{s}', \vec{o}') = 1, \\
				& \vec{s} \neq \vec{s}'
			\end{aligned}
			\,\left|\,\vphantom{\begin{aligned}
                  & \\
                  & \\
                  &
                \end{aligned}}
			\begin{aligned}
				& \srs \gets \kgen(\secparam, \maxdeg),\\
				& (\vec{c}, \vec{z}, \vec{s}, \vec{s}', \vec{o}, \vec{o}') \gets \adv(\srs)
			\end{aligned}
			\right.\right] \leq \negl\,.
	\]

\end{description}
	
We say that $\PCOM$ has the unique opening property if the following holds:
\begin{description}
\item[Opening uniqueness:] Let $k \in \NN$ be the number of committed
  polynomials, $l \in \NN$ number of evaluation points, $\vec{c} \in \GRP^k$ be
  the commitments, $\vec{z} \in \FF_p^l$ be the arguments the polynomials are
  evaluated at, $\vec{s} \in \FF_p^k$ the evaluations, and $\vec{o} \in \FF_p^l$
  be the commitment openings. Then for every $\ppt$ adversary $\adv$
	\[
		\Pr
			\left[
			\begin{aligned}
				& \verify(\srs, \vec{c}, \vec{z}, \vec{s}, \vec{o}) = 1,  \\ 
				& \verify(\srs, \vec{c}, \vec{z}, \vec{s}, \vec{o'}) = 1, \\
				& \vec{o} \neq \vec{o'}
			\end{aligned}
			\,\left|\, \vphantom{\begin{aligned}
                  & \\
                  & \\
                  &
                \end{aligned}}
			\begin{aligned}
				& \srs \gets \kgen(\secparam, \maxdeg),\\
				& (\vec{c}, \vec{z}, \vec{s}, \vec{o}, \vec{o'}) \gets \adv(\srs)
			\end{aligned}
			\right.\right] \leq \negl\,.
	\]
\end{description}
Intuitively, opening uniqueness assures that there is only one valid opening
for the committed polynomial and given evaluation point. This property is
crucial in showing special simulation-extractability of $\plonk$ and $\sonic$. We show
that the $\plonk$'s and $\sonic$'s polynomial commitment schemes satisfy this
requirement in \cref{lem:pcomp_unique_op} and \cref{lem:pcoms_unique_op}
respectively.

\begin{description}
\item[Commitment of knowledge]  For every $\ppt$ adversary $\adv$ who produces
  commitment $c$, evaluation point $z$, evaluation $s$ and opening $o$ there
  exists a $\ppt$ extractor $\ext$ such that
\[
  \Pr \left[
    \begin{aligned}
      & \p{f} = \ext_\adv(\srs, c),\\
      & c = \com(\srs, \p{f}),\\
      & \verify(\srs, c, z, s, o) = 1
    \end{aligned}
    \,\left|\,
      \vphantom{
        \begin{aligned}
          & \\
          & \\
          &
        \end{aligned}
        }
    \begin{aligned}
      & \srs \gets \kgen(\secparam, \maxdeg),\\
      & (c, z, s, o) \gets \adv(\srs)
    \end{aligned}
  \right.\right]
  \geq 1 - \epsk(\secpar).
\]
In that case we say that $\PCOM$ is $\epsk$-knowledge.
\end{description}
Intuitively when a commitment scheme is a commitment of knowledge then if an
adversary produces a (valid) commitment $c$, which it can open, then it also
knows the underlying polynomial $\p{f}$ which commits to that value.
\cite{CCS:MBKM19} shows, using AGM, that $\PCOMs$ is a commitment of knowledge.
The same reasoning could be used to show that property for $\PCOMp$. We skip a
proof for that fact due to the lack of space.


\subsection{Zero knowledge}
In a zero-knowledge proof system, a prover convinces the verifier of veracity of
a statement without leaking any other information. The zero-knowledge property
is proven by constructing a simulator that can simulate the view of a cheating
verifier without knowing the secret information---witness---of the prover. A
proof system has to be sound as well, i.e.~for a malicious prover it should be
infeasible to convince a verifier on a false statement. Here, we focus on proof
systems that guarantee soundness against $\ppt$ malicious provers.

More precisely, let $\RELGEN(\secparam) = \smallset{\REL}$ be a family of
$\npol$ relations. Denote by $\LANG_\REL$ the language determined by $\REL$. Let
$\prover$ and $\verifier$ be $\ppt$ algorithms, the former called \emph{prover}
and the latter \emph{verifier}. We allow our proof system to have a setup,
i.e.~there is a $\kgen$ algorithm that takes as input the relation description
$\REL$ and outputs a common reference string $\srs$. We denote by
$\ip{\prover(\REL, \srs, \inp, \wit)}{\verifier(\REL, \srs,\inp)}$ a
\emph{transcript} (also called \emph{proof}) $\zkproof$ of a conversation
between $\prover$ with input $(\REL, \srs, \inp, \wit)$ and $\verifier$ with
input $(\REL, \srs, \inp)$. We write
$\ip{\prover (\REL, \srs, \inp, \wit)}{\verifier(\REL, \srs, \inp)} = 1$ if in
the end of the transcript the verifier $\verifier$ returns $1$ and say that
$\verifier$ accepts it. We sometimes abuse notation and write
$\verifier(\REL, \srs, \inp, \zkproof) = 1$ to denote a fact that $\zkproof$ is
accepted by the verifier. (This is especially handy when the proof system is
non-interactive, i.e.~the whole conversation between the prover and verifier
consists of a single message $\zkproof$ sent by $\prover$).

A proof system $\proofsystem = (\kgen, \prover, \verifier, \simulator)$ for $\RELGEN$ is required to have three properties: completeness, soundness and zero knowledge, which are defined as follows:
% \begin{description}
\paragraph{Completeness.}
%\item[Completeness]
  An interactive proof system $\proofsystem$ is
  \emph{complete} if an honest prover always convinces an honest verifier, that
  is for all $\REL \in \RELGEN(\secparam)$ and $(\inp, \wit) \in \REL$
	\[
		\condprob{\ip{\prover (\REL, \srs, \inp, \wit)}{\verifier (\REL, \srs,
        \inp)} = 1}{\srs \gets \kgen(\REL)} = 1\,.
	\]
    % \item[Soundness]
\paragraph{Soundness.}
    We say that $\proofsystem$ for $\RELGEN$ is \emph{sound} if no
  $\ppt$ prover $\adv$ can convince an honest verifier $\verifier$ to accept a
  proof for a false statement $\inp \not\in\LANG$. More precisely, for
  all $\REL \in \RELGEN(\secparam)$
	\[
      \condprob{\ip{\adv(\REL, \srs, \inp)}{\verifier(\REL, \srs, \inp)} =
        1}{\srs \gets \kgen(\REL), \inp \gets \adv(\REL, \srs); \inp \not\in
        \LANG_\REL} \leq \negl\,;
	\]
%\end{description}
Sometimes a stronger notion of soundness is required---except requiring that the
verifier rejects proofs of statements outside the language, we request from the
prover to know a witness corresponding to the proven statement. This property is
formalised by the so-called \emph{knowledge soundness}.
%\begin{description}
%\begin{description}
%\item[Knowledge soundness]
That is, we call an interactive proof system $\proofsystem$
\emph{knowledge-sound} if for any $\REL \in \RELGEN(\secparam)$ and a $\ppt$
adversary $\adv$
	% \begin{multline*}
	\[
	\Pr\left[
		\begin{aligned}
			& \verifier(\REL, \srs, \inp, \zkproof) = 1, \\
			& \REL(\inp, \wit) = 0
	 \end{aligned}
	  \,\left|\,
	 \begin{aligned}
		 & \srs \gets \kgen(\REL), \inp \gets \adv(\REL, \srs), \\
		 & (\wit, \zkproof) \gets \ext^{\ip{\adv(\REL, \srs, \inp)}{\verifier(\REL, \srs, \inp)}}(\REL, \inp)
	 \end{aligned}
	 \vphantom{\begin{aligned}
		 \adv (\zkproof) = 1, \\
		 \text{if $\zkproof{}$ is accepting} \\
		 \pcind \text{then $\REL(\inp, \wit)$}
	 \end{aligned}}\right.
	 \right] \leq \negl\,,
 % \end{multline*}
 \]
 % \end{description}

 % Usually the verifier verifies messages send by the prover by checking a number
 % of equations depend on the instance, SRS and the proof sent. These equations
 % are often called \emph{verification equations} and denoted $\vereq_i$, for $i$
 % being an index of the equation. It is usually required that an acceptable proof
 % yields $\vereq_i = 0$. In the proof systems we consider---$\plonk$ and
 % $\sonic$---verification equations can be seen as polynomials evaluated at the
 % trapdoor $\chi$. Thus, the verifier checks that $\vereq_i(\chi) =
 % 0$. Sometimes, we consider an \emph{idealised verifier},
 % cf.~\cite{EPRINT:GabWilCio19}, who instead of checking that polynomial
 % $\vereq_i(X)$ evaluates to $0$ at $\chi$ just checks that $\vereq_i(X)$ is a
 % zero polynomial.

 \paragraph{Zero knowledge.}
 We call a proof system $\proofsystem$ \emph{zero-knowledge} if for any
 $\REL \in \RELGEN(\secparam)$, $(\inp, \wit) \in \REL$, and adversary $\adv$
 there exists a $\ppt$ simulator $\simulator$ such that
	\begin{multline*}
	  \left\{\ip{\prover(\REL, \srs, \inp, \wit)}{\adv(\REL, \srs, \inp, \wit)}
      \,\left|\, \srs \gets \kgen(\REL)\COMMENT{, (\inp, \wit) \gets \adv(\REL,
          \srs)}\vphantom{\simulator^\adv}\right.\right\} \approx_\secpar
		%\\
		\left\{\simulator^{\adv}(\REL, \srs, \inp)\,\left|\, \srs \gets
        \kgen(\REL)\COMMENT{, (\inp, \wit) \gets \adv(\REL,
          \srs)}\vphantom{\simulator^\adv}\right.\right\}\,.
\end{multline*}
	%
We call zero knowledge \emph{perfect} if the distributions are equal and
\emph{computational} if they are indistinguishable for any $\nuppt$
distinguisher.

%\end{description}
An SRS $\srs$ comes with a secret string called \emph{trapdoor} $\td$ that
allows the simulator to produce a simulated proof. In that case algorithm
$\kgen(\REL)$ outputs $(\srs, \td)$ and $\td$ is given to the simulator. In this
paper we distinguish simulators that requires a trapdoor to simulate and those
that do not. We call the former \emph{SRS-simulators} and denote them by
$\simulator_\td$.

Occasionally, a weaker version of zero knowledge is sufficient. So called
\emph{honest verifier zero knowledge} (HVZK) assumes that the verifier's
challenges are picked at random from some predefined set. Although weaker, this
definition suffices in many applications. Especially, an interactive
zero-knowledge proof that is HVZK and \emph{public-coin} (i.e.~the verifier
outputs as challenges its random coins) can be made non-interactive and
zero-knowledge in the random oracle model by using the Fiat--Shamir
transformation.

\subsubsection{Idealised verifier and verification equations}
Let $(\kgen, \prover, \verifier)$ be a protocol -- in this paper, either a proof
systems or a polynomial commitment scheme. Observe that the $\kgen$ algorithm
provides an SRS which can be interpreted as a set of group representation of
polynomials evaluated at trapdoor elements. E.g.~for a trapdoor $\chi$ the SRS
contains $\gone{\p{p_1}(\chi), \ldots, \p{p_k}(\chi)}$, for some polynomials
$\p{p_1}(X), \ldots, \p{p_k}(X) \in \FF_p[X]$. On the other hand, the verifier
$\verifier$ accepts if a verification equation $\vereq$ (or a set of them),
which can also be interpreted as a polynomial in $\FF_p[X]$ and which
coefficients depend on messages sent by the prover, zeroes at $\chi$. Following
\cite{EPRINT:GabWilCio19} We call verifiers who checks that $\vereq(\chi) = 0$
\emph{real verifiers} as opposed to \emph{ideal verifiers} who accepts only when
$\vereq(X) = 0$. That is, while a real verifier accepts when a polynomial
\emph{evaluates} to zero, an ideal verifier accepts only when the polynomial
\emph{is} zero.

Although ideal verifiers are impractical, the are very useful in our
proofs. More precisely, we show that
\begin{enumerate}[(1)]
\item the idealised verifier accepts an incorrect proof (what ``incorrect''
  means depends on the situation) with at most negligible probability (and many
  cases---never);
\item when the real verifier accepts, but not the idealised one, then we show
  how to use a malicious $\prover$ to break the underlying security assumption
  (in our case---a variant of $\dlog$.)
\end{enumerate}

\subsubsection{Sigma protocols}
A sigma protocol $\sigmaprot = (\prover, \verifier, \simulator)$ for a relation
$\REL \in \RELGEN(\secparam)$ is a special case of an interactive proof which
transcript compounds of three messages $(a, b, z)$, the middle being a challenge
provided by the verifier. Sigma protocols are honest verifier zero-knowledge in
the standard model and specially-sound. That is, there exists an extractor
$\ext$ which given two accepting transcripts $(a, b, z)$, $(a, b', z')$ for a
statement $\inp$ can recreate the corresponding witness if $b \neq b'$.
Formally,
%\begin{description}
\paragraph{Special soundness.} A sigma protocol $\sigmaprot$ is \emph{specially-sound}
  if for any adversary $\adv$ the probability
	\[
		\Pr\left[
		\begin{aligned}
				& \wit \gets \ext(\REL, \inp, (a, b, z), (a, b', z')),\\
				& \REL(\inp, \wit) = 0
		\end{aligned}
		\,\left|\,
		\begin{aligned}
          & (\inp, (a, b, z), (a, b', z')) \gets \adv(\REL), %\\
           b \neq b',  \\
          & \verifier(\REL, \inp, (a, b, z)) = %\\
            \verifier(\REL, \inp, (a, b', z')) = 1, \\
		\end{aligned}
		\right.\right] \leq  \negl\,.
	\]
%\end{description}

Another property that sigma protocols may have is a unique response
property \cite{C:Fischlin05} which states that no $\ppt$ adversary can
produce two accepting transcripts that differ only on the last
element. More precisely, 
%\begin{description} 
\paragraph{Unique response property.} Let
$\sigmaprot = (\prover, \verifier, \simulator)$ be a sigma-protocol for
$\REL \in \RELGEN(\secparam)$ which proofs compound of three messages
$(a, b, z)$. We say that $\sigmaprot$ is has a unique response property if for
all $\ppt$ algorithms $\adv$ holds
\[ \condprob{\verifier (\REL, \inp, (a, b, z)) = \verifier (\REL, \inp, (a, b,
    z')) = 1}{(\inp, a, b, z, z') \gets \adv(\REL), z \neq z'} \leq \negl\,.  \]
%\end{description} 
If this property holds even against unbounded adversaries, it is called
\emph{strict}, cf.~\cite{INDOCRYPT:FKMV12}. Later on we call protocols that
follows this notion \emph{ur-protocols}. For the sake of completeness we note
that many sigma protocols, like e.g.~Schnorr's protocol \cite{C:Schnorr89},
fulfil this property.

\subsection{From interactive to non-interactive---the Fiat--Shamir transform}
Consider a $(2\mu + 1)$-message, public-coin, honest verifier zero-knowledge
interactive proof system
$\proofsystem = (\kgen, \prover, \verifier, \simulator)$ for
$\REL \in \RELGEN(\secparam)$.  Let $\zkproof$ be a proof performed by the
prover $\prover$ and verifier $\verifier$ compound of messages
$(a_1, b_1, \ldots, a_{\mu - 1}, b_{\mu}, a_{\mu + 1})$, where $a_i$ comes from
$\prover$ and $b_i$ comes from $\verifier$.  Denote by $\ro$ a random oracle.
Let $\proofsystem_\fs = (\kgen_\fs, \prover_\fs, \verifier_\fs, \simulator_\fs)$
be a proof system such that
\begin{itemize}
  \item $\kgen_\fs$ behaves as $\kgen$.
  \item $\prover_\fs$ behaves as $\prover$ except after sending message
    $a_i$, $i \in \range{1}{\mu}$, the prover does not wait for
    the message from the verifier but computes it locally setting $b_i
    = \ro(\zkproof[0..i])$, where $\zkproof[0..j] = (\inp, a_1, b_1, \ldots,
    a_{j - 1}, b_{j - 1}, a_j)$. (Importantly, $\zkproof[0..\mu + 1] =
    (\inp, \zkproof)$).
  \item $\verifier_\fs$ behaves as $\verifier$ but does not provide
    challenges to the prover's proof. Instead it computes the
    challenges locally as $\prover_\fs$ does. Then it verifies the
    resulting transcript $\zkproof$ as the verifier $\verifier$ would. 
  \item $\simulator_\fs$ behaves as $\simulator$, except when
    $\simulator$ picks challenge $b_i$, $\simulator_\fs$ programs the
    random oracle to output $b_i$ on $\zkproof[0, i]$.
  \end{itemize}

Fiat--Shamir heuristic states that $\proofsystem_\fs$ is zero-knowledge
non-interactive proof system for $\REL \in \RELGEN(\secparam)$.

\subsection{Special simulation extractable NIZKs from sigma protocols}
\label{sec:simext_def}
Real life applications often require from a NIZK proof system to be
non-malleable. That is, no adversary seeing a proof $\zkproof$ for a statement
$\inp$ should be able to provide a new proof $\zkproof'$ related to $\zkproof$.
A strong version of non-malleability is formalised by so-called \emph{simulation extractability}
which assures that no adversary can produce a valid proof without knowing the
corresponding witness. This must hold even if the adversary is allowed to see
polynomially many simulated proofs for any statements it wishes.

\begin{definition}[Special simulation-extractable NIZK, \cite{INDOCRYPT:FKMV12}]
	\label{def:simext}
	Let $\ps = (\kgen, \prover, \verifier, \simulator)$ be a computationally
  special-sound HVZK proof and $\ps_\fs = (\kgen_\fs, \prover_\fs,
  \verifier_\fs, \simulator_\fs)$ be $\ps$ transformed by the Fiat--Shamir
  transform. We say that $\ps_\fs$ is \emph{special simulation-extractable} with
  \emph{extraction error} $\nu$ if for any $\ppt$ adversary $\adv$ that is given
  oracle access to a random oracle $\ro$ and simulator $\simulator_\fs$, and
  produces an accepting transcript of $\ps$ with probability $\accProb$, that is
	\[
		\accProb = \Pr \left[
		\begin{aligned}
			& \verifier_\fs(\REL, \srs, \inp_{\advse}, \zkproof_{\advse}) = 1,\\
			& (\inp_{\advse}, \zkproof_{\advse}) \not\in Q
		\end{aligned}
		\, \left| \,
		\begin{aligned}
			& \srs \gets \kgen(\REL), r \sample \RND{\advse}, \\
			& (\inp_{\advse}, \zkproof_{\advse}) \gets \advse^{\simulator_\fs,
			\ro} (\REL, \srs; r) 
		\end{aligned}
		\right.\right]\,,
	\]
	probability
	\[
		\extProb = \Pr \left[
		\begin{aligned}
			& \verifier_\fs(\REL, \srs, \inp_{\advse}, \zkproof_{\advse}) = 1,\\
			& (\inp_{\advse}, \zkproof_{\advse}) \not\in Q,\\
			& \REL(\inp_{\advse}, \wit_{\advse}) = 1
		\end{aligned}
		\, \left| \,
		\begin{aligned}
			& \srs \gets \kgen(\REL), r \sample \RND{\advse},\\
			& (\inp_{\advse}, \zkproof_{\advse}) \gets \advse^{\simulator_\fs,
			\ro} (\REL, \srs; r) \\
			& \wit_{\advse} \gets \ext_\se (\REL, \srs, \advse, r, \inp_{\advse}, \zkproof_{\advse},
			Q, Q_\ro,) 
		\end{aligned}
		\right.\right]
	\]
	is at at least 
	\[
		\extProb \geq \frac{1}{\poly} (\accProb - \nu)^d - \eps(\secpar)\,,
	\]
	for some polynomial $\poly$, constant $d$ and negligible $\eps$ whenever
  $\accProb \geq \nu$. List $Q$ contains all $(\inp, \zkproof)$ pairs where
  $\inp$ is an instance provided to the simulator by the adversary and
  $\zkproof$ is the simulator's answer. List $Q_\ro$ contains all $\advse$'s
  queries to $\ro$ and $\ro$'s answers.
\end{definition}

Consider a sigma protocol $\sigmaprot = (\prover, \verifier, \simulator)$ that
is specially sound and has a unique response property. Let $\sigmaprot_\fs =
(\prover_\fs, \verifier_\fs, \simulator_\fs)$ be a NIZK obtained by applying the
Fiat--Shamir transform to $\sigmaprot$. Faust et al.~\cite{INDOCRYPT:FKMV12}
show that every such $\sigmaprot_\fs$ is special simulation-extractable. This result is
presented in \cref{sec:forking_lemma} along with the instrumental forking lemma,
cf.~\cite{CCS:BelNev06}.

\subsubsection{Simulation sound NIZKs}
Another notion for non-malleable NIZKs is \emph{simulation soundness} which
allows the adversary to see simulated proof, but, differently to simulation
extractability, does not require an extractor to provide a witness for the
proven statement. Instead, it is only necessary, that an adversary who sees
simulated proofs cannot make verifier accept a proof of an incorrect
statement. More precisely,

\begin{definition}[Simulation soundness]
  	\label{def:simsnd}
    Let $\ps = (\kgen, \prover, \verifier, \simulator)$ be a NIZK proof and
    $\ps_\fs = (\kgen_\fs, \prover_\fs, \verifier_\fs, \simulator_\fs)$ be $\ps$
    transformed by the Fiat--Shamir transform. We say that $\ps_\fs$ is
    \emph{simulation-sound}
    for any $\ppt$ adversary $\adv$ that is given oracle access to a random
    oracle $\ro$ and simulator $\simulator_\fs$, probability
    \[
      \ssndProb =
      \Pr\left[
        \begin{aligned}
          & \verifier_\fs(\REL, \srs, \inp_{\adv}, \zkproof_{\adv}) = 1,\\
          & (\inp_{\advse}, \zkproof_{\advse}) \not\in Q,\\
          & \neg \exists \wit_{\adv}: \REL(\inp_{\adv}, \wit_{\adv}) = 1
        \end{aligned}
        \, \left| \,
          \vphantom{\begin{aligned}
          & \verifier_\fs(\REL, \srs, \inp_{\adv}, \zkproof_{\adv}) = 1,\\
          & (\inp_{\advse}, \zkproof_{\advse}) \not\in Q,\\
          & \neg \exists \wit_{\adv}: \REL(\inp_{\adv}, \wit_{\adv}) = 1
        \end{aligned}}
      \begin{aligned}
        & \srs \gets \kgen(\REL), r \sample \RND{\advse},\\
        & (\inp_{\advse}, \zkproof_{\advse}) \gets \advse^{\simulator_\fs,
          \ro} (\REL, \srs; r)
      \end{aligned}
		\right.  \right]
    \]
    is at most negligible.  List $Q$ contains all $(\inp, \zkproof)$ pairs where
  $\inp$ is an instance provided to the simulator by the adversary and
  $\zkproof$ is the simulator's answer. 
\end{definition}

\begin{remark}
  \label{rem:simext_to_simsnd}
  We note that probability $\ssndProb$ \cref{def:simsnd} can be expressed in
  terms from the simulation-extractability definition. More precisely, the
  condition $\neg \exists \wit: \REL(\inp_\adv, \wit_\adv) = 1$ can be substituted with
  $\REL(\inp_\adv, \wit_\adv) = 0$, where $\wit_\adv$, returned by a possibly unbounded
  extractor is either a witness to $\inp_\adv$ (if there exists any) or $\bot$ (if
  there is none). More precisely
\[
      \ssndProb =
      \Pr\left[
        \begin{aligned}
          & \verifier_\fs(\REL, \srs, \inp_{\adv}, \zkproof_{\adv}) = 1,\\
          & (\inp_{\advse}, \zkproof_{\advse}) \not\in Q,\\
          & \REL(\inp_{\adv}, \wit_{\adv}) = 0
        \end{aligned}
        \, \left| \,
      \begin{aligned}
        & \srs \gets \kgen(\REL), r \sample \RND{\advse},\\
        & (\inp_{\advse}, \zkproof_{\advse}) \gets \advse^{\simulator_\fs,
          \ro} (\REL, \srs; r)\\
        & \wit_{\adv} \gets \ext(\REL, \srs, \advse, r, \inp_{\advse}, \zkproof_{\advse},
			Q, Q_\ro,) 
      \end{aligned}
		\right.  \right],
\]
where the only necessary input to the unbounded extractor $\ext$ is the instance
$\inp_\adv$ (the rest is given for the consistency with the simulation extractability
definition). 
%
With the probabilities in \cref{def:simext} holding regardless of whether the extractor
is unbounded or not, we obtain the following equality
$ \ssndProb = \accProb - \extProb$.
\end{remark}

In \cref{cor:simext_to_ssnd} we show that (under some mild conditions) this is enough
to conjecture that probability $\ssndProb$ is not only at most negligible, but
also, in some parameters, exponentially smaller than $(1 - \extProb)$
(probability of extraction failure in \cref{def:simext}).

\section{Special simulation extractability of multi-message protocols,
  definitions and lemmas}
\label{sec:se_definitions}
Unfortunately, Faust et al.'s result cannot be directly applied in our case
since the protocols we consider have more than three messages, require more than
just two transcripts for the extractor to work and are not special
sound. In this part we generalize the forking lemma, special
soundness, and the unique response property to make them compatible with
multi-message protocols.

\subsection{Generalised forking lemma.}
%\label{sec:forking_lemma}
First of all, although dubbed ``general'', \cref{lem:forking_lemma} is not
general enough for our purpose as it is useful only for protocols where witness
can be extracted from just two transcripts. To be able to extract a witness
from, say, an execution of $\plonkprot$ we need to obtain at least
$\numberofconstrains + 3$ valid proofs, and even more for $\sonicprot$. Here we
propose a generalisation of the general forking lemma that given probability of
producing an accepting transcript $\accProb$ lower-bounds the probability of
generating a \emph{tree of accepting transcripts} $\tree$, which allows to
extract a witness.

\begin{definition}[Tree of accepting transcripts, cf.~{\cite{EC:BCCGP16}}]
	\label{def:tree_of_accepting_transcripts}
	Consider a $(2\mu + 1)$-message interactive proof system $\ps$. An $(n_1,
  \ldots, n_\mu)$-tree of accepting transcript is a tree where each node on
  depth $i$, for $i \in \range{1}{\mu + 1}$, is an $i$-th prover's message in an
  acceptable transcript; edges between the nodes are labeled with verifier's
  challenges, such that no two edges on the same depth have the same
  label; and each node on depth $i$ has $n_{i} - 1$ siblings and $n_{i +
    1}$ children. Altogether, the tree consists of $N = \prod_{i = 1}^\mu n_i$
  branches, which makes $N$ acceptable transcripts. We require $N = \poly$.
\end{definition}


\begin{lemma}[General forking lemma II]
	\label{lem:generalised_forking_lemma}
	Fix $q \in \ZZ$ and set $H$ of size $h \geq m$. Let $\zdv$ be a $\ppt$
  algorithm that on input $y, h_1, \ldots, h_q$ returns $(i, s)$ where $i \in
  \range{0}{q}$ and $s$ is called a side output. Denote by $\ig$ a randomised
  instance generator. We denote by $\accProb$ the probability
	\[
		\condprob{i \neq 0}{ y \gets \ig;\ h_1, \ldots, h_q \sample H;\ (i, s)
		\gets \zdv(y, h_1, \ldots, h_q)}\,.
	\]
	Let $\genforking_{\zdv}^{m}$ denote the algorithm described in
  \cref{fig:genforking_lemma} then the probability $\frkProb := \condprob{b =
    1}{y \gets \ig;\ h_1, \ldots, h_{q} \sample H;\ (b, \vec{s}) \gets
    \genforking_{\zdv}^{m}(y, h_1, \ldots, h_q)}$ is at least
	\[
		\frac{\accProb^m}{q^{m - 1}} - \accProb \cdot \left(1 -
    \frac{h!}{(h - m)! \cdot h^{m}}\right).
	\]
		
	\begin{figure}[t]
		\centering
		\fbox{
		\procedure{$\genforking_{\zdv}^{m} (y,h_1^{1}, \ldots, h_{q}^{1})$}		
		{
		\rho \sample \RND{\zdv}\\
		(i, s_1) \gets \zdv(y, h_1^{1}, \ldots, h_{q}^{1}; \rho)\\
    i_1 \gets i\\
		\pcif i = 0\ \pcreturn (0, \bot)\\
		\pcfor j \in \range{2}{m}\\
		\pcind h_{1}^{j}, \ldots, h_{i - 1}^{j} \gets h_{1}^{j - 1}, \ldots,
		h_{i - 1}^{j - 1}\\
		\pcind h_{i}^{j}, \ldots, h_{q}^{j} \sample H\\
		\pcind (i_j, s_j) \gets \zdv(y, h_1^{j}, \ldots, h_{i - 1}^{j}, h_{i}^{j},
		\ldots, h_{q}^{j}; \rho)\\
		\pcind \pcif i_j = 0 \lor i_j \neq i\ \pcreturn (0, \bot)\\
    \pcif \exists (j, j') \in \range{1}{m}^2, j \neq j' : (h_{i}^{j} = h_{i}^{j'})\
	\pcreturn (0, \bot)\\
		\pcelse \pcreturn (1, \vec{s})
	}}
	\caption{Generalised forking algorithm $\genforking_{\zdv}^{m}$}
	\label{fig:genforking_lemma}
\end{figure}
\end{lemma}
\begin{proof}
First let denote by $\accProb(y)$ and $\frkProb(y)$ the following probabilities
\begin{align*}
\accProb(y) & =  \condprob{i \neq 0}{h_1, \ldots, h_q \sample H;\ (i, s)
\gets \zdv(y, h_1, \ldots, h_q)}\,.\\
	\frkProb(y) & = \condprob{b = 1}{(b, \vec{s}) \gets
\genforking_{\zdv}^{m}(y, h_1, \ldots, h_q)}\,.
\end{align*}

We start by claiming that for all $y$ 
\begin{equation}\label{eq:frkProb_y}
	\frkProb(y) \geq 
	\frac{\accProb(y)^m}{q^{m - 1}} - \accProb(y) \cdot \left(1 -
  \frac{h!}{(h - m)! \cdot h^{m}}\right)
	\end{equation}
Then with the expectation taken over $y \sample \ig$, we have
\begin{align}
	\frkProb & = \expected{\frkProb(y)} \geq
	\expected{\frac{\accProb(y)^m}{q^{m - 1}} -  \accProb(y) \cdot \left(1 -
  \frac{h!}{(h - m)! \cdot h^{m}}\right)} \label{eq:use_eq1}\\
	& \geq \frac{\expected{\accProb(y)}^m}{q^{m - 1}} -
	\expected{\accProb(y)} \cdot \left(1 - \frac{h!}{(h - m)! \cdot
  h^{m}}\right) \label{eq:by_lemma_jensen}\\
	& = \frac{\accProb^m}{q^{m - 1}} -  \accProb \cdot \left(1 -
  \frac{h!}{(h - m)! \cdot h^{m}}\right)\label{eq:by_accProb}\,.
\end{align}
Where \cref{eq:use_eq1} comes from \cref{eq:frkProb_y};
\cref{eq:by_lemma_jensen} comes from linearity of expected value and \cref{lem:jensen}; and
\cref{eq:by_accProb} holds by the fact that $\expected{\accProb(y)} =
\accProb$.

We now show \cref{eq:frkProb_y}.
Denote by $J = \range{1}{m}^2 \setminus \smallset{(j, j)}_{j \in \range{1}{m}}$. 
For any input $y$, with probabilities taken over the coin tosses of
$\genforking_{\zdv}^{m}$ we have
\begin{align*}
	\frkProb (y) & = \prob{i_j = i_{j'} \land i_j \geq 1 \land
h_{i_j}^{j} \neq h_{i_{j'}}^{j'} \text{ for } (j, j') \in J}	\\
	& \geq \prob{i_j = i_{j'} \land i_j \geq 1 \text{ for } (j, j') \in J} %\\
   - \prob{i_j \geq 1 \land h_{i_j}^{j} = h_{i_{j'}}^{j'} \text{ for some } (j, j') \in J}\\
	& = \prob{i_j = i_{j'} \land i_j \geq 1 \text{ for } (j, j') \in J} -
	\prob{i_j \geq 1} \cdot 
  \left(1 - \frac{h!}{(h - m)! \cdot h^{m}}\right) \\ 
	& = \prob{i_j = i_{j'} \land
	i_j \geq 1 \text{ for } (j, j') \in J} - \accProb(y) \cdot \left(1 -
\frac{h!}{(h - m)! \cdot h^{m}}\right)\,.
\end{align*}

Probability that for some $(j, j') \in J$ and $i_j = i_{j'}$ holds
$h_{i_j}^{j} \neq h_{i_{j'}}^{j'}$ equals 
\[
  \frac{h \cdot (h - 1)
\cdot \ldots \cdot (h - m - 1)}{h^m} = \frac{h!}{(h - m)! \cdot h^m}.
\]
That is, it equals the number
of all $m$-element strings where each element is different divided by
the number of all $m$-element strings, where elements are taken from a
set of size $h$. 

It remains to show that $\prob{i_j = i_{j'} \land i_j \geq 1 \text{ for } (j,
  j') \in J} \geq \infrac{\accProb(y)^m}{q^{m - 1}}$. Let $\RND{\zdv}$ denote
the set from which $\zdv$ picks its coins at random. For each $\iota \in
\range{1}{q}$ let $X_\iota \colon \RND{\zdv} \times H^{\iota - 1} \to [0, 1]$ be
defined by setting $X_\iota(\rho, h_1, \ldots, h_{\iota - 1})$ to
\[
  \condprob{i = \iota}{h_\iota, \ldots, h_q \sample H; (i, s) \gets \zdv(y, h_1,
    \ldots, h_q; \rho)}
\]
for all $\rho \in \RND{\zdv}$ and $h_1, \ldots, h_{\iota - 1} \in H$. Consider
$X_\iota$ be a random variable over the uniform distribution on its domain. Then
\begin{align*}
	& \prob{i_j = i_{j'} \land i_j \geq 1 \text{ for } (j, j') \in J} 
	 = \sum_{\iota = 1}^{q} \prob{i_1 = \iota \land \ldots \land i_m = \iota} \\
	& = \sum_{\iota = 1}^{q} \prob{i_1 = \iota} \cdot \condprob{i_2 = \iota}{i_1 = \iota} \cdot \ldots \cdot \condprob{i_m = \iota}{i_1 = \ldots = i_{m - 1} = \iota} \\
	& = \sum_{\iota = 1}^{q} \sum_{\rho, h_1, \ldots, h_{\iota - 1}} X_{\iota}
   (\rho, h_1, \ldots, h_{\iota - 1})^{m} \cdot \frac{1}{\abs{\RND{\zdv}} \cdot \abs{H}^{\iota - 1}}
   = \sum_{\iota = 1}^{q} \expected{X_\iota^m} \,.
\end{align*}
Importantly, $\sum_{\iota = 1}^q \expected{X_{\iota}} = \accProb(y)$.

By \cref{lem:jensen} we get
\[
	\sum_{\iota = 1}^{q} \expected{X_\iota^m} \geq \sum_{\iota = 1}^{q} \expected{X_\iota}^m\,.
\]
Note that for e.g.~$X_i = 1$, $i \in \range{1}{q}$ the inequality becomes equality, that is, it is tight.
 
We now use the H\"older inequality, cf.~\cref{lem:holder}, for $x_i = \expected{X_i}$, $y_i = 1$, $p = m$, and $q = m/(m - 1)$ obtaining
\begin{gather}
	\left(\sum_{i = 1}^{q} \expected{X_i}\right)^{m}  \leq \left(\sum_{i = 1}^{q} \expected{X_i}^m\right) \cdot q^{m - 1}\\
	\frac{1}{q^{m - 1}} \cdot \accProb(y)^{m} \leq \sum_{i = 1}^{q} \expected{X_i}^m\,.
\end{gather}
Finally, we get
\[
	\frkProb(y) \geq \frac{\accProb(y)^m}{q^{m - 1}} - 
	 \accProb(y) \cdot \left(1 - \frac{h!}{(h - m)! \cdot h^m}\right)\,.
\]
\qed
\end{proof}

To highlight importance of the generalised forking lemma we describe how we use
it in our special simulation-extractability proof.  Let $\proofsystem$ be a
computationally special sound proof system where for an instance $\inp$ the
corresponding witness can be extracted from an
$(1, \ldots, 1, n_k, 1, \ldots, 1)$-tree of accepting transcripts.  Let $\advse$
be the simulation-extractability adversary that outputs an acceptable proof with
probability at least $\accProb$. (Although we use the same $\accProb$ to denote
probability of $\zdv$ outputing a non-zero $i$ and probability of $\advse$
outputing an acceptable proof we claim that these probabilities are exactly the
same what comes from how we define $\zdv$.)  Let $\advse$ produce an acceptable
proof $\zkproof_{\advse}$ for instance $\inp_{\advse}$; $r$ be $\advse$'s
randomness; $Q$ the list of queries submitted by $\advse$ along with simulator's
$\simulator_\zkproof$ answers; and $Q_\ro$ be the list of all random oracle
queries made by $\advse$.  All of these are given to the extractor $\ext$ that
internally runs the forking algorithm $\genforking_\zdv^{n_k}$.  Algorithm $\zdv$
takes $(\REL, \srs, \advse,
%\inp_\advse,
%\zkproof_\advse, 
Q, r)$ as input $y$ and $Q_\ro$ as input $h_1^1, \ldots,
h_q^1$. 
(For the sake of completeness, we allow $\genforking_\zdv^{n_k}$ to
pick $h^1_{l + 1}, \ldots, h^1_q$ responses if $Q_\ro$ has only $l < q$
elements.)  

Next, $\zdv$ runs internally $\advse(\REL, \srs; r)$ and responds to its random
oracle and simulator queries by using $Q_\ro$ and $Q$. Note that $\advse$ makes
the same queries as it did before it output $(\inp_{\advse}, \zkproof_{\advse})$
as it is run on the same random tape and with the same answers from the
simulator and random oracle. After $\advse$ finishes its acceptable proof
$\zkproof_{\advse}$, algorithm $\zdv$ outputs $(i, \zkproof_{\advse})$, where
$i$ is the index of a random oracle query submitted by $\advse$ to get the challenge after
$k$-th message from the prover---a message where the tree of transcripts
branches.
Then, after the first run of $\advse$ is done, the extractor runs $\zdv$ again,
but this time it provides fresh random oracle responses $h^2_i, \ldots,
h^2_q$. Note that this is equivalent to rewinding $\advse$ to a point just
before $\advse$ is about to ask its $i$-th random oracle
query. Probability that the adversary produces an acceptable transcript with the
fresh random oracle responses is at least $\accProb$. This continues until the
required number of transcripts is obtained. 

We note that in the original forking lemma the forking algorithm $\forking$,
cf.~\cref{fig:forking_lemma}, gets only as input $y$ and elements $h^1_1, \ldots,
h^1_q$ are randomly picked from $H$ internally by $\forking$. However, assuming
that $h^1_1, \ldots, h^1_q$ are random oracle responses, thus are random, makes
the change only notational.

We also note that the general forking lemma proposed in
\cref{lem:generalised_forking_lemma} works for protocols which have extractable
witness from a $(1, \ldots, 1, n_k, 1, \ldots, 1)$-tree of acceptable
transcripts. This limitation however does not affect the main result of this
paper, i.e.~showing that both $\plonk$ and $\sonic$ are special simulation extractable.

\subsection{Unique-response protocols.}
Another problem comes with another assumption required by Faust et al.---the
unique response property of the transformed sigma protocol. The original
Fischlin's formulation, although suitable for applications presented in
\cite{C:Fischlin05,INDOCRYPT:FKMV12}, is not enough in our case. First of all,
the property assumes that the protocol has three messages, with the middle being
the challenge from the verifier. That is not the case we consider here. Second,
it is not entirely clear how to generalize the property. Should one require that
after the first challenge from the verifier the prover's responses are fixed?
That could not work since the prover needs to answer differently on different
verifier's challenges, as otherwise the protocol could have fewer
rounds. Another problem arises when the protocol contains some
message---obviously, except the first one---where the prover randomises its
message. In that case unique-responsiveness can not hold as well. Last but not
least, the protocols we consider here are not designed to be in the standard
model, but utilises SRS what also complicates things considerably.

We walk around these obstacles by providing a generalised notion of the unique
response property. More precisely, we say that a $(2\mu + 1)$-message protocol
has \emph{unique responses from $i$}, and call it an $\ur{i}$-protocol, if it
follows the definition below:

\begin{definition}[$\ur{i}$-protocol]
\label{def:wiur}
Let $\proofsystem$ be a $(2\mu + 1)$-message proof system $\ps = (\kgen,
\prover, \verifier, \simulator)$. Let $\proofsystem_\fs$ be $\proofsystem$ after the
Fiat--Shamir transform, Denote by $a_1, \ldots, a_{\mu}, a_{\mu + 1}$ protocol messages
output by the prover, We say that $\proofsystem$ has \emph{unique responses
  from $i$ on} if for any $\ppt$ adversary $\adv$:
\[
	\prob{
		\begin{aligned}
		&	\inp, \vec{a} = (a_1, \ldots, a_{\mu + 1}), \vec{a'} = (a'_1, \ldots,
    a'_{\mu + 1})
		\gets \adv^\ro(\REL, \srs), \\
    & \vec{a} \neq \vec{a'}, a_1, \ldots, a_{i} = a'_1,
    \ldots, a'_{i}, \\
		& \verifier^\ro_\fs (\REL, \srs, \inp, \vec{a}) =
		\verifier^\ro_\fs(\REL, \srs, \inp, \vec{a'}) = 1
		\end{aligned}
		\ \left|\  
	\vphantom{\begin{aligned}
	&	\vec{a} = (a_0, b_0, \ldots, a_j, b_j, a_\mu), \vec{a'} = (a'_0, b'_0, \ldots, a'_j,
	b'_j a'_\mu) \gets \adv(\REL, \srs), \vec{a} \neq \vec{a'}, \\
	& b_k = b'_k, k \in \range{1, \mu - 1},\\ a_l = a'_l, l \in
\range{1}{j}, j > i 
	\end{aligned}}
\srs \gets \kgen_\fs(\REL) \right.
} \leq \negl.
\]
\end{definition}
Intuitively, a protocol is $\ur{i}$ if it is infeasible for a $\ppt$ adversary
to produce a pair of acceptable and different proofs $\zkproof$, $\zkproof'$
that are the same on  first $i$ messages. 
% after $i$-th prover's message, all
% $\prover$'s further messages are determined by the witness it knows, the
% messages it already send and received and the future challenges from the
% verifier.
We note that the definition above is independent on whether the proof
system $\proofsystem$ utilises SRS (and compounds of the SRS-generating $\kgen$
algorithm) or not.

\subsection{Computational special soundness}
Note that the special soundness property (as usually defined) holds for
all---even computationally unbounded---adversaries. Unfortunately, since a
simulation trapdoors for $\plonkprot$ and $\sonicprot$ exist, the protocols
cannot be special sound in that regard. This comes since an unbounded adversary
could reveal the trapdoor and build a number of simulated proofs for a fake
statement. Hence, we provide a weaker, yet sufficient, definition of
\emph{computational special soundness}. More precisely, we state that an
adversary that is able to answer correctly multiple challenges either knows the
witness or can be used to break some computational assumption.

\begin{definition}[Computational special soundness]
  Let $\proofsystem = (\kgen, \prover, \verifier, \simulator)$ be an
  $(2 \mu + 1)$-message proof system for a relation $\REL$. We say that
  $\proofsystem$ is $(\epsss, (n_1, \ldots, n_\mu))$-\emph{computationally special sound}
  if there exists an extractor $\extt$ that given an $(n_1, \ldots, n_\mu)$-tree
  of acceptable transcripts $\tree$ and instance $\inp$ output by some $\ppt$ adversary $\adv(\REL,
  \srs)$, for $\srs \sample \kgen(\REL)$, outputs $\wit$ such that $\REL(\inp,
  \wit) = 1$ with probability at least $1 - \epsss$.
\end{definition}

Since we do not utilise the classical special soundness (that holds for all,
even unbounded, adversaries) all references to that property should be
understood as references to its computational version.

\section{Special simulation-extractability---the general result}
\label{sec:general}
Equipped with definitional framework of \cref{sec:se_definitions} we are ready
to present the main result of this paper---special simulation extractability of
multi-round protocols.

\begin{theorem}[Special simulation-extractable multi-message protocols]
  \label{thm:se}
  Let $\ps = (\kgen, \prover, \verifier, \simulator)$ be an interactive
  $(2 \mu + 1)$-message proof system for $\RELGEN(\secparam)$ that is honest
  verifier zero-knowledge in the standard model\footnote{Crucially, we require
    that one can provide an indistinguishable simulated proof without any
    additional knowledge, as e.g~knowledge of a SRS trapdoor.}, has $\ur{k}$
  property with security $\epsur$, is $(n_1, \ldots, n_\mu)$-special sound for
  $n_i = 1, i \in \range{1}{\mu} \setminus \smallset{k}$ and $n_k = n$.

Let $\ro\colon \bin^{*} \to \bin^{\secpar}$ be a random oracle. 
Then $\psfs$ is special simulation-extractable with extraction error $\epsur$
against $\ppt$ algebraic adversaries that makes up to $q$ random oracle queries and
returns an acceptable proof with probability at least $\accProb$. 
The extraction probability $\extProb$ is at least
\[
	\extProb \geq \frac{1}{q^{n - 1}} (\accProb - \epsur)^{n} -\eps\,,
\]
for some negligible $\eps$.	
\end{theorem}
\begin{proof}		
  The proof goes by game hoping. The games are controlled by an environment
  $\env$ that internally runs a simulation extractability adversary $\advse$,
  provides it with access to a random oracle and simulator, and when necessary
  rewinds it. The games differ by various breaking points, i.e.~points where the
  environment decides to abort the game.

  Denote by $\zkproof_{\advse}, \zkproof_{\simulator}$ proofs returned by the
  adversary and the simulator respectively. We use $\zkproof[i]$ to denote
  prover's message in the $i$-th round of the proof (counting from 1), i.e.~$(2i
  - 1)$-th message exchanged in the protocol. $\zkproof[i].\ch$ denotes the
  challenge that is given to the prover after $\zkproof[i]$, and
  $\zkproof[i..j]$ to denote all messages of the proof including challenges
  between rounds $i$ and $j$, but not challenge $\zkproof[j].\ch$. When it is
  not explicitly stated we denote the proven instance $\inp$ by $\zkproof[0]$
  (however, there is no following challenge $\zkproof[0].\ch$).

  Without loss of generality, we assume that whenever the accepting proof
  contains a response to a challenge from a random oracle, then the
  adversary queried the oracle to get it. It is straightforward to transform any
  adversary that violates this condition into an adversary that makes these
  additional queries to the random oracle and wins with the same probability.

  \ngame{0} This is a simulation extraction game played between an adversary
  $\advse$ who has given access to a random oracle $\ro$ and simulator
  $\psfs.\simulator$. There is also an extractor $\ext$ that, from a proof
  $\zkproof_{\advse}$ for instance $\inp_{\advse}$ output by the adversary and from
   transcripts of $\advse$'s operations is tasked to extract a witness
  $\wit_{\advse}$ such that $\REL(\inp_{\advse}, \wit_{\advse})$ holds. $\advse$ wins
  if it manages to produce an acceptable proof and the extractor fails to reveal
  the corresponding witness. In the following game hops we upper-bound the
  probability that this happens.

  \ngame{1} This is identical to $\game{0}$ except that now the game is aborted
  if there is a simulated proof $\zkproof_\simulator$ for $\inp_{\advse}$ such
  that $(\inp_{\advse}, \zkproof_\simulator[1..k]) = (\inp_{\advse},
  \zkproof_{\advse}[1..k])$. That is, the adversary in its final proof
  reuses a part of a simulated proof it saw before and the proof is acceptable.
  Denote that event by $\event{\errur}$.

  \ncase{$\game{0} \mapsto \game{1}$} We have, \( \prob{\game{0} \land
    \nevent{\errur}} = \prob{\game{1} \land \nevent{\errur}} \) and, from the
  difference lemma, cf.~\cref{lem:difference_lemma},
  \[ \abs{\prob{\game{0}} - \prob{\game{1}}} \leq \prob{\event{\errur}}\,. \]
  Thus, to show that the transition from one game to another introduces only
  minor change in probability of $\advse$ winning it should be shown that
  $\prob{\event{\errur}}$ is small.

  We can assume that $\advse$ queried the simulator on the instance it wishes to
  output---$\inp_{\advse}$. We show a reduction $\rdvur$ that utilises $\advse$,
  who outputs a valid proof for $\inp_{\advse}$, to break the $\ur{k}$ property of
  $\ps$. Let $\rdvur$ run $\advse$ internally as a black-box:
\begin{itemize}
	\item The reduction answers both queries to the simulator $\psfs.\simulator$ and to the random oracle. 
	It also keeps lists $Q$, for the simulated proofs, and $Q_\ro$ for the random oracle queries. 
\item When $\advse$ makes a fake proof $\zkproof_{\advse}$ for $\inp_{\advse}$,
  $\rdvur$ looks through lists $Q$ and $Q_\ro$ until it finds
  $\zkproof_{\simulator}[0..k]$ such that
  $\zkproof_{\advse}[0..k] = \zkproof_{\simulator}[0..k]$
  and a random oracle query $\zkproof_{\simulator}[k].\ch$ on
  $\zkproof_{\simulator}[0..k]$.
	\item $\rdvur$ returns two proofs for $\inp_{\advse}$:
	\begin{align*}
		\zkproof_1 = (\zkproof_{\simulator}[1..k],
		\zkproof_{\simulator}[k].\ch, \zkproof_{\simulator}[k + 1..\mu + 1])\\
		\zkproof_2 = (\zkproof_{\simulator}[1..k],
		\zkproof_{\simulator}[k].\ch, \zkproof_{\advse}[k + 1..\mu + 1])
	\end{align*}
	\end{itemize}  
	If $\zkproof_1 = \zkproof_2$, then $\advse$ fails to break simulation
  extractability, as $\zkproof_2 \in Q$. On the other hand, if the proofs are
  not equal, then $\rdvur$ breaks $\ur{k}$-ness of $\ps$. This happens only with
  negligible probability $\epsur$, hence \( \prob{\event{\errur}} \leq
  \epsur\,. \)
	
  \ngame{2} This is identical to $\game{1}$ except that now the environment
  aborts also when it fails to build a $(1, \ldots, 1, n, 1, \ldots, 1)$-tree
  of accepting transcripts $\tree$ by rewinding $\advse$. Denote that event by
  $\event{\errfrk}$. 

  \ncase{$\game{1} \mapsto \game{2}$} Note that for every acceptable proof
  $\zkproof_{\advse}$, we may assume that whenever $\advse$ outputs in Round $k$
  message $\zkproof_{\advse}[k]$, then the
  $(\inp_{\advse}, \zkproof_{\advse}[1..k])$ random oracle query was made
  by the adversary, not the simulator\footnote{\cite{INDOCRYPT:FKMV12} calls
    these queries \emph{fresh}.}, i.e.~there is no simulated proof
  $\zkproof_\simulator$ on $\inp_\simulator$ such that
  $(\inp_{\advse}, \zkproof_{\advse} [1..k]) = (\inp_\simulator,
  \zkproof_\simulator[1..k])$. Otherwise, the game would be already interrupted
  by the error event in Game $\game{1}$.  As previously,
\[
  \abs{\prob{\game{1}} - \prob{\game{2}}} \leq \prob{\event{\errfrk}}\,.
\]

We describe our extractor $\ext$ here. The extractor takes as input relation
$\REL$, SRS $\srs$, $\advse$'s code, its randomness $r$, the output instance
$\inp_{\advse}$ and proof $\zkproof_{\advse}$, as well as the list $Q$ of
simulated proofs (and their instances) and the list of random oracle queries and
responses $Q_\ro$. Then, $\ext$ starts a forking algorithm
$\genforking^{n}_\zdv(y,h_1, \ldots, h_q)$ for
$y = (\REL, \srs, \advse, r, \inp_{\advse}, \zkproof_{\advse}, Q)$ where we set
$h_1, \ldots, h_q$ to be the consecutive queries from list $Q_\ro$. We run
$\advse$ internally in $\zdv$.% which returns the proof $\zkproof$ and index $i$
%of the random oracle query that $\advse$ used to answer $\zkproof$'s $k$-th challenge. 

To assure that in the first execution of $\zdv$ the adversary $\advse$ produce
the same $(\inp_{\advse}, \zkproof_{\advse})$ as in the extraction game, $\zdv$
provides $\advse$ with the same randomness $r$ and answers queries to the random
oracle and simulator with pre-recorded responses in $Q_\ro$ and $Q$.
%
Note, that since the view of the adversary when run inside $\zdv$ is the same as its
view with access to the real random oracle and simulator, it produces exactly the
same output. After the first run, $\zdv$ outputs the index $i$ of a random oracle
query that was used by $\advse$ to compute the challenge $\zkproof[k].\ch =
\ro(\zkproof_{\advse}[0..k])$ it had to answer in the $(k + 1)$-th round and
adversary's transcript, denoted by $s_1$ in $\genforking$'s description. If no
such query took place $\zdv$ outputs $i = 0$.

Then new random oracle responses are picked for queries indexed by
$i, \ldots, q$ and the adversary is rewound to the point just prior to when it gets the
response to RO query $\zkproof_{\advse}[0..k]$. The adversary gets a random
oracle response from a new set of responses $h^2_i, \ldots, h^2_q$. If the
adversary requests a simulated proof after seeing $h^2_i$ then $\zdv$ computes
the simulated proof on its own. Eventually, $\zdv$ outputs index $i'$ of a query
that was used by the adversary to compute $\ro(\zkproof_{\advse}[0..k])$, and a
new transcript $s_2$. $\zdv$ is run $n + 1$ times with different random oracle
responses. If a tree $\tree$ of $n + 1$ transcripts is build then $\ext$
runs internally the tree extractor $\extt(\tree)$ and outputs what it returns.

We emphasize here the importance of the unique response property. If it does not
hold then in some $j$-th execution of $\zdv$ the adversary could reuse a
challenge that it learned from observing proofs in $Q$. In that case, $\zdv$
would output $i = 0$, making the extractor fail. Fortunately, the case that the
adversary breaks the unique response property has already been covered by the
abort condition in $\game{1}$.

Denote by $\waccProb$ the probability that $\advse$ outputs a proof that is
accepted and does not break $\ur{k}$-ness of $\ps$.  Denote by $\waccProb'$ the
probability that algorithm $\zdv$, defined in the lemma, produces an accepting
proof with a fresh challenge after Round $k$. Given the discussion above, we can
state that $\waccProb = \waccProb'$.

Next, from the generalised forking lemma, cf.~\cref{lem:generalised_forking_lemma}, we get that
\begin{equation}
  \begin{split}
    \prob{\event{\errfrk}} \leq 1 - \waccProb \cdot \left(\frac{\waccProb^{n -
          1}}{q^{n - 1}} + \frac{(2^\secpar) !}{(2^\secpar - n)! \cdot
        (2^\secpar)^{n}} - 1\right) = \\
    1 - \left(\frac{\waccProb^{n}}{q^{n - 1}} + 
      \waccProb \cdot \frac{(2^\secpar) !}{(2^\secpar - n)! \cdot
        (2^\secpar)^{n}} - \waccProb\right)\,.
\end{split}
\end{equation}
% For the sake of simplicity we loose this approximation a bit and state
% \[
% 	\prob{\event{\errfrk}} \leq 1 -
% 	\left(\frac{\waccProb^{n}}{q^{n - 1}} +
% 		\waccProb \cdot \left(\frac{2^\secpar - n}{2^\secpar}\right)^{n} -
% 	\waccProb\right)\,.
% \]
\ngame{3} This game is identical to $\game{2}$ except that it aborts if
$\extt(\tree)$ run by $\ext$ fails to extract the witness. Denote that event by
$\event{\errss}$.

\ncase{$\game{2} \mapsto \game{3}$}	
As previously, 
\[
	\abs{\prob{\game{2}} - \prob{\game{3}}} \leq \event{\errss}\,.
\]
Since $\ps$ is special-sound the probability of $\event{\errss}$ is
upper-bounded by some negligible $\eps_\ss$.

Game $\game{3}$ is aborted when it is impossible to extract the correct witness
from $\tree$, hence the adversary $\advse$ cannot win.  Thus, by the game-hoping
argument,
\[
	\abs{\prob{\game{0}} - \prob{\game{3}}} \leq 1 -
	\left(\frac{\waccProb^{n}}{q^{n - 1}} + \waccProb \cdot
	\frac{(2^\secpar) !}{(2^\secpar - n)! \cdot
        (2^\secpar)^{n}} - \waccProb\right) + \epsur + \epsss\,.
\]
Thus the probability that extractor $\extss$ succeeds is at least
\[
	\frac{\waccProb^{n}}{q^{n - 1}} + 
	\waccProb \cdot
	\frac{(2^\secpar) !}{(2^\secpar - n)! \cdot
        (2^\secpar)^{n}} -
\waccProb - \epsur - \epsss\,.
\]
Since $\waccProb$ is probability of $\advse$ outputting acceptable transcript
that does not break $\ur{k}$-ness of $\ps$, then $\waccProb \geq \accProb -
\epsur$, where $\accProb$ is the probability of $\advse$ outputing an acceptable
proof as defined in \cref{def:simext}. It thus holds
\[
	\label{eq:frk}
	\extProb \geq \frac{(\accProb - \epsur)^{n}}{q^{n - 1}} -
	\underbrace{(\accProb - \epsur) \cdot \left( 1 -
      \frac{(2^\secpar) !}{(2^\secpar - n)! \cdot
        (2^\secpar)^{n}}\right)
- \epsur - \epsss}_{\eps}\,.
\]
Note that the part of \cref{eq:frk} denoted by $\eps$ is negligible as
$\epsur, \epsss$ are negligible, and $\frac{(2^\secpar) !}{(2^\secpar - n)! \cdot
        (2^\secpar)^{n}} \geq \left(\infrac{(2^\secpar
- n)}{2^\secpar}\right)^{n}$ is overwhelming.
Thus, 
\[
	\extProb \geq \frac{1}{q^{n - 1}} (\accProb - \epsur)^{n} -\eps\,.
\] 
thus
$\psfs$ is special simulation extractable with extraction error $\epsur$.
\qed
\end{proof}

\paragraph{Inefficient simulation extractability gives efficient simulation soundness.}
As noted in \cref{sec:simext_def}, simulation soundness can be expressed in
terms of simulation extractability with an  unbounded extractor. Holds then,

\begin{corollary}[Simulation extractability to simulation soundness.]
  \label{cor:simext_to_ssnd}
  Let $\ps$, as defined in \cref{thm:se} be simulation-extractable with
	\[
    \extProb \geq \frac{(\accProb - \epsur)^{n}}{q^{n - 1}} -
	(\accProb - \epsur) \cdot \left( 1 -
      \frac{(2^\secpar) !}{(2^\secpar - n)! \cdot
        (2^\secpar)^{n}}\right)
    - \epsur - \epsss,
  \]
  then it is simulation sound with
  $\ssndProb \leq 2\epsur + \epsss 2^{-\secpar}(\accProb - \epsur)$
\end{corollary}
\begin{proof}
  Since according to the discussion in \cref{rem:simext_to_simsnd} $\ssndProb =
  \accProb - \extProb$ it holds:
  \begin{align*}
    \ssndProb \leq \accProb - \left(\frac{(\accProb - \epsur)^{n}}{q^{n - 1}} -
	(\accProb - \epsur) \cdot \left( 1 -
      \frac{(2^\secpar) !}{(2^\secpar - n)! \cdot
        (2^\secpar)^{n}}\right)
    - \epsur - \epsss,\right).
  \end{align*}
  Furthermore, as the extractor $\ext$ from \cref{rem:simext_to_simsnd} is unbounded, we can
  assume that it is able to extract the witness from a single run of a
  simulation-extractability adversary $\adv$. hence $n = 1$. Thus,
  \begin{equation*}
    \begin{split}
      \ssndProb & \leq \accProb - \left(\accProb - \epsur -
                  (\accProb - \epsur) \cdot \left( 1 -
                  1)\right)
                  - \epsur - \epsss\right) \\
                & =  2 \epsur 
                  + \epsss \\
                \end{split}
              \end{equation*}
              \qed
\end{proof}


\section{Special simulation extractability of $\plonkprotfs$} 
\label{sec:plonk}
In this section we show that $\plonkprotfs$ is special simulation-extractable. To that
end, we proceed as follows. First we show that the version of the KZG polynomial
commitment scheme that is proposed in the \plonk{} paper has the unique opening
property, cf.~\cref{sec:poly_com} and \cref{lem:pcomp_unique_op}. This is then
used to show that $\plonkprot$ has the $\ur{3}$ property,
cf.~\cref{lem:plonkprot_ur}.

Next, we show that $\plonkprot$ is computationally special-sound. That is, given a
number of acceptable transcripts which match on the first 3 rounds of the
protocol we can either reveal a correct witness for the proven statement or use
one of the transcripts to break the $\dlog$ assumption. This result is shown in
the AGM, cf.~\cref{lem:plonkprot_ss}.

Given special-soundness of $\plonkprot$, we use the fact that it is also
$\ur{3}$ and show, in a similar fashion to \cite{INDOCRYPT:FKMV12}, that it is
simulation-extractable. That is, we build reductions that given a simulation
extractability adversary $\advse$ either break the protocol's unique response
property or break the $\dlog$ assumption, if extracting a valid witness from a
tree of transcripts is impossible. See \cref{thm:plonkprotfs_se}.

\subsection{$\plonk$ protocol rolled out}
\label{sec:plonk_explained}
\newcommand{\vql}{\vec{q_{L}}}
\newcommand{\vqr}{\vec{q_{R}}}
\newcommand{\vqm}{\vec{q_{M}}}
\newcommand{\vqo}{\vec{q_{O}}}
\newcommand{\vx}{\vec{x}}
\newcommand{\vqc}{\vec{q_{C}}}
\subsubsection{The constrain system}
Assume $\CRKT$ is a fan-in two arithmetic circuit,
which fan-out is unlimited and has $\numberofconstrains$ gates and $\noofw$ wires
($\numberofconstrains \leq \noofw \leq 2\numberofconstrains$). \plonk's constraint
system is defined as follows:
\begin{itemize}
\item Let $\vec{V} = (\va, \vb, \vc)$, where $\va, \vb, \vc
  \in \range{1}{\noofw}^\numberofconstrains$. Entries $\va_i, \vb_i, \vc_i$ represent indices of left,
  right and output wires of circuits $i$-th gate.
\item Vectors $\vec{Q} = (\vql, \vqr, \vqo, \vqm, \vqc) \in
  (\FF^\numberofconstrains)^5$ are called \emph{selector vectors}:
  \begin{itemize}
  \item If the $i$-th gate is a multiplicative gate then $\vql_i = \vqr_i = 0$,
    $\vqm_i = 1$, and $\vqo_i = -1$. 
  \item If the $i$-th gate is an addition gate then $\vql_i = \vqr_i  = 1$, $\vqm_i =
    0$, and $\vqo_i = -1$. 
  \item $\vqc_i = 0$ always. 
  \end{itemize}
\end{itemize}

We say that vector $\vx \in \FF^\noofw$ satisfies constraint system if for all $i
\in \range{1}{\numberofconstrains}$
\[
  \vql_i \cdot \vx_{\va_i} + \vqr_i \cdot \vx_{\vb_i} + \vqo \cdot \vx_{\vc_i} +
  \vqm_i \cdot (\vx_{\va_i} \vx_{\vb_i}) + \vqc_i = 0. 
\]

\subsubsection{Algorithms rolled out}
\label{sec:plonk_explained}
\plonk{} argument system is universal. That is, it allows to verify computation
of any arithmetic circuit which has no more than $\numberofconstrains$
gates using a single SRS. However, to make computation efficient, for each
circuit there is allowed a preprocessing phase which extend the SRS with
circuit-related polynomial evaluations.

For the sake of simplicity of the security reductions presented in this paper, we
include in the SRS only these elements that cannot be computed without knowing
the secret trapdoor $\chi$. The rest of the SRS---the preprocessed input---can
be computed using these SRS elements thus we leave them to be computed by the
prover, verifier, and simulator.

\paragraph{$\plonk$ SRS generating algorithm $\kgen(\REL)$:}
The SRS generating algorithm picks at random $\chi \sample \FF_p$, computes
and outputs
\[
	\srs = \left(\gone{\smallset{\chi^i}_{i = 0}^{\numberofconstrains + 2}},
	\gtwo{\chi} \right).
\]

\paragraph{Preprocessing:}
Let $H = \smallset{\omega^i}_{i = 1}^{\numberofconstrains }$ be a
(multiplicative) $\numberofconstrains$-element subgroup of a field $\FF$
compound of $\numberofconstrains$-th roots of unity in $\FF$. Let $\lag_i(X)$ be
the $i$-th element of an $\numberofconstrains$-elements Lagrange basis. During
the preprocessing phase polynomials $\p{S_{id j}}, \p{S_{\sigma j}}$, for
$\p{j} \in \range{1}{3}$, are computed:
\begin{equation*}
  \begin{aligned}
    \p{S_{id 1}}(X) & = X,\vphantom{\sum_{i = 1}^{\noofc} \sigma(i) \lag_i(X),}\\
    \p{S_{id 2}}(X) & = k_1 \cdot X,\vphantom{\sum_{i = 1}^{\noofc} \sigma(i) \lag_i(X),}\\
    \p{S_{id 3}}(X) & = k_2 \cdot X,\vphantom{\sum_{i = 1}^{\noofc} \sigma(i) \lag_i(X),}
  \end{aligned}
  \qquad
\begin{aligned}
  \p{S_{\sigma 1}}(X) & = \sum_{i = 1}^{\noofc} \sigma(i) \lag_i(X),\\
  \p{S_{\sigma 2}}(X) & = \sum_{i = 1}^{\noofc}
  \sigma(\noofc + i) \lag_i(X),\\
  \p{S_{\sigma 3}}(X) & =\sum_{i = 1}^{\noofc} \sigma(2 \noofc + i) \lag_i(X).
\end{aligned}
\end{equation*}
Coefficients $k_1$, $k_2$ are such that $H, k_1 \cdot H, k_2 \cdot H$ are
different cosets of $\FF^*$, thus they define $3 \cdot \noofc$
different elements. \cite{EPRINT:GabWilCio19} notes that it is enough to set
$k_1$ to a quadratic residue and $k_2$ to a quadratic non-residue.

Furthermore, we define polynomials $\p{q_L}, \p{q_R}, \p{q_O}, \p{q_M}, \p{q_C}$
such that
\begin{equation*}
  \begin{aligned}
  \p{q_L}(X) & = \sum_{i = 1}^{\noofc} \vql_i \lag_i(X), \\
  \p{q_R}(X) & = \sum_{i = 1}^{\noofc} \vqr_i \lag_i(X), \\
  \p{q_M}(X) & = \sum_{i = 1}^{\noofc} \vqm_i \lag_i(X),
\end{aligned}
\qquad
\begin{aligned}
  \p{q_O}(X) & = \sum_{i = 1}^{\noofc} \vqo_i \lag_i(X), \\
  \p{q_C}(X) & = \sum_{i = 1}^{\noofc} \vqc_i \lag_i(X). \\
  \vphantom{\p{q_M}(X)  = \sum_{i = 1}^{\noofc} \vqm_i \lag_i(X),}
\end{aligned}
\end{equation*}

\paragraph{$\plonk$ prover
  $\prover(\REL, \srs, \inp, \wit = (\wit_i)_{i \in \range{1}{3 \cdot
      \noofc}})$.}
\begin{description}
\item[Round 1] Sample $b_1, \ldots, b_9 \sample \FF_p$; compute
  $\p{a}(X), \p{b}(X), \p{c}(X)$ as
	\begin{align*}
		\p{a}(X) &= (b_1 X + b_2)\p{Z_H}(X) + \sum_{i = 1}^{\noofc} \wit_i \lag_i(X) \\
		\p{b}(X) &= (b_3 X + b_4)\p{Z_H}(X) + \sum_{i = 1}^{\noofc} \wit_{\noofc + i} \lag_i(X) \\
		\p{c}(X) &= (b_5 X + b_6)\p{Z_H}(X) + \sum_{i = 1}^{\noofc} \wit_{2 \cdot \noofc + i} \lag_i(X) 
	\end{align*}
	Output polynomial commitments $\gone{\p{a}(\chi), \p{b}(\chi), \p{c}(\chi)}$.
	
	\item[Round 2]
	Get challenges $\beta, \gamma \in \FF_p$
	\[
		\beta = \ro(\zkproof[0..1], 0)\,, \qquad \gamma = \ro(\zkproof[0..1], 1)\,.
	\]
	Compute permutation polynomial $\p{z}(X)$
	\begin{multline*}
		\p{z}(X) = (b_7 X^2 + b_8 X + b_9)\p{Z_H}(X) + \lag_1(X) + \\
			+ \sum_{i = 1}^{\noofc - 1} 
			\left(\lag_{i + 1} (X) \prod_{j = 1}^{i} 
			\frac{
			(\wit_j +\beta \omega^{j - 1} + \gamma)(\wit_{\noofc + j} + \beta k_1 \omega^{j - 1} + \gamma)(\wit_{2 \noofc + j} +\beta k_2 \omega^{j- 1} + \gamma)}
			{(\wit_j+\sigma(j) \beta + \gamma)(\wit_{\noofc + j} + \sigma(\noofc + j)\beta + \gamma)(\wit_{2 \noofc + j} + \sigma(2 \noofc + j)\beta + \gamma)}\right)
	\end{multline*}
	Output polynomial commitment $\gone{\p{z}(\chi)}$
		
	\item[Round 3]
	Get the challenge $\alpha = \ro(\zkproof[0..2])$, compute the quotient polynomial 
	\begin{align*}
	& \p{t}(X)  = \\
	& (\p{a}(X) \p{b}(X) \selmulti(X) + \p{a}(X) \selleft(X) + 
	\p{b}(X)\selright(X) + \p{c}(X)\seloutput(X) + \pubinppoly(X) + \selconst(X)) 
	\frac{1}{\p{Z_H}(X)} +\\
	& + ((\p{a}(X) + \beta X + \gamma) (\p{b}(X) + \beta k_1 X + \gamma)(\p{c}(X) 
	+ \beta k_2 X + \gamma)\p{z}(X)) \frac{\alpha}{\p{Z_H}(X)} \\
	& - (\p{a}(X) + \beta \p{S_{\sigma 1}}(X) + \gamma)(\p{b}(X) + \beta 
	\p{S_{\sigma 2}}(X) + \gamma)(\p{c}(X) + \beta \p{S_{\sigma 3}}(X) + 
	\gamma)\p{z}(X \omega))  \frac{\alpha}{\p{Z_H}(X)} \\
	& + (\p{z}(X) - 1) \lag_1(X) \frac{\alpha^2}{\p{Z_H}(X)}
	\end{align*}
	Split $\p{t}(X)$ into degree less then $\noofc$ polynomials $\p{t_{lo}}(X), \p{t_{mid}}(X), \p{t_{hi}}(X)$, such that
	\[
		\p{t}(X) = \p{t_{lo}}(X) + X^{\noofc} \p{t_{mid}}(X) + X^{2 \noofc} \p{t_{hi}}(X)\,.
	\]
	Output $\gone{\p{t_{lo}}(\chi), \p{t_{mid}}(\chi), \p{t_{hi}}(\chi)}$.
	
	\item[Round 4]
	Get the challenge $\chz \in \FF_p$, $\chz = \ro(\zkproof[0..3])$.
	Compute opening evaluations
	\begin{align*}
      \p{a}(\chz), \p{b}(\chz), \p{c}(\chz), \p{S_{\sigma 1}}(\chz), \p{S_{\sigma 2}}(\chz), \p{t}(\chz), \p{z}(\chz \omega),
	\end{align*}
	Compute the linearisation polynomial
	\[
		\p{r}(X) = 
		\begin{aligned}
          & \p{a}(\chz) \p{b}(\chz) \selmulti(X) + \p{a}(\chz) \selleft(X) + \p{b}(\chz) \selright(X) + \p{c}(\chz) \seloutput(X) + \selconst(X) \\
          & + \alpha \cdot \left( (\p{a}(\chz) + \beta \chz + \gamma) (\p{b}(\chz) + \beta k_1 \chz + \gamma)(\p{c}(\chz) + \beta k_2 \chz + \gamma) \cdot \p{z}(X)\right) \\
          & - \alpha \cdot \left( (\p{a}(\chz) + \beta \p{S_{\sigma 1}}(\chz) + \gamma) (\p{b}(\chz) + \beta \p{S_{\sigma 2}}(\chz) + \gamma)\beta \p{z}(\chz\omega) \cdot \p{S_{\sigma 3}}(X)\right) \\
          & + \alpha^2 \cdot \lag_1(\chz) \cdot \p{z}(X)
		\end{aligned}
	\]
	Output $\p{a}(\chz), \p{b}(\chz), \p{c}(\chz), \p{S_{\sigma 1}}(\chz), \p{S_{\sigma 2}}(\chz), \p{t}(\chz), \p{z}(\chz \omega), \p{r}(\chz).$
	
	\item[Round 5]
	Compute the opening challenge $v \in \FF_p$, $v = \ro(\zkproof[0..4])$.
	Compute the openings for the polynomial commitment scheme 
	\begin{align*}
	& \p{W_\chz}(X) = \frac{1}{X - \chz} \left(
	\begin{aligned}
		& \p{t_{lo}}(X) + \chz^\noofc \p{t_{mid}}(X) + \chz^{2 \noofc} \p{t_{hi}}(X) - \p{t}(\chz)\\
		& + v(\p{r}(X) - \p{r}(\chz)) \\
		& + v^2 (\p{a}(X) - \p{a}(\chz))\\
		& + v^3 (\p{b}(X) - \p{b}(\chz))\\
		& + v^4 (\p{c}(X) - \p{c}(\chz))\\
		& + v^5 (\p{S_{\sigma 1}}(X) - \p{S_{\sigma 1}}(\chz))\\
		& + v^6 (\p{S_{\sigma 2}}(X) - \p{S_{\sigma 2}}(\chz))
	\end{aligned}
	\right)\\
	& \p{W_{\chz \omega}}(X) = \frac{\p{z}(X) - \p{z}(\chz \omega)}{X - \chz \omega}
\end{align*}
	Output $\gone{\p{W_{\chz}}(\chi), \p{W_{\chz \omega}}(\chi)}$.
\end{description}

\ncase{$\plonk$ verifier $\verifier(\REL, \srs, \inp, \zkproof)$}\ \newline
The \plonk{} verifier works as follows
\begin{description}
	\item[Step 1] Validate all obtained group elements.
	\item[Step 2] Validate all obtained field elements.
	\item[Step 3] Validate the instance
      $\inp = \smallset{\wit_i}_{i = 1}^\instsize$.
	\item[Step 4] Compute challenges $\beta, \gamma, \alpha, \chz, v,
      u$ from the transcript.
	\item[Step 5] Compute zero polynomial evaluation
      $\p{Z_H} (\chz) =\chz^\noofc - 1$.
	\item[Step 6] Compute Lagrange polynomial evaluation
      $\lag_1 (\chz) = \frac{\chz^\noofc -1}{\noofc (\chz - 1)}$.
	\item[Step 7] Compute public input polynomial evaluation
      $\pubinppoly (\chz) = \sum_{i \in \range{1}{\instsize}} \wit_i
      \lag_i(\chz)$.
	\item[Step 8] Compute quotient polynomials evaluations
	\begin{multline*}
      \p{t} (\chz) = \frac{1}{\p{Z_H}(\chz)} \Big(
      \p{r} (\chz) + \pubinppoly(\chz) - (\p{a}(\chz) + \beta \p{S_\sigma 1}(\chz) + \gamma) (\p{b}(\chz) + \beta \p{S_\sigma 2}(\chz) + \gamma) \\
      (\p{c}(\chz) + \gamma)\p{z}(\chz \omega) \alpha - \lag_1 (\chz) \alpha^2
      \Big) \,.
	\end{multline*}
	\item[Step 9] Compute batched polynomial commitment
	$\gone{D} = v \gone{r} + u \gone {z}$ that is
	\begin{align*}
		\gone{D} & = v
		\left(
		\begin{aligned}
          & \p{a}(\chz)\p{b}(\chz) \cdot \gone{\selmulti} + \p{a}(\chz)  \gone{\selleft} + \p{b}  \gone{\selright} + \p{c}  \gone{\seloutput} + \\
          & + (	(\p{a}(\chz) + \beta \chz + \gamma) (\p{b}(\chz) + \beta k_1 \chz + \gamma) (\p{c} + \beta k_2 \chz + \gamma) \alpha  + \lag_1(\chz) \alpha^2)  + \\
			% &   \\
          & - (\p{a}(\chz) + \beta \p{S_{\sigma 1}}(\chz) + \gamma) (\p{b}(\chz)
          + \beta \p{S_{\sigma 2}}(\chz) + \gamma) \alpha \beta \p{z}(\chz
          \omega) \gone{\p{S_{\sigma 3}}(\chi)})
		\end{aligned}
		\right) + \\
		& + u \gone{\p{z}(\chi)}\,.
	\end{align*}
	\item[Step 10] Computes full batched polynomial commitment $\gone{F}$:
	\begin{align*}
      \gone{F} & = \left(\gone{\p{t_{lo}}(\chi)} + \chz^\noofc \gone{\p{t_{mid}}(\chi)} + \chz^{2 \noofc} \gone{\p{t_{hi}}(\chi)}\right) + u \gone{\p{z}(\chi)} + \\
               & + v
                 \left(
		\begin{aligned}
			& \p{a}(\chz)\p{b}(\chz) \cdot \gone{\selmulti} + \p{a}(\chz)  \gone{\selleft} + \p{b}(\chz)   \gone{\selright} + \p{c}(\chz)  \gone{\seloutput} + \\
			& + (	(\p{a}(\chz) + \beta \chz + \gamma) (\p{b}(\chz) + \beta k_1 \chz + \gamma) (\p{c}(\chz)  + \beta k_2 \chz + \gamma) \alpha  + \lag_1(\chz) \alpha^2)  + \\
			% &   \\
			& - (\p{a}(\chz) + \beta \p{S_{\sigma 1}}(\chz) + \gamma) (\p{b}(\chz) + \beta \p{S_{\sigma 2}}(\chz) + \gamma) \alpha  \beta \p{z}(\chz \omega) \gone{\p{S_{\sigma 3}}(\chi)})
		\end{aligned}
		\right) \\
		& + v^2 \gone{\p{a}(\chi)} + v^3 \gone{\p{b}(\chi)} + v^4 \gone{\p{c}(\chi)} + v^5 \gone{\p{S_{\sigma 1}(\chi)}} + v^6 \gone{\p{S_{\sigma 2}}(\chi)}\,.
	\end{align*}
	\item[Step 11] Compute group-encoded batch evaluation $\gone{E}$
	\begin{align*}
		\gone{E}  = \frac{1}{\p{Z_H}(\chz)} & \gone{
		\begin{aligned}
			& \p{r}(\chz) + \pubinppoly(\chz) +  \alpha^2  \lag_1 (\chz) + \\
			& - \alpha \left( (\p{a}(\chz) + \beta \p{S_{\sigma 1}} (\chz) + \gamma) (\p{b}(\chz) + \beta \p{S_{\sigma 2}} (\chz) + \gamma) (\p{c}(\chz) + \gamma) \p{z}(\chz \omega) \right)
		\end{aligned}
           }\\
      + & \gone{v \p{r}(\chz) + v^2 \p{a}(\chz) + v^3 \p{b}(\chz) + v^4 \p{c}(\chz) + v^5 \p{S_{\sigma 1}}(\chz) + v^6 \p{S_{\sigma 2}}(\chz) + u \p{z}(\chz \omega) }\,.
	\end{align*}
\item[Step 12] Check whether the verification
 % $\vereq_\zkproof(\chi)$
  equation holds
	\begin{multline}
		\label{eq:ver_eq}
		\left( \gone{\p{W_{\chz}}(\chi)} + u \cdot \gone{\p{W_{\chz
                \omega}}(\chi)} \right) \bullet
		\gtwo{\chi} - %\\
		\left( \chz \cdot \gone{\p{W_{\chz}}(\chi)} + u \chz \omega \cdot
          \gone{\p{W_{\chz \omega}}(\chi)} + \gone{F} - \gone{E} \right) \bullet
        \gtwo{1} = 0\,.
	\end{multline}
  The verification equation is a batched version of the verification equation
  from \cite{AC:KatZavGol10} which allows the verifier to check openings of
  multiple polynomials in two points (instead of checking an opening of a single
  polynomial at one point).
\end{description}

\subsection{Unique opening property of $\PCOMp$}
\begin{lemma}
\label{lem:pcomp_unique_op}
Let $\PCOMp$ be a batched version of a batched KZG polynomial
commitment. $\PCOMp$ has the unique opening property in the AGM.\end{lemma}
\begin{proof}
  Let $\vec{z} = (z, z') \in \FF_p^2$ be the two points the polynomials are
  evaluated at, $k \in \NN$ be the number of the committed polynomials to be
  evaluated at $z$, and $k' \in \NN$ be the number of the committed polynomials
  to be evaluated at $z'$, $\vec{c} \in \GRP^k, \vec{c'} \in \GRP^{k'}$ be the
  commitments, $\vec{s} \in \FF_p^k, \vec{s'} \in \FF_p^{k'}$ the evaluations,
  and $\vec{o} = (o, o') \in \FF_p^2$ be the commitment openings. We need to
  show that for every $\ppt$ adversary $\adv$ the probability
\[
	\Pr
		\left[
			\begin{aligned}
				& \verify(\srs, \vec{c}, \vec{c'}, (z, z'), \vec{s}, \vec{s'}, \vec{o}), \\
				& \verify(\srs, \vec{c}, \vec{c'}, (z, z'), \vec{s}, \vec{s'}, \vec{\tilde{o}}) \\
				& \vec{o} \neq \vec{\tilde{o}}
			\end{aligned}
		\,\left|\,
		\vphantom{\begin{aligned}
			& \verify(\srs, \vec{c}, \vec{c}, \vec{z}, \vec{s}, \vec{s'}, \vec{o}), \\
			& \verify(\srs, \vec{c}, \vec{c}, \vec{z}, \vec{s}, \vec{s'}, \vec{\tilde{o}}) \\
			&\vec{o} \neq \vec{\tilde{o}})
		\end{aligned}}
		\begin{aligned}
			& \srs \gets \kgen(\secparam, \maxdeg), \\
			&	(\vec{c}, \vec{c'}, \vec{z}, \vec{s}, \vec{s'}, \vec{o}, \vec{\tilde{o}}) \gets \adv(\srs)
		\end{aligned}
		\right.\right]
	 % \leq \negl.
\]
is at most negligible.

\ncase{Step 1} First, consider a case where the commitment is limited to
commit to multiple polynomials which are evaluated at the same point $z$. As
noted in \cite[Lemma 2.2]{EPRINT:GabWilCio19} it is enough to upper bound the
probability of the adversary succeeding using the idealised verification
equation---which considers equality between polynomials---instead of the real
verification equation---which consider equality of the polynomials' evaluations.
This holds since an adversary that manages to provide a commitment opening that
holds for the real verifier, but does not hold for the idealised verifier can be
used to break the dlog assumption and reveal the secret trapdoor used to produce
the commitment's SRS, cf.~\cite[Lemma 2.2]{EPRINT:GabWilCio19} for more details.

For polynomials $\vec{\p{f}} = \p{f}_1, \ldots, \p{f}_k$, evaluation point $z$, evaluation
result $\vec{s} = s_1, \ldots, s_k$, random $\gamma$, and opening $\p{o}(X)$ the
idealised check verifies that
\begin{equation}
	\sum_{i = 1}^k \gamma^{i - 1} \p{f}_i(X) - \sum_{i = 1}^{k} \gamma^{i - 1} s_i \equiv \p{o}(X) (X - z)\,.
	\label{eq:pcom_idealised_check}
\end{equation}
Since $\p{o}(X)(X - z) \in \FF_p[X]$ then from the uniqueness of polynomial
composition, there is only one $\p{o} (X)$ that fulfils the equation above.

\ncase{Step 2} Second, consider a case when the polynomials are evaluated on two
points $\vec{z} = (z, z')$ and the adversary is asked to provide two openings
$\vec{o} = (o, o')$. Similarly, we analyse the case of the ideal verification.
In that scenario, the verifier checks whether the following equality, for
$\gamma, r'$ picked at random, holds:
\begin{multline}
	\label{eq:ver_eq_poly}
	\sum_{i = 1}^{k} \gamma^{i - 1} \cdot \p{f}_i(X) - \sum_{i = 1}^{k} \gamma^{i - 1} \cdot s_i  + r' \left(\sum_{i = 1}^{k'} \gamma'^{i - 1} \cdot \p{f'}_i(X) - \sum_{i = 1}^{k'} \gamma'^{i - 1} \cdot s'_i \right)\\
	\equiv \p{o}(X)(X - z) + r' \p{o}'(X)(X- z')
\end{multline}
Since $r'$ has been picked at random, probability that \cref{eq:ver_eq_poly} holds while either
\[
	\sum_{i = 1}^{k} \gamma^{i - 1} \cdot \p{f}_i(X) - \sum_{i = 1}^{k}  \gamma^{i - 1} \cdot s_i \not\equiv \p{o}(X)(X - z)
\]
or 
\[
	\sum_{i = 1}^{k'} \gamma'^{i - 1} \cdot \p{f'}_i(X) - \sum_{i = 1}^{k'} \gamma'^{i - 1} \cdot s'_i \not\equiv \p{o'}(X)(X - z')
\]
is negligible~\cite{EPRINT:GabWilCio19}. This brings the proof back to
Step 1 above. \qed
\end{proof}

\subsection{Unique response property}
\begin{lemma}
	\label{lem:plonkprot_ur}
  Let $\PCOMp$ be commitment of knowledge with security $\epsk$,
  $\epsbind$-binding and has unique opening property with security $\epsop$,
  then probability that a $\ppt$ adversary $\adv$ breaks $\plonkprot$'s $\ur{3}$
  property is at most $\epsk + 2\cdot\epsbind +
  \epsop$.% assuming the polynomial commitment scheme $\PCOMp$
%is of knowledge.
\end{lemma}
\begin{proof}[sketch]
  Let
  $\adv(\REL,\srs = (\gone{1, \chi, \ldots, \chi^{\noofc + 2}}, \gtwo{\chi}))$
  be an adversary tasked to break the $\ur{3}$-ness of $\plonkprot$. It is
  sufficient to observe that the first 2 rounds of the protocol determines,
  along with the verifiers challenges, the rest of it.

  In Round 3 the adversary outputs a commitment to polynomial $\pt(X)$ which
  assures that all constraints of the system are fulfilled. Since the commitment
  scheme is deterministic, there is only one value $c$ that is a commitment to
  $\pt(X)$. Assume that $\adv$ outputs $c' \neq c$ and is later able to open
  $c'$ to $y = \pt(\chz)$, where $\chz$ is a random point determined later.
  Using the AGM and arguments similar to \cite{CCS:MBKM19}, we argue that
  $\PCOMp$ is a commitment of knowledge. That is, an AGM adversary $\adv$ that
  outputs a commitment $c'$ which it can later open, knows a polynomial $\p{f}$
  of degree-$(\leq \noofc + 2)$ such that $\gone{\p{f}(\chi)} = c'$. Thus if
  $c' \neq c$ and the commitment scheme is evaluation binding then $\adv$ when
  picking $c'$ picks it as a commitment to $\p{f}$ which evaluates at $\chz$ to
  $\pt(\chz)$. The probability of that is negligible as there can only be
  no more than $\noofc + 2$  overlapping points between $\p{f}(X)$ and $\p{t}(X)$, and
  $\chz$ remains random for $\adv$ when it computes $c'$. Hence, the probability
  that $\adv$ is able to produce two different outputs of Round 3 is
  upper-bounded by $\epsk + \epsbind$.

  In Round 4 the prover is asked to give evaluations of predefined polynomials
  at some point $\chz$. Naturally, for the given polynomials only one value at
  $\chz$ is correct. Assume $\adv$ is able to produce two different outputs in
  that round: $\vec{r_4} = (\ev{\p{a}}, \ev{\p{b}}, \ev{\p{c}}, \ev{\p{S_{\sigma
        1}}}, \ev{\p{S_{\sigma 2}}}, \ev{\p{r}}, \ev{\p{z}})$ and $\vec{r_4} =
  (\ev{\p{a}}', \ev{\p{b}}', \ev{\p{c}}', \ev{\p{S_{\sigma 1}}}',
  \ev{\p{S_{\sigma 2}}}', \ev{\p{r}}', \ev{\p{z}}')$ which suppose to be
  evaluations at $\chz$ of polynomials $\p{a}, \p{b}, \p{c}, \p{S_{\sigma 1}},
  \p{S_{\sigma 2}}, \p{r}$ and an evaluation at $\chz \omega$ of $\p{z}$.
  Clearly, at least one of $\vec{r_4}$, $\vec{r'_4}$ has to be incorrect, thus
  if both evaluations are acceptable by the $\PCOMp.\verify$ then the evaluation
  binding property of $\PCOMp$ is broken. This happens with probability
  upper-bounded by $\epsbind$.

  In the last round of the protocol the prover provides openings for the
  polynomial commitment evaluations done before. Assume $\adv$ is able to
  produce two different polynomial commitment openings pairs: $\vec{r_5} =
  (\ev{\p{W_\chz}}, \ev{\p{W_{\chz \omega}}})$ and $\vec{r'_5} =
  (\ev{\p{W_\chz}}', \ev{\p{W_{\chz \omega}}}')$.
% Since \cref{lem:pcomp_unique_op}, 
  Since $\PCOMp$ has unique opening property, one of the openings has to be
  incorrect and should be rejected by the polynomial commitment verifier. This
  happens except with probability $\epsop$.

  Hence, the probability that after fixing the two first rounds, the adversary
  is able to produce two different outputs in one of the following rounds is
  upper-bounded by
\[
	\epsk + 2 \cdot \epsbind + \epsop\,.
\]
\qed


\end{proof}
\COMMENT{
\begin{lemma}
\label{lem:plonkprot_ur}
If a polynomial commitment scheme $\PCOM$ is evaluation binding with parameter
$\epsbind$ and has unique openings property with parameter $\epsop$, then
$\plonkprot$ is $\ur{2}$. However, in the case presented here, less is required.
with parameter $\epsur \leq \epsbind + \epsop$.
\end{lemma}
\begin{proof}
  Let $\adv$ be an adversary that breaks $\ur{2}$-ness of $\plonkprot$.  We
  consider two cases, depending on which round $\adv$ is able to provide at
  least two different outputs such that the resulting transcripts are
  acceptable.  For the first case we show that $\adv$ breaks the evaluation
  binding property of $\PCOM$, while for the second case we show that it breaks
  the unique opening property of $\PCOM$.

\case{1} In Round 4 the prover is asked to give evaluations of predefined
polynomials at some point $\chz$. Naturally, for the given polynomials only one
value at $\chz$ is correct. Assume $\adv$ is able to produce two different
outputs in that round: $\vec{r_4} = (\ev{\p{a}}, \ev{\p{b}}, \ev{\p{c}},
\ev{\p{S_{\sigma 1}}}, \ev{\p{S_{\sigma 2}}}, \ev{\p{r}}, \ev{\p{z}})$ and
$\vec{r_4} = (\ev{\p{a}}', \ev{\p{b}}', \ev{\p{c}}', \ev{\p{S_{\sigma 1}}}',
\ev{\p{S_{\sigma 2}}}', \ev{\p{r}}', \ev{\p{z}}')$ which suppose to be
evaluations at $\chz$ of polynomials $\p{a}, \p{b}, \p{c}, \p{S_{\sigma 1}},
\p{S_{\sigma 2}}, \p{r}$ and an evaluation at $\chz \omega$ of $\p{z}$. Clearly,
at least one of $\vec{r_4}$, $\vec{r'_4}$ has to be incorrect, thus if both
evaluations are acceptable by the $\PCOM.\verify$ then the evaluation binding
property of $\PCOM$ is broken. This happens with probability upper-bounded by
$\epsbind$.

\case{2} In the last round of the protocol the prover provides openings for the
polynomial commitment evaluations done before. Assume $\adv$ is able to produce
two different polynomial commitment openings pairs: $\vec{r_5} =
(\ev{\p{W_\chz}}, \ev{\p{W_{\chz \omega}}})$ and $\vec{r'_5} =
(\ev{\p{W_\chz}}', \ev{\p{W_{\chz \omega}}}')$.
% Since \cref{lem:pcomp_unique_op},
Since $\PCOM$ has unique opening property, one of the openings has to be
incorrect and should be rejected by the polynomial commitment verifier. This
happens except with probability $\epsop$

\conclude Hence the probability that $\adv$ breaks $\ur{2}$-property of
$\plonkprot$ is upper-bounded by $\epsbind + \epsop$. \qed
\end{proof}
}

\subsection{Computational special soundness}
\begin{lemma}
\label{lem:plonkprot_ss}
$\plonkprot$ is $(\epsss, (1, 1, \noofc + 3, 1))$-computational special sound
for 
 \[
	\epsss \leq \epsbatch + \epsdlog\,,
 \] 
 where $\epsbatch$ is the (negligible) probability that $\plonkprot$'s idealised
 verification equation $\vereq_\zkproof(X)$ accepts an invalid proof because of batching
 and $\epsdlog$ is a probability that a $\ppt$ algorithm breaks the
 $(\noofc + 2, 1)$-$\dlog$ assumption.
\end{lemma}
\begin{proof}
  Let $\srs$ be $\plonkprot$'s SRS and denote by $\srs_1$ all SRS's
  $\GRP_1$-elements; that is,
  $\srs_1 = \gone{1, \chi, \ldots, \chi^{\noofc + 2}}$. Let $\adv$ be an
  algebraic adversary that produces a statement $\inp$ and a
  $(1, 1, \noofc + 3, 1)$-tree of acceptable transcripts $\tree$.  Note that in
  all transcripts the instance $\inp$, proof elements
  $\gone{\p{a}(\chi), \p{b}(\chi), \p{c}(\chi), \p{z}(\chi), \p{t}(\chi)}$ and
  challenges $\alpha, \beta, \gamma$ are common as the transcripts share the
  first three rounds. The tree ``branches'' after the third round of the protocol
  where the challenge $\chz$ is presented, thus tree $\tree$ is build using different
  values of $\chz$. 

  We consider two mutually disjunctive events. The first, $\event{E}$ holds when
  all of the transcripts are acceptable by the idealised verification equation,
  i.e.~$\vereq_\zkproof(X) = 0$, cf.~\cref{eq:ver_eq}. The second, $\nevent{E}$ holds
  when there is a transcript that is acceptable by the real verifier, but not by
  the ideal verifier. That is, for that particular transcript it holds that
  $\vereq_\zkproof(\chi) = 0$, but $\vereq_\zkproof(X) \neq 0$.  When event $\event{E}$ holds we show that the extractor $\extt$ is successful. If it does not hold, we show a reduction
  $\rdvdlog$ that breaks the $\dlog$ assumption.

  \ncase{When $\event{E}$ holds} Let $(\inp, \tree) \gets \adv(\REL,
  \srs)$. Since the protocol $\plonkprot$, instantiated with the idealised
  verification equation, is perfectly sound, except with probability of batching
  failure $\epsbatch$, for a valid proof $\zkproof$ of a statement $\inp$ there
  exists a witness $\wit$, such that $\REL(\inp, \wit)$ holds. Note that 
  polynomials $\p{a}(X), \p{b}(X), \p{c}(X)$, which contain witness in their
  coefficients, have degree $(\noofc + 2)$ and since $\adv$ answered correctly
  on $(\noofc + 3)$ different challenges $\chz$ then $(\noofc + 3)$ evaluations
  of these polynomials (at different points) are known. The extractor $\extt$
  interpolates the polynomials and reveals the corresponding witness
  $\wit$. The extractor finds the witness with probability $1$.

  \ncase{When $\nevent{E}$ holds} Let $\adv$ be an adversary that for relation
  $\REL$ and randomly picked $\srs \sample \kgen(\REL)$ produces a tree of
  acceptable transcripts such that $\nevent{E}$ holds. Let $\rdvdlog$ be a
  reduction that gets as input an $(\noofc + 2, 1)$-dlog instance
  $\gone{1, \ldots, \chi^{\noofc}}, \gtwo{\chi}$ and is tasked to output
  $\chi$. The reduction proceeds as follows---it gives the input instance to the
  adversary as the SRS. Let $(\inp, \tree)$ be the output returned by $\adv$.
  Consider a transcript $\zkproof \in \tree$ such that
  $\vereq_\zkproof(X) \neq 0$, but $\vereq_\zkproof(\chi) = 0$. Since the
  adversary is algebraic, all group elements included in $\tree$ are extended by
  their representation as a combination of the input $\GRP_1$-elements. Hence all coefficients of
  the verification equation polynomial $\vereq_\zkproof(X)$ are known and $\rdvdlog$ can
  find its zero points. Since $\vereq_\zkproof(\chi) = 0$, the targeted discrete log
  value $\chi$ is among them. \qed
\end{proof}

\subsection{Honest verifier zero-knowledge}
\begin{lemma}
  $\plonkprot$ is computationally honest verifier zero-knowledge and its simulator $\simulator$ does
  not require a SRS trapdoor.\footnote{The simulator works as a simulator for
    proofs that are zero-knowledge in the standard model. However, we do not say
    that $\plonk$ is HVZK in the standard model as proof of that \emph{requires}
    the SRS simulator.} More precisely, assume that $\plonk$'s SRS simulator
  $\simulator_\chi$ produces proof that are distributed at most $\epszk$-far
  from real proofs, and $(\pR, \pS, \pT, \pf, 1)$-uber assumption for
  $\pR, \pS, \pT, \pf$ as defined in \cref{eq:uber} is $\epsuber$-secure. Then
  any $\ppt$ adversary $\adv$ has advantage in telling a proof produced by
  $\simulator$ from a real proof upper-bounded by $\epszk + \epsuber$.
\end{lemma}
\begin{proof}
  The proof goes by game-hoping. The environment that controls the games
  provides the adversary with a SRS $\srs$, then the adversary outputs an
  instance--witness pair $(\inp, \wit)$ and, depending on the game, is provided
  with either real or simulated proof for it. In the end of the game the
  adversary outputs either $0$ if it believes that the proof it saw was provided
  by the simulator and $1$ in the other case.

  \ngame{0} In this game $\adv(\REL, \srs)$ picks an instance--witness pair
  $(\inp, \wit)$ and gets a real proof $\zkproof$ for it.

  \ngame{1} In this game for $\adv(\REL, \srs)$ picks an instance--witness pair
  $(\inp, \wit)$ and gets a proof $\zkproof$ that is simulated by a simulator
  $\simulator_\chi$ which utilises for the simulation the SRS trapdoor and
  proceeds as follows. In the first round the simulator $\simulator_\chi$ picks
  randomisers $b_1, \ldots b_9$, sets $\wit_i = 0$, for $i \in \range{1}{3
    \noofc}$, computes polynomials $\pa(X), \pb(X), \pc(X)$ and
  outputs $\gone{\pa(\chi), \pb(\chi), \pc(\chi)}$. Then it picks Round 1
  challenges $\beta, \gamma$ honestly.

  In Round 2 $\simulator_\chi$ computes the polynomial $\pz(X)$ and
  outputs $\gone{\pz(\chi)}$. Then it picks randomly Round 2 challenge $\alpha$.

  In Round 3 the simulator computes polynomial $\pt(X)$ and evaluates it
  at $\chi$, then outputs $\gone{\ptlo(\chi), \ptmid(\chi), \pthi(\chi)}$. Note
  that this evaluation is feasible (in the polynomial time with non-negligible
  probability) only since $\simulator_\chi$ knows the trapdoor.

  In the last two rounds the simulator proceeds as an honest prover would
  proceed and picks corresponding challenges at random as an honest verifier
  would.

  \ncase{$\game{0} \mapsto \game{1}$} Since $\plonk$ is zero-knowledge,
  probability that $\adv$ outputs a different bit in both games is negligible.
  Hence
  \[
	\abs{\prob{\game{0}} - \prob{\game{1}}} \leq \epszk.
\]

\ngame{2} In this game $\adv(\REL, \srs)$ picks an instance--witness pair
$(\inp, \wit)$ and gets a proof $\zkproof$ simulated by the simulator
$\simulator$ which proceeds as follows.

In Round 1 the simulator  picks randomly both the randomisers $b_1, \ldots, b_6$ and
sets $\wit_i = 0$ for $i \in \range{1}{3\noofc}$. Then $\simulator$
outputs $\gone{\p{a}(\chi), \p{b}(\chi), \p{c}(\chi)}$. For the first round
challenge, the simulator picks permutation argument challenges $\beta, \gamma$
randomly.

In Round 2, the simulator computes $\p{z}(X)$ from
the newly picked randomisers $b_7, b_8, b_9$ and coefficients of polynomials
$\p{a}(X), \p{b}(X), \p{c}(X)$. Then it evaluates $\p{z}(X)$ honestly and outputs
$\gone{\p{z}(\chi)}$. Challenge $\alpha$ that should be sent by the verifier
after Round 2 is picked by the simulator at random.

In Round 3 the simulator starts by picking at random a challenge $\chz$, which
in the real proof comes as a challenge from the verifier sent \emph{after} Round
3. Then $\simulator$ computes evaluations
\(\p{a}(\chz), \p{b}(\chz), \p{c}(\chz), \p{S_{\sigma 1}}(\chz), \p{S_{\sigma
    2}}(\chz), \pubinppoly(\chz), \lag_1(\chz), \p{Z_H}(\chz),\allowbreak
\p{z}(\chz\omega)\) and computes $\p{t}(X)$ honestly. Since for a random
$\p{a}(X), \p{b}(X), \p{c}(X), \p{z}(X)$ the constraint system is (with
overwhelming probability) not satisfied and the constraints-related polynomials
are not divisible by $\p{Z_H}(X)$, hence $\p{t}(X)$ is a rational function
rather than a polynomial. Then, the simulator evaluates $\p{t}(X)$ at $\chz$ and
picks randomly a degree-$(3 \noofc - 1)$ polynomial $\p{\tilde{t}}(X)$ such that
$\p{t}(\chz) = \p{\tilde{t}}(\chz)$ and publishes a commitment
$\gone{\p{\tilde{t}_{lo}}(\chi), \p{\tilde{t}_{mid}}(\chi),
  \p{\tilde{t}_{hi}}(\chi)}$. After this round the simulator outputs $\chz$ as a
challenge.

In the next round, the simulator computes polynomial $\p{r}(X)$ as an honest
prover would, cf.~\cref{sec:plonk_explained} and evaluates $\p{r}(X)$ at $\chz$.

The rest of the evaluations are already computed, thus $\simulator$ simply
outputs
\[
  \p{a}(\chz), \p{b}(\chz), \p{c}(\chz), \p{S_{\sigma 1}}(\chz), \p{S_{\sigma
      2}}(\chz), \p{t}(\chz), \p{z}(\chz \omega)\,.
\]
After that it picks randomly the challenge $v$, proceeds in the last round as an
honest prover would proceed and outputs the final challenge, $u$, by picking it
at random as well.

\ncase{$\game{1} \mapsto \game{2}$} We now describe the reduction $\rdv$ which
relies on the $(\pR, \pS, \pT, \pF, 1)$-uber assumption where $\pR, \pS, \pT, \pF$
are polynomials over variables $\vB = B_1, \ldots, B_9$ and are defined as
follows. Let $E = \smallset{\smallset{2}, \smallset{3, 4}, \smallset{5, 6},
  \smallset{7, 8, 9}}$ and $E' = E \setminus \smallset{2}$. Let
\begin{align}
\label{eq:uber}
\pF(\vB) & = \smallset{B_1} \cup \smallset{B_1B_i \mid i \in A,\ A \in E'} \cup
             \smallset{B_1B_iB_j \mid i \in A, j \in B,\ A, B \in E', B
             \neq A} \cup \notag\\
           & \smallset{B_1B_iB_jB_k \mid i \in A, j \in
             B, k \in C,\ A, B, C \in E', A \neq B \neq C \neq A}\notag\,,\\
  \pR(\vB) & = \smallset{B_i \mid i \in A,\ A \in E} \cup \smallset{B_i B_j \mid i \in
             A, j \in B,\ A \neq B, A, B \in E} \cup \\ 
           & \smallset{B_i B_j B_k \mid i \in A,\ j \in
             B,\ k \in C,\
             A, B, C \text{ all different and in } E} \cup \notag \\
           & \smallset{B_i B_j B_k B_l \mid i \in A,\ j \in B,\ k \in C,\ l \in D,\
             A, B, C, D \text{ all different and in } E} \notag \\
           & \setminus \pF(\vB)\,,\notag \\
  \pS(\vB) & = \emptyset \notag\,, \\
  \pT(\vB) & = \emptyset \notag\,.
\end{align}
That is, the elements of $\pR$ are all singletons, pairs, triplets and
quadruplets of $B_i$ variables that occur in polynomial $\pt(\vB)$ except the
challenge element $\pf(\vB)$ which are all elements that depends on a variable
$B_1$. Variables $\vB$ are evaluated to randomly picked
$\vb = b_1, \ldots, b_9$.

The reduction $\rdv$ learns $\gone{\pR}$ and challenge
$\gone{\vec{w}} = \gone{w_1, \ldots, w_{12}}$ where $\vec{w}$ is either a vector
of evaluations $\pF(\vb)$ or a sequence of random values $y_1, \ldots, y_{12}$,
for the sake of concreteness we state $w_1 = b_1$ or $w_1 = y_1$ (depending on
the chosen random bit). Then it picks $\chi$, $\chz$ and computes the SRS $\srs$
from $\chi$. Elements $b_i$ are interpreted as polynomials in $X$ that are
evaluated at $\chi$, i.e. $b_i = b_i(\chi)$. Next, $\rdv$ sets for
$\xi_i, \zeta_i \sample \FF_p$
\[
  \gone{\p{\tb}_1}(X) =
(X - \chz)(X - \ochz) \gone{w_1}(X) + \xi_i (X - \chz) \gone{1} +
\zeta_i (X - \ochz) \gone{1}, % \text{ for } i \in % \range{1}{9}, u_1
\]
and
\[
  \gone{\p{\tb}_i}(X) =
(X - \chz)(X - \ochz) \gone{b_i}(X) + \xi_i (X - \chz) \gone{1} +
\zeta_i (X - \ochz) \gone{1}, % \text{ for } i \in % \range{1}{9}, u_1
\] 
for $i \in \range{2}{9}$.

Denote by $\tb_i$ evaluations of $\p{\tb}_i$ at $\chi$.  The reduction computes
all
$\gone{\tb_i \tb_j}, \gone{\tb_i \tb_j \tb_k}, \gone{\tb_i \tb_j \tb_k \tb_l}$
such that $\gone{B_i B_j, B_i B_j B_k, B_i B_j B_k B_l} \in \pR$.  This is
possible since $\rdv$ knows all singletons $\gone{w_1, b_2, \ldots, b_9}$ and pairs
$\gone{b_i b_j} \in \pR$ which can be used to compute all required pairs
$\gone{\tb_i \tb_j}$:
\begin{align*}
\gone{\tb_i \tb_j} 
& = ((\chi - \chz)(\chi - \ochz)\gone{b_i} + \xi_i (\chi - \chz)\gone{1} +
\zeta_i (\chi - \ochz) \gone{1}) 
\cdot \\
 & ((\chi - \chz)(\chi - \ochz)\gone{b_j} + \xi_j (\chi - \chz)\gone{1} +
\zeta_j (\chi - \ochz) \gone{1}) = \\
 & ((\chi - \chz)(\chi - \ochz))^2 \gone{b_i b_j} +  ((\chi - \chz)(\chi -
 \ochz)\gone{b_i} (\xi_j (\chi - \chz) \gone{1} + \zeta_j (\chi - \ochz)
 \gone{1}) + \\
 & ((\chi - \chz)(\chi -
 \ochz)\gone{b_j} (\xi_i (\chi - \chz) \gone{1} + \zeta_i (\chi - \ochz)
 \gone{1}) + \psi,
\end{align*}
where $\psi$ compounds of $\xi_i, \xi_j, \zeta_i, \zeta_j, \chz, \ochz, \chi$ which
are all known by $\rdv$ and no $b_i$ nor $b_j$.
Analogously for the triplets and quadruplets and elements dependent on $\vec{w}$. 

Next the reduction runs the adversary $\adv(\REL, \srs)$ and obtains from $\adv$ an
instance--witness pair $(\inp, \wit)$.  $\rdv$ now prepares a simulated proof as follows:
\begin{description} 
\item[Round 1] $\rdv$ computes $\gone{\pa(\chi)}$ using as
randomisers $\gone{\tb_1}, \gone{\tb_2}$ and setting $\wit_i = 0$, for $i
\in \range{1}{3 \noofc}$. Similarly it computes
$\gone{\pb(\chi)}, \gone{\pc(\chi)}$.  $\rdv$ publishes the obtained values
and picks a Round 1 challenge $\beta, \gamma$ at random.  Note that regardless
$w_1 = b_1$ or a random element, $\gone{a(\chi)}$ is random. Thus $\rdv$'s
output has the same distribution as output of a real prover.  
\item[Round 2]
$\rdv$ computes $\gone{\pz(\chi)}$ using $\tb_7, \tb_8, \tb_9$ and publishes
it. Then it picks randomly the challenge $\alpha$. This round output is
independent on $b_1$ thus $\rdv$'s output is indistinguishable from the prover's. 
\item[Round 3] The reduction computes
  $\p{t_{lo}}(\chi), \p{t_{mid}}(\chi), \p{t_{hi}}(\chi)$, which all depend on
  $b_1$. To that end $\gone{\tb_1}$ is used. Note that if $\vec{w}$ is a vector
  of $\pF(b_1, \ldots, b_9)$ evaluations then
  $\gone{\p{t_{lo}}(\chi), \p{t_{mid}}(\chi), \p{t_{hi}}(\chi)}$ is the same as
  the real prover's. Alternatively, if $\vec{w}$ is a vector of random values,
  then $\p{t_{lo}}(\chi), \p{t_{mid}}(\chi), \p{t_{hi}}(\chi)$ are all random
  polynomials which evaluates at $\chz$ to the same value as the polynomials
  computed by the real prover. That is, in that case
  $\p{t_{lo}}(\chi), \p{t_{mid}}(\chi), \p{t_{hi}}(\chi)$ are as the simulator
  $\simulator$ would compute. Eventually, $\rdv$ outputs $\chz$.
\item[Round 4] The reduction outputs
  $\pa(\chz), \pb(\chz), \pc(\chz), \p{S_{\sigma 1}}(\chz), \p{S_{\sigma 2}
    (\chz)}, \pt(\chz), \pz(\ochz)$.  For the sake of concreteness, denote by
  $S = \smallset{\pa, \pb, \pc, \pt, \pz}$. Although for a polynomial
  $\p{p} \in S$, reduction $\rdv$ does not know $\p{p}(\chi)$ or even do not
  know all the coefficients of $\p{p}$, the polynomials in $S$ was computed such
  that the reduction always knows their evaluation at $\chz$ and $\ochz$.
\item[Round 5] $\rdv$ computes the openings of the polynomial commitments
assuring that evaluations at $\chz$ it provided were computed honestly.
\end{description}

If the adversary $\adv$'s output distribution differ in Game $\game{1}$ and
$\game{2}$ then the reduction uses it to distinguish between
$\vec{w} = \pF(b_1, \ldots, b_9)$ and $\vec{w}$ being random, thus
\( \abs{\prob{\game{1}} - \prob{\game{2}}} \leq \epsuber.  \) Eventually,
\(
\abs{\prob{\game{0}} - \prob{\game{2}}} \leq \epszk + \epsuber.  \) \qed

% \ncase{$\game{1} \mapsto \game{2}$} We now describe the reduction $\rdv$ which
% relies on the $(\pR, \pS, \pT, \pf, 1)$-uber assumption where $\pR, \pS, \pT, \pf$
% are polynomials over variables $\vB = B_1, \ldots, B_9$ and are defined as
% follows. Let $E = \smallset{\smallset{1, 2}, \smallset{3, 4}, \smallset{5, 6},
%   \smallset{7, 8, 9}}$. Let
% \begin{align}
% \label{eq:uber}
% \pR(\vB) & = \smallset{B_i \mid i \in A,\ A \in E} \cup \smallset{B_i B_j \mid i \in
% A, j \in B,\ A \neq B, A, B \in E} \cup \\ 
% 		& \smallset{B_i B_j B_k \mid i \in A,\ j \in
% B,\ k \in C,\
% 		A, B, C \text{ all different and in } E} \cup \notag \\
% 	& \smallset{B_i B_j B_k B_l \mid i \in A,\ j \in B,\ k \in C,\ l \in D,\
% 		A, B, C, D \text{ all different and in } E} \notag \\
% 	& \setminus \smallset{B_1
% B_3 B_5 B_7}\,,\notag \\
% \pS(\vB) & = \emptyset \notag\,, \\
% \pT(\vB) & = \emptyset \notag\,, \\
% \pf(\vB) & = B_1 B_3 B_5 B_7 \notag\,.
% \end{align}
% That is, the elements of $\pR$ are all singletons, pairs, triplets and
% quadruplets of $B_i$ variables that occur in polynomial $\pt(\vB)$ except the
% challenge element $\pf(\vB) = B_1 B_3 B_5 B_7$. Variables $\vB$ are evaluated to
% randomly picked $\vb = b_1, \ldots, b_9$.

% The reduction $\rdv$ learns $\gone{\pR}$ and challenge $\gone{w}$ where $w$ is
% either $\pf(\vb) = b_1 b_3 b_5 b_7$ or a random value. Then it picks $\chi$,
% $\chz$ and computes the SRS $\srs$ from $\chi$. Elements $b_i$ are interpreted as
% polynomials in $X$ that are evaluated at $\chi$, i.e. $b_i = b_i(\chi)$. Next,
% $\rdv$ sets
% \[
%   \gone{\p{\tb}_i}(X) =
% (X - \chz)(X - \ochz) \gone{b_i}(X) + \xi_i (X - \chz) \gone{1} +
% \zeta_i (X - \ochz) \gone{1}, % \text{ for } i \in % \range{1}{9}, u_1
% \] 
% for $i \in \range{1}{9}$ and $\xi_i, \zeta_i \sample \FF_p$. Denote by $\tb_i$
% evaluations of $\p{\tb}_i$ at $\chi$.
% The reduction computes all $\gone{\tb_i \tb_j}, \gone{\tb_i \tb_j \tb_k},
% \gone{\tb_i \tb_j \tb_k \tb_l}$ such that
% $\gone{B_i B_j, B_i B_j B_k, B_i B_j B_k B_l} \in \pR$.
% This is possible since $\rdv$ knows all singletons $\gone{b_1, \ldots,
% b_9}$ and pairs $\gone{b_i b_j} \in \pR$ which can be used to compute all
% required pairs $\gone{\tb_i \tb_j}$: 
% \begin{align*}
% \gone{\tb_i \tb_j} 
% & = ((\chi - \chz)(\chi - \ochz)\gone{b_i} + \xi_i (\chi - \chz)\gone{1} +
% \zeta_i (\chi - \ochz) \gone{1}) 
% \cdot \\
%  & ((\chi - \chz)(\chi - \ochz)\gone{b_j} + \xi_j (\chi - \chz)\gone{1} +
% \zeta_j (\chi - \ochz) \gone{1}) = \\
%  & ((\chi - \chz)(\chi - \ochz))^2 \gone{b_i b_j} +  ((\chi - \chz)(\chi -
%  \ochz)\gone{b_i} (\xi_j (\chi - \chz) \gone{1} + \zeta_j (\chi - \ochz)
%  \gone{1}) + \\
%  & ((\chi - \chz)(\chi -
%  \ochz)\gone{b_j} (\xi_i (\chi - \chz) \gone{1} + \zeta_i (\chi - \ochz)
%  \gone{1}) + \psi,
% \end{align*}
% where $\psi$ compounds of $\xi_i, \xi_j, \zeta_i, \zeta_j, \chz, \ochz, \chi$ which
% are all known by $\rdv$ and no $b_i$ nor $b_j$.
% Analogously for the triplets and quadruplets. 

% For the challenge $\gone{w}$, $\rdv$ sets $\gone{\tw} = \gone{\tb_1 \tb_3
% \tb_5 \tb_7}$, where $\gone{b_1 b_3 b_5 b_7}$ is substituted by $\gone{w}$.
% Next it runs the adversary $\adv(\REL, \srs)$ and obtains from $\adv$ an
% instance--witness pair $(\inp, \wit)$.  $\rdv$ now prepares a simulated proof as follows:
% \begin{description} 
% \item[Round 1] $\rdv$ computes $\gone{\pa(\chi)}$ using as
% randomisers $\gone{\tb_1}, \gone{\tb_2}$ and setting $\wit_i = 0$, for $i
% \in \range{1}{3 \noofc}$. Similarly it computes
% $\gone{\pb(\chi)}, \gone{\pc(\chi)}$.  $\rdv$ publishes the obtained values
% and picks a Round 1 challenge $\beta, \gamma$ at random.  
% \item[Round 2]
% $\rdv$ computes $\gone{\pz(\chi)}$ using $\tb_7, \tb_8, \tb_9$ and publishes
% it. Then it picks randomly the challenge $\alpha$.  
% \item[Round 3] The
% reduction computes $\gone{\pt(\chi)}$ using $\tw$ as it was equal $\tb_1
% \tb_3 \tb_5 \tb_7$. That is, if $w = b_1 b_3 b_5 b_7$ then $\pt(\chi)$ is as
% computed by the simulator $\simulator_\chi$, otherwise, if $w$ is random
% then $\pt(\chi)$ is random as well, thus it is computed as $\simulator$
% would compute. The reduction computes and outputs $\gone{\ptlo(\chi),
% \ptmid(\chi), \pthi(\chi)}$ such that $\pt(X) = \ptlo(X) +
% X^\noofc \ptmid(X) + X^{2\noofc} \pthi(X)$.
% Eventually, $\rdv$ outputs $\chz$.  
% \item[Round 4] The reduction outputs
% $\pa(\chz), \pb(\chz), \pc(\chz), \p{S_{\sigma 1}}(\chz), \p{S_{\sigma 2}
% 	(\chz)}, \pt(\chz), \pz(\ochz)$.  For the sake of concreteness, denote by
% 	$S = \smallset{\pa, \pb, \pc, \pt, \pz}$. Although for a polynomial $\p{p}
% 	\in S$, reduction $\rdv$ does not know $\p{p}(\chi)$ or even do not know
% 	all the coefficients of $\p{p}$, the polynomials in $S$ was computed such
% 	that the reduction always knows their evaluation at $\chz$ and $\ochz$.
% \item[Round 5] $\rdv$ computes the openings of the polynomial commitments
% assuring that evaluations at $\chz$ it provided were computed honestly.
% \end{description} Is the adversary $\adv$'s output distribution differ in
% Game $\game{1}$ and $\game{2}$ then the reduction uses it to distinguish
% between $w = b_1 b_3 b_5 b_7$ and $w$ being random, thus \(
% \abs{\prob{\game{1}} - \prob{\game{2}}} \leq \epsuber.  \) Eventually, \(
% \abs{\prob{\game{0}} - \prob{\game{2}}} \leq \epszk + \epsuber.  \) \qed
\end{proof}

\subsection{From special-soundness and unique response property to special simulation
  extractability of $\plonkprotfs$}

Since \cref{lem:plonkprot_ur,lem:plonkprot_ss} hold, $\plonkprot$ is $\ur{3}$
and computationally special sound. We now 
make use of \cref{thm:se} and show that
$\plonkprot_\fs$ is special simulation-extractable as defined in \cref{def:simext}.

\begin{corollary}[Special simulation extractability of $\plonkprot_\fs$]
\label{thm:plonkprotfs_se}
Assume that $\plonkprot$ is $\ur{3}$ with security $\epsur(\secpar)$, and
computational special-sound with security $\epsss(\secpar)$. Let $\ro\colon
\bin^* \to \bin^\secpar$ be a random oracle. Let $\advse$ be a $\ppt$ adversary
that can make up to $q$ random oracle queries and outputs an acceptable proof
for $\plonkprotfs$ with probability at least $\accProb$. Then $\plonkprotfs$ is
special simulation-extractable with extraction error $\eta = \epsur$. The extraction
probability $\extProb$ is at least
\[
	\extProb \geq \frac{1}{q^{\noofc + 2}} (\accProb - \epsur)^{\noofc + 3} -\eps\,,
\]
for some negligible $\eps$ and $\noofc$ being the number of
constrains in the proven circuit.
\end{corollary}

\section{Special simulation extractability of $\sonicprotfs$}
\label{sec:sonic}
\subsection{\sonic{} protocol rolled out}
In this section we present $\sonic$'s constraint system and algorithms. Reader
familiar with them may jump directly to the next section.

\subsubsection{The constraint system}
\label{sec:sonic_constraint_system}
\sonic's system of constraints composes of three $\multconstr$-long vectors
$\va, \vb, \vc$ which corresponds to left and right inputs to multiplication
gates and their outputs. It hence holds $\va \cdot \vb = \vc$.

There is also $\linconstr$ linear constrains of the form
\[
  \va \vec{u_q} + \vb \vec{v_q} + \vc \vec{w_q} = k_q,
\]
where $\vec{u_q}, \vec{v_q}, \vec{w_q}$ are vectors for the $q$-th linear
constraint with instance value $k_q \in \FF_p$. Furthermore define polynomials
\begin{equation}
  \begin{split}
    \p{u_i}(Y) & = \sum_{q = 1}^\linconstr Y^{q + \multconstr} u_{q, i}\,,\\
    \p{v_i}(Y) & = \sum_{q = 1}^\linconstr Y^{q + \multconstr} v_{q, i}\,,\\
  \end{split}
  \qquad
  \begin{split}
    \p{w_i}(Y) & = -Y^i - Y^{-i} + \sum_{q = 1}^\linconstr Y^{q +
      \multconstr} w_{q, i}\,,\\
    \p{k}(Y) & = \sum_{q = 1}^\linconstr Y^{q + \multconstr} k_{q}.
  \end{split}
\end{equation}
In \sonic{} we will use commitments to the following polynomials.
\begin{align*}
  \pr(X, Y) & = \sum_{i = 1}^{\multconstr} \left(a_i X^i Y^i + b_i X^{-i} Y^{-i}
              + c_i X^{-i - \multconstr} Y^{-i - \multconstr}\right) \\
  \p{s}(X, Y) & = \sum_{i = 1}^{\multconstr} \left( u_i (Y) X^{-i} +
                v_i(Y) X^i + w_i(Y) X^{i + \multconstr}\right)\\
  \pt(X, Y) & = \pr(X, 1) (\pr(X, Y) + \p{s}(X, Y)) - \p{k}(Y)\,.
\end{align*}
	
\subsubsection{Algorithms rolled out}
\paragraph{$\sonic$ SRS generation $\kgen(\REL)$.} The SRS generating algorithm picks
randomly $\alpha, \chi \sample \FF_p$ and outputs
	\[
      \srs = \left( \gone{\smallset{\chi^i}_{i = -\dconst}^{\dconst},
          \smallset{\alpha \chi^i}_{i = -\dconst, i \neq 0}^{\dconst}},
        \gtwo{\smallset{\chi^i, \alpha \chi^i}_{i = - \dconst}^{\dconst}},
        \gtar{\alpha} \right)
	\]
\paragraph{$\sonic$ prover $\prover(\REL, \srs, \inp, \wit=\va, \vb, \vc)$.}
\begin{description}
\item[Round 1] The prover picks randomly randomisers
  $c_{\multconstr + 1}, c_{\multconstr + 2}, c_{\multconstr + 3}, c_{\multconstr
    + 4} \sample \FF_p$. Set
  $\pr(X, Y) \gets \pr(X, Y) + \sum_{i = 1}^4 c_{\multconstr + i} X^{- 2
    \multconstr - i}$. Commits to $\pr(X, 1)$ and outputs
  $\gone{r} \gets \com(\srs, \multconstr, \pr(X, 1))$.  Then it gets challenge $y$ from
  the verifier.
\item[Round 2] $\prover$ commits to $\pt(X, y)$ and outputs
  $\gone{t} \gets \com(\srs, \dconst, \pt(X, y))$. Then it gets a challenge $z$ from
  the verifier.
\item[Round 3] The prover computes commitment openings. That is, it outputs
  \begin{align*}
    \gone{o_a} & = \open(\srs, z, \pr(z, 1), \pr(X, 1)) \\
    \gone{o_b} & = \open(\srs, yz, \pr(yz, 1), \pr(X, 1)) \\
    \gone{o_t} & = \open(\srs, z, \pt(z, y), \pt(X, y)) 
  \end{align*}
  along with evaluations $a' = \pr(z, 1), b' = \pr(y, z), t' = \pt(z, y)$.  Then it
  engages in the signature of correct computation playing the role of the
  helper, i.e.~it commits to $\p{s}(X, y)$ and sends the commitment $\gone{s}$, commitment opening
  \begin{align*}
    \gone{o_s} & = \open(\srs, z, \p{s}(z, y), \p{s}(X, y)), \\
  \end{align*} and $s'=\p{s}(z, y)$. 
%
  Then
  it obtains a challenge $u$ from the verifier.
\item[Round 4] In the next round the prover computes
  $\gone{c} \gets \com(\srs, \dconst, \p{s}(u, Y))$ and
  computes commitments' openings
  \begin{align*}
    \gone{w} & = \open(\srs, u, \p{s}(u, y), \p{s}(X, y)), \\
    \gone{q_y} & = \open(\srs, y,\p{s}(u, y), \p{s}(u, Y)),
  \end{align*}
  and returns $\gone{w}, \gone{q_y}, s = \p{s}(u, y)$. Eventually the prover gets the last challenge
  from the verifier---$z'$.
\item[Round 5] In the final round, $\prover$ computes opening
  $\gone{q_{z'}} = \open(\srs, z', \p{s}(u, z'), \p{s}(u, X))$ and outputs $\gone{q_{z'}}$.
\end{description}

\paragraph{$\sonic$ verifier $\verifier(\REL, \srs, \inp, \zkproof)$.} The verifier
in \sonic{} runs as subroutines the verifier for the polynomial commitment. That
is it sets $t' = a'(b' + s') - \p{k}(y)$ and checks the following:
\begin{equation*}
  \begin{split}
    &\PCOMs.\verifier(\srs, \multconstr, \gone{r}, z, a', \gone{o_a}), \\
    &\PCOMs.\verifier(\srs, \multconstr, \gone{r}, yz, b', \gone{o_b}),\\
    &\PCOMs.\verifier(\srs, \dconst, \gone{t}, z, t', \gone{o_t}),\\
    &\PCOMs.\verifier(\srs, \dconst, \gone{s}, z, s', \gone{o_s}),\\
  \end{split}
  \qquad
  \begin{split}
    &\PCOMs.\verifier(\srs, \dconst, \gone{s}, u, s, \gone{w}),\\
    &\PCOMs.\verifier(\srs, \dconst, \gone{c}, y, s, \gone{q_y}),\\
    &\PCOMs.\verifier(\srs, \dconst, \gone{c}, z', \p{s}(u, z'), \gone{q_{z'}}),
  \end{split}
\end{equation*}
and accepts the proof iff all the checks holds. Note that the value $\p{s}(u, z')$ that is recomputed by the verifier uses separate challenges $u$ and $z'$. This enables the batching of many proof and the outsourcing of this part of the proof to an untrusted helper. 

\subsection{Unique opening property of $\PCOMs$}
\begin{lemma}
\label{lem:pcoms_unique_op}
$\PCOMs$ has the unique opening property in the AGM. 
\end{lemma}
\begin{proof}
Let 
$z \in \FF_p$ be the attribute the polynomial is evaluated at,
$\gone{c} \in \GRP$ be the commitment,  
$s \in \FF_p$ the evaluation value, and 
$o \in \GRP$ be the commitment opening. 
We need to show that for every $\ppt$ adversary $\adv$ probability
\[
  \Pr \left[
    \begin{aligned}
      & \verify(\srs, \gone{c}, z, s, \gone{o}) = 1, \\
      & \verify(\srs, \gone{c}, z, \tilde{s}, \gone{\tilde{o}}) = 1
    \end{aligned}
    \,\left|\, \vphantom{\begin{aligned}
          & \verify(\srs, \gone{c}, z, s, \gone{o}),\\
          & \verify(\srs, \gone{c}, z, \tilde{s}, \gone{\tilde{o}}) \\
          &o \neq \tilde{o})
		\end{aligned}}
      \begin{aligned}
        & \srs \gets \kgen(\secparam, \maxdeg), \\
        & (\gone{c}, z, s, \tilde{s}, \gone{o}, \gone{\tilde{o}}) \gets \adv(\srs)
      \end{aligned}
    \right.\right]
  % \leq \negl.
\]
is at most negligible.

As noted in \cite[Lemma 2.2]{EPRINT:GabWilCio19} it is enough to upper bound the
probability of the adversary succeeding using the idealised verification
equation---which considers equality between polynomials---instead of the real
verification equation---which considers equality of the polynomials' evaluations.

For a polynomial $f$, its degree upper bound $\maxconst$, evaluation point $z$,
evaluation result $s$, and opening $\gone{o(X)}$ the idealised check verifies that
\begin{equation}
  \alpha (X^{\dconst - \maxconst}f(X) \cdot X^{-\dconst + \maxconst} -  s) \equiv \alpha \cdot o(X) (X - z)\,,
\end{equation}
what is equivalent to 
\begin{equation}
	f(X) -  s \equiv o(X) (X - z)\,.
	\label{eq:pcoms_idealised_check}
\end{equation}
Since $o(X)(X - z) \in \FF_p[X]$ then from the uniqueness of polynomial
composition, there is only one $o(X)$ that fulfils the equation above.
\qed
\end{proof}


\subsection{Unique response property}
The unique response property of $\sonicprot$ follows from the unique opening
property of the used polynomial commitment scheme $\PCOMs$.
\begin{lemma}
\label{lem:sonicprot_ur}
If a polynomial commitment scheme $\PCOMs$ is evaluation binding with
parameter $\epsbind$ and has unique openings property with parameter
$\epsop$, then $\sonicprot$ is $\ur{2}$ with parameter $\epsur \leq
\epsbind + \epsop$.  
\end{lemma}
\begin{proof}
  Let $\adv$ be an adversary that breaks $\ur{2}$-ness of $\sonicprot$.  We
  consider two cases, depending on which round $\adv$ is able to provide at
  least two different outputs such that the resulting transcripts are
  acceptable.  For the first case we show that $\adv$ can be used to break the
  evaluation binding property of $\PCOMs$, while for the second case we show
  that it can be used to break the unique opening property of $\PCOMs$.

  The proof goes similarly to the proof of \cref{lem:plonkprot_ur} thus we
  provide only draft of it here.  In each Round $i$, for $i > 1$, the prover
  either commits to some well-defined polynomials (deterministically), evaluates
  these on randomly picked points, or shows that the evaluations were performed
  correctly.  Obviously, for a committed polynomial $\p{p}$ evaluated at point
  $x$ only one value $y = \p{p}(x)$ is correct. If the adversary was able to
  provide two different values $y$ and $\tilde{y}$ that would be accepted as an
  evaluation of $\p{p}$ at $x$ then the $\PCOMs$'s evaluation binding would be
  broken.  Alternatively, if $\adv$ was able to provide two openings $\p{W}$ and
  $\p{\tilde{W}}$ for $y = \p{p}(x)$ then the unique opening property would be
  broken.
%
Hence the probability that $\adv$ breaks $\ur{2}$-property of $\PCOMs$ is
upper-bounded by $\epsbind + \epsop$. 
\qed

\end{proof}

\subsection{Computational special soundness}
\begin{lemma}
	\label{lem:sonicprot_ss}
	Let $\adv$ be a $\ppt$ algebraic adversary. The probability $\epsss$ that
  $\adv$ breaks computational special soundness of $\sonicprot$ is upper-bounded
  as
	\[
			\epsss \leq \epss + \epsldlog\,,
	\]
	where $\epss$ is a soundness error of the protocol, and $\epsldlog$ is a
  probability that a $\ppt$ algorithm can break the $(\dconst,
  \dconst)$-$\ldlog$ assumption.
\end{lemma}
\begin{proof}
  The proof goes similarly to the proof of \cref{lem:plonkprot_ss}.
%
  Let $\adv$ be an adversary that produces a
  $(1, \multconstr + \linconstr + 1, 1, 1)$-tree of acceptable transcripts
  $\tree$ for a statement $\inp$. We consider two disjunctive events $\event{E}$
  and $\nevent{E}$. The first corresponds to a case when all transcripts in
  $\tree$ are acceptable for the ideal verifier,
  i.e.~$\vec{\vereq}(X) = \vec{0}$. In that case we show an extractor $\extss$
  that from $\tree$ extracts a valid witness $\wit$. The second, corresponds to
  a case when $\tree$ contains a transcript that is acceptable by the real
  verifier but is not acceptable by the ideal verifier.  In that case we show a
  reduction $\rdvldlog$ that uses $\adv$ to break the
  $(\dconst, \dconst)$-$\ldlog$ assumption.

  \ncase{When $\event{E}$ happens} Since $\sonicprot$ is statistically sound
  regarding the ideal verifier, for an acceptable proof $\zkproof$ for a
  statement $\inp$ there exists a witness $\wit$ such that $\REL(\inp, \wit)$
  holds and the polynomial $\p{r}(X, y)$ contains witness at its coefficients.
  Note that the polynomial $\p{r}(X, y)$, which has witness elements at its
  coefficients, has degree at most $\multconstr + \linconstr$ and since $\adv$
  answered correctly on $(\multconstr + \linconstr + 1)$ different challenges
  $z$ (for the sake of concreteness let us call them
  $z_1, \ldots, z_{\multconstr + \linconstr + 1}$) then
  $(\multconstr + \linconstr + 1)$ evaluations
  $\p{r}(z_1, y), \ldots, \p{r}(z_{\multconstr + \linconstr + 1}, y)$ of
  $\p{r}(X, y)$ are known. The extractor $\extss$ interpolates $\p{r}(X, y)$ and
  reveals the corresponding witness $\wit$.

  \ncase{When $\nevent{E}$ happens} Consider a transcript such that for some
  verification equation $\vereq_i(X) \neq 0$, but $\vereq_i(\chi) = 0$. Since
  the adversary is algebraic, all group elements included in the tree of
  transcripts are extended by their representation as a combination of the input
  $\GRP_1$ or $\GRP_2$-elements. Hence all coefficients of the verification
  equation polynomial $\vereq_i(X)$ are known and $\rdvdlog$ can find its zero
  points. Since $\vereq_i(\chi) = 0$, the targeted discrete log value $\chi$ is
  among them. \qed
\end{proof}

\subsection{Honest verifier zero-knowledge}
\begin{lemma}
\label{lem:sonic_hvzk}
$\sonic$ is honest verifier
zero-knowledge.	
\end{lemma}
% Here we show that $\sonic$ is trapdoor-less zero-knowledge.
\begin{proof}
The simulator proceeds as follows.
In the first round, it picks randomly vectors $\vec{a}$, $\vec{b}$ and sets
\begin{equation}
	\label{eq:ab_eq_c}
	\vec{c} = \vec{a} \cdot \vec{b}. 
\end{equation}
Then it pick randomisers $c_{\multconstr + 1}, \ldots, c_{\multconstr + 4}$,
honestly computes polynomials $\p{r}(X, Y), \p{r'}(X, Y), \p{s}(X, Y)$ and
$\p{t}(X, Y)$ and concludes the first round as an honest prover would. Because
of the randomisers the polynomial $\pr(X, Y)$ computed by the simulator is
indistinguishable from a polynomial provided by an honest user for an adversary
that only learns $\gone{r},\gone{t}, a', b'$ from the proof (the other proof elements are
fixed by these elements and the public information,
see~\cref{sec:sonic_constraint_system}).

Next, $\simulator$ computes the first verifier's challenge $y$ such that
$\pt(X, y)$ is a polynomial that has $0$ as a coefficient next to $X^0$.
I.e.~$\pt(0, y) = 0$. By the definition of $\pt(X, Y)$, the coefficient next to
$X^0$ in $\p{t}(X, Y)$ equals
\begin{equation}
	\label{eq:x0}
	\pt(0,Y) = 
	\vec{a} \cdot \vec{\p{u}}(Y) + 
	\vec{b} \cdot \vec{\p{v}}(Y) + 
	\vec{c} \cdot \vec{\p{w}}(Y) +	
	\sum_{i = 1}^{\multconstr} a_i b_i (Y^i + Y^{-i}) - \p{k}(Y), 
\end{equation}
for public $\vec{\p{u}}(Y), \vec{\p{v}}(Y), \vec{\p{w}}(Y), \p{k}(Y)$ as defined
in \cref{sec:sonic_constraint_system} (Vectors $\vec{u_q}, \vec{v_q}, \vec{w_q}$
are $\multconstr$-elements long and correspond to $\linconstr$ linear constrains of
the proof system. Field element $k_{q}$ is the instance value). When the proven
instance is correct, $\pt(0,Y)$ is a zero polynomial. See \cite{CCS:MBKM19} for
details. Also, when \cref{eq:ab_eq_c} holds, that polynomial simplifies to
\begin{equation}
	\label{eq:x0_simpl}
	\pt(0,Y) = \vec{a} \cdot \vec{\p{u}}(Y) + 
	\vec{b} \cdot \vec{\p{v}}(Y) + 
	\vec{c} \cdot \vec{\p{\tilde{w}}}(Y)	
	- \p{k}(Y), 
\end{equation}
where $\vec{\p{\tilde{w}}}(Y)$ is defined as
\[
	\p{\tilde{w}_i}(Y) = \sum_{q = 1}^\linconstr Y^{q +
	\multconstr} w_{q, i}\,.
\]
Note that $\pt(0, Y)$ is a ``classical'', i.e.~non-Laurent, polynomial. Also,
it is a polynomial of degree $\linconstr + \multconstr$ with $0$
coefficients for $Y^{i}$, for $i \in \range{0}{\multconstr}$. Also, since
$\vec{a}, \vec{b}$ were picked at random, $\pt(0,Y) / Y^{\multconstr + 1}$ is a
degree-$(\linconstr - 1)$ polynomial of random coefficients. Recall, that the
view of the adversary is independent of $\vec{a}, \vec{b}$ because of
randomisers $c_{\multconstr + 1}, \ldots, c_{\multconstr + 4}$.

The probability that a random degree-$(\linconstr - 1)$ polynomial over
$\FF_p[Y]$ has a root is at least $\infrac{1}{(\linconstr - 1) !}$, see
\cref{lem:root_prob} for a proof of that bound. Since we assume that $\linconstr
= \poly$, we can say that the polynomial $\pt(0,Y)$ as computed by the simulator
has roots with non-negligible probability. Furthermore, these roots can be found
and the simulator picks fresh $\vec{a}, \vec{b}$ until $\pt(0,Y)$ has a root. As
the roots of a random polynomial are themselves random $\FF_p$ elements, the
challenge $y$ picked by the simulator comes from the same distribution as if it
was picked by an honest verifier.

The simulator continues building the transcript by honestly computing the
prover's messages and by picking verifier's challenges at random. This and the
fact that $\pt(0,y) = 0$ guarantees that the transcript provided by the simulator is
acceptable and comes from the same distribution as a transcript of an honest
prover and verifier. \qed
\end{proof}

\begin{remark} 
  As noted in \cite{CCS:MBKM19}, $\sonic$ is statistically subversion-zero
  knowledge (Sub-ZK). As noted in \cite{AC:ABLZ17}, one way to achieve
  subversion zero knowledge is to utilise an extractor that extracts a SRS
  trapdoor from a SRS-generator. Unfortunately, a NIZK made subversion
  zero-knowledge by this approach cannot achieve perfect Sub-ZK as one has to
  count in the probability of extraction failure. However, with the simulation
  presented in \cref{lem:sonic_hvzk}, the trapdoor is not required for the
  simulator as it is able to simulate the execution of the protocol just by
  picking appropriate (honest) verifier's challenges. This result transfers to
  $\sonicprotfs$, where the simulator can program the random oracle to provide
  challenges that fits it.
\end{remark}

\subsection{From special-soundness and unique response property to special simulation extractability of $\sonicprotfs$}
Since \cref{lem:sonicprot_ur,lem:sonicprot_ss} hold, $\sonicprot$ is $\ur{2}$
and computationally special sound. We now make use
of \cref{thm:se} and show that $\sonicprotfs$ is special simulation-extractable as defined in \cref{def:simext}.

\begin{corollary}[Special simulation extractability of $\sonicprotfs$]
  \label{thm:sonicprotfs_se}
  Assume that $\sonicprot$ is $\ur{2}$ with security $\epsur(\secpar)$, and
  computational special-sound with security $\epsss(\secpar)$. Let $\ro\colon
  \bin^* \to \bin^\secpar$ be a random oracle. Let $\advse$ be a $\ppt$
  adversary that can make up to $q$ random oracle queries and outputs an
  acceptable proof for $\sonicprotfs$ with probability at least $\accProb$. Then
  $\sonicprotfs$ is special simulation-extractable with extraction error $\eta =
  \epsur$. The extraction probability $\extProb$ is at least
\[
		\extProb  \geq \frac{1}{q^{\multconstr + \linconstr}} (\accProb - \epsur)^{\multconstr +
		\linconstr + 1} - \eps.
	\]
	for some negligible $\eps$, $\multconstr$ and $\linconstr$ being,
  respectively, the number of multiplicative and linear constrains of the system.
\end{corollary}


% \markulf{03.11.2020}{Comment again at this being huge? Maybe mention
% simulation-soundness result?}

%\begin{proof}
% 	The theorem holds as a corollary to \cref{thm:se}. The proof goes by game
%  hoping. The games are controlled by an environment $\env$ that internally
%  runs a simulation extractability adversary $\advse$, simulates its with
%  access to a random oracle and simulator, and when necessary rewinds it. The
%  games differ by various breaking points, i.e.~points where the environment
%  decides to abort the game.
% 
% Denote by $\zkproof_{\advse}, \zkproof_{\simulator}$ proofs
% returned by the adversary and the simulator respectively. We use $\zkproof[i]$
% to denote prover's message in the $i$-th round of the proof, $\zkproof[i].\ch$
% to denote the challenge that is given to the prover after $\zkproof[i]$, and
% $\zkproof\range{i}{i'}$ to denote all messages of the proof exchanged between rounds $i$ and $i'$, i.e.~$\zkproof[i], \zkproof[i].\ch, \ldots, \zkproof[i']$.
% 
% Without loss of generality, we assume that whenever the accepting proof
% contains a response to a challenge from a random oracle, we assume that the
% adversary queried the oracle to get it. It is straightforward to transform any
% adversary that violates this condition into an adversary that makes these
% additional queries to the random oracle and wins with the same probability.
% 
% \ngame{0}
% This is a simulation extraction game played between an adversary $\advse$ who
% has given access to a random oracle $\ro$ and simulator
% $\plonkprotfs.\simulator$.  There is also an extractor $\ext$ that, from the
% proof $\zkproof_\advse$ for instance $\inp_\advse$ output by the adversary and
% from a transcripts of $\advse$'s operations, is tasked to extract a witness
% $\wit_\advse$ such that $\REL(\inp_\advse, \wit_\advse)$ holds. $\advse$ wins
% if it manages to produce an acceptable proof and the extractor fails to reveal
% the corresponding witness. In the following game hops we upper-bound
% probability of this happening.
% 
% \ngame{1}
% This is identical to $\game{0}$ except that now the game is aborted if there
% is a simulated proof $\zkproof_\simulator$ such that
% $\zkproof_\simulator\range{1}{3} = \zkproof_\advse\range{1}{3}$. That is, the
% adversary in its final proof reuses a part of a simulated proof it saw before
% and the proof is acceptable. Denote that event by $\event{\errur}$.
% 
% \ncase{$\game{0} \mapsto \game{1}$}	
% We have, 
% \[
% 	\prob{\game{0} \land \nevent{\errur}} = \prob{\game{1} \land \nevent{\errur}}
% \]
% and, from \cref{lem:difference_lemma}
% \[
% 	\abs{\prob{\game{0}} - \prob{\game{1}}} \leq \event{\errur}\,.
% \]
% Thus, to show that the transition from one game to another introduces only
% minor change in probability of $\advse$ winning it should be shown that
% $\prob{\event{\errur}}$ is small.
% 
% Assume that $\advse$ queried the simulator on $\inp_{\advse}$---the instance
% which $\advse$ outputs. We show a reduction $\rdvur$ that utilises $\advse$,
% who outputs a valid proof for $\inp_\advse$, to break the $\ur{3}$ property of
% $\plonkprot$.
% 
% Consider an algorithm $\rdvur$ that runs $\advse$ internally as a black-box:
% \begin{itemize} \item The reduction answers both queries to the simulator
% $\plonkprotfs.\simulator$ and to the random oracle.  It also keeps lists $Q$,
% for the simulated proofs, and $Q_\ro$ for the random oracle queries.  \item
% When $\advse$ outputs a fake proof $\zkproof_{\advse}$ for  $\inp_\advse$,
% $\rdvur$ looks through lists $Q$ and $Q_\ro$ until it finds
% $\zkproof_{\simulator}\range{1}{3}$ such that $\zkproof_{\advse}\range{1}{3} =
% \zkproof_{\simulator}\range{1}{3}$ and a random oracle query
% $\zkproof_{\simulator}[3].\ch$ on $\zkproof_{\simulator}\range{1}{3}$. \item
% $\rdvur$ returns two proofs for $\inp_\advse$: \begin{align*} \zkproof_1 =
% (\zkproof_{\simulator}\range{1}{3}, \zkproof_{\simulator}[3].\ch,
% \zkproof_{\simulator}\range{4}{5})\\ \zkproof_2 =
% (\zkproof_{\simulator}\range{1}{3}, \zkproof_{\simulator}[3].\ch,
% \zkproof_{\advse}\range{4}{5}) \end{align*} \end{itemize}   If $\zkproof_1 =
% \zkproof_2$, then $\advse$ fails to break simulation extractability, as
% $\zkproof_2 \in Q$. On the other hand, if the proofs are not equal, then
% $\rdvur$ breaks $\ur{3}$-ness of $\plonkprot$, what may happen with some
% negligible probability $\epsur$ only, hence 
% \[ 
% \prob{\event{\errur}} \leq \epsur\,. 
% \]
% 
% \ngame{2} This is identical to $\game{1}$ except that now the environment
% aborts also when it fails to build a $(1, 1, 1, \noofc + 3,
% 1)$-tree of accepting transcripts $\tree$ by rewinding $\advse$. Denote that
% event by $\event{\errfrk}$. Note that for every acceptable proof
% $\zkproof_{\advse}$, we may assume that whenever $\advse$ outputs in Round
% $i$, for $i \in \smallset{4, 5}$, a message $\zkproof_{\advse}[i]$, then
% $\zkproof_{\advse}\range{1}{i}$ is a query to the random oracle that was made
% by the adversary, not the simulator\footnote{\cite{INDOCRYPT:FKMV12} calls
% these queries \emph{fresh}.}. That is, assume that is not true and for some
% query $\zkproof_{\advse}\range{1}{i}$ holds $\zkproof_{\advse}\range{1}{i} =
% \zkproof_\simulator\range{1}{i'}$, for $i, i' \in \smallset{4, 5}$, the proof
% is acceptable and not in $Q$, then the unique response property would be
% broken.
% 
% \ncase{$\game{1} \mapsto \game{2}$}	
% As previously, 
% \[
% 	\abs{\prob{\game{1}} - \prob{\game{2}}} \leq \event{\errfrk}\,.
% \]
% Denote by $\accProb$ the probability that $\advse$ outputs a proof which is
% acceptable and does not break $\ur{3}$-ness of $\plonkprot$. From the
% generalised forking lemma, cf.~\cref{lem:generalised_forking_lemma},
% \[
%   \prob{\event{\errfrk}} \leq 1 - \left(\frac{\accProb^{\noofc +
%     3}}{q^{\noofc + 2}} - \frac{\accProb \cdot
%     (\noofc + 2)}{2^\secpar}\right)\,.
% \]
% 
% \ngame{3} This is identical to $\game{2}$ except that now the environment uses
% the tree $\tree$ to extract the witness for the proven statement and aborts
% when it fails. Denote that event by $\event{\errss}$.
% 
% \ncase{$\game{2} \mapsto \game{3}$}	
% As previously, 
% \[
% 	\abs{\prob{\game{2}} - \prob{\game{3}}} \leq \event{\errss}\,.
% \]
% Since $\plonkprot$ is special-sound the probability that $\env$ fails in
% extracting the witness is upper-bounded by some negligible $\eps_\ss$.
% 
% In the last game, Game $\game{3}$, the environment aborts when it fails to
% extract the correct witness, hence the adversary $\advse$ cannot win. Thus, by
% the game-hoping argument,
% \[
%   \abs{\prob{\game{0}} - \prob{\game{3}}} \leq 1 -
% 	 \left(\frac{\accProb^{\noofc + 3}}{q^{\noofc +
%     2}} - \frac{\accProb \cdot (\noofc + 2)}{2^\secpar}\right) +
% 	 \epsur + \epsss\,.
% \]
% Thus the probability that extractor $\extss$ succeeds is at least
% \[
%   \left(\frac{\accProb^{\noofc + 3}}{q^{\noofc + 2}}
%     - \frac{\accProb \cdot (\noofc + 2)}{2^\secpar}\right) -
%   \epsur - \epsss\,.
% \]
% Since $\accProb$ is probability of $\advse$ outputting acceptable transcript
% that does not break $\ur{3}$-ness of $\plonkprot$, then $\accProb \leq
% \accProb + \epsur$, where $\accProb$ is the probability of $\advse$ outputing
% an acceptable proof as defined in \cref{def:simext}. It thus holds
% \[
% 	\label{eq:frk}
% 	\extProb \geq \frac{(\accProb - \epsur)^{\noofc +
%  3}}{q^{\noofc + 2}} - \underbrace{\frac{(\accProb - \epsur)
%  \cdot (\noofc + 2)}{2^\secpar} - \epsur - \epsss}_{\eps}\,.
% \]
% Note that the part of \cref{eq:frk} denoted by $\eps$ is negligible and 
% \[
%   \extProb \geq \frac{1}{q^{\noofc + 2}} (\accProb -
% 	 \epsur)^{\noofc + 3} -\eps\,.
% \] 
% thus
% $\plonkprot_\fs$ is simulation extractable with extraction error $\epsur$.
% 	\qed
% \end{proof}


\section{Further work}
We identify a number of problems which we left as further work. First of all,
the generalised version of the forking lemma presented in this paper can be
generalised even further to include protocols where $(n_1, \ldots,
n_\mu)$-special soundness holds for more than one $n_j > 1$. I.e.~to include
protocols that for witness extraction require transcripts that branch at more
than one point.

Although we picked $\plonk$ and $\sonic$ as examples for our framework, it is
not limited to SRS-based NIZKs. Thus, it would be interesting to apply it to
known so-called transparent zkSNARKs like Bulletproofs \cite{SP:BBBPWM18},
Aurora \cite{EC:BCRSVW19} or AuroraLight \cite{EPRINT:Gabizon19a}.

Since the rewinding technique and the forking lemma used to show simulation
extractability of $\plonkprotfs$ and $\sonicprotfs$ come with security loss,
it would be interesting to show SE of these protocols directly in the
algebraic group model.

Although we focused here only on zkSNARKs, it is worth to
investigating other protocols that may benefit from our framework, like
e.g.~identification schemes.

Last, but not least, this paper would benefit greatly if a more tight version
of the generalised forking lemma was provided. However, we have to note here
that some of the inequalities used in the proof are already tight, i.e.~for
specific adversaries, some of the inequalities are already equalities.

% \section*{Acknowledgement}
% The second author thanks Antoine Rondelet for helpful discussions.

\bibliographystyle{abbrv}
\bibliography{cryptobib/abbrev1,cryptobib/crypto,additional_bib}
%\clearpage
\appendix
%{\Huge{Supplementary Material}} 

\section{Special simulation-extractability of sigma protocols and forking lemma}
\label{sec:forking_lemma}
\begin{theorem}[Special simulation extractability of the Fiat--Shamir transform
  \cite{INDOCRYPT:FKMV12}]
	Let $\sigmaprot = (\prover, \verifier, \simulator)$ be a non-trivial sigma
  protocol with unique responses for a language $\LANG \in \npol$. In the random
  oracle model, the NIZK proof system $\sigmaprot_\fs = (\prover_\fs,
  \verifier_\fs, \simulator_{\fs})$ resulting by applying the Fiat--Shamir
  transform to $\sigmaprot$ is special simulation extractable with extraction error
  $\eta = q/h$ for the simulator $\simulator$. Here, $q$ is the number of random
  oracle queries and $h$ is the number of elements in the range of $\ro$.
\end{theorem}

The theorem relies on the following \emph{general forking lemma} \cite{JC:PoiSte00}.

\begin{lemma}[General forking lemma, cf.~\cite{INDOCRYPT:FKMV12,CCS:BelNev06}]
	\label{lem:forking_lemma}
	Fix $q \in \ZZ$ and a set $H$ of size $h > 2$. Let $\zdv$ be a $\ppt$
  algorithm that on input $y, h_1, \ldots, h_q$ returns $(i, s)$, where $i
  \in\range{0}{q}$ and $s$ is called a \emph{side output}. Denote by $\ig$ a
  randomised instance generator. We denote by $\accProb$ the probability
	\[
		\condprob{i > 0}{y \gets \ig; h_1, \ldots, h_q \sample H; (i, s) \gets
		\zdv(y, h_1, \ldots, h_q)}\,.
	\]
	Let $\forking_\zdv(y)$ denote the algorithm described in
  \cref{fig:forking_lemma}, then the probability $\frkProb$ defined as $
  \frkProb := \condprob{b = 1}{y \gets \ig; (b, s, s') \gets \forking_{\zdv}(y)}
  $ holds
	\[
		\frkProb \geq \accProb \brak{\frac{\accProb}{q} - \frac{1}{h}}\,.
	\]
	%
	\begin{figure}
		\centering
		\fbox{
		\procedure{$\forking_\zdv (y)$}
		{
			\rho \sample \RND{\zdv}\\
			h_1, \ldots, h_q \sample H\\
			(i, s) \gets \zdv(y, h_1, \ldots, h_q; \rho)\\
			\pcif i = 0\ \pcreturn (0, \bot, \bot)\\
			h'_{i}, \ldots, h'_{q} \sample H\\
			(i', s') \gets \zdv(y, h_1, \ldots, h_{i - 1}, h'_{i}, \ldots,  h'_{q};
			\rho)\\
			\pcif (i = i') \land (h_{i} \neq h'_{i})\ \pcreturn (1, s, s')\\
			\pcind \pcelse \pcreturn (0, \bot, \bot)
		}}
		\caption{Forking algorithm $\forking_\zdv$}
		\label{fig:forking_lemma}
\end{figure}
\end{lemma}



\section{Omitted lemmas and proofs}
\begin{lemma}\label{lem:jensen}
	Let $\RND{\zdv}$ denote the set from which $\zdv$ picks its coins at random.
	For each $\iota \in \range{1}{q}$ let $X_\iota \colon \RND{\zdv} \times
	H^{\iota - 1} \to [0, 1]$ be defined by setting $X_\iota(\rho, h_1, \ldots,
h_{\iota - 1})$ to 
\[
  \condprob{i = \iota}{h_\iota, \ldots, h_q \sample H; (i, s) \gets \zdv(y, h_1,
    \ldots, h_q; \rho)}
	\] 
	for all $\rho \in \RND{\zdv}$ and $h_1, \ldots, h_{\iota - 1} \in H$. Consider
  $X_\iota$ as a random variable over the uniform distribution on its domain.
  Then $\expected{X_\iota^m} \geq \expected{X_\iota}^m$.
\end{lemma}
\begin{proof}
	First we recall the Jensen inequality \cite{W:Weissten20}, if for some random
  variable $X$ holds $\abs{\expected{X}} \leq \infty$ and $f$ is a Borel convex
  function then
	\[
		f(\expected{X}) \leq \expected{f(X)}\,.
	\] 
	Finally, we note that $\abs{\expected{X}} \leq \infty$ and taking to the
  $m$-th power is a Borel convex function on $[0, 1]$ interval. \qed
\end{proof}

\begin{lemma}[H\"older's inequality. Simplified.]\label{lem:holder}
	Let $x_i, y_i$, for $i \in \range{1}{q}$, and $p, q$ be real numbers such that
  $1/p + 1/q = 1$. Then
	\[
		\sum_{i = 1}^{q} x_i y_i \leq \left(\sum_{i = 1}^{q}
      x_i^p\right)^{\frac{1}{p}} \cdot \left(\sum_{i = 1}^{q}
      y_i^p\right)^{\frac{1}{q}}\,.
	\]
\end{lemma}

\begin{remark}[Tightness of the H\"older inequality]
	In is important to note that Inequality (\ref{eq:tightness}) is tight. More
  precisely, for $\expected{X_i} = x$, $i \in \range{1}{q}$ we have
	\begin{gather*}
		\sum_{i = 1}^q x = \left(\sum_{i = 1}^{q} x^m\right)^\frac{1}{m} \cdot \left(\sum_{i = 1}^{q} 1^{\frac{m}{m - 1}}\right)^{\frac{m - 1}{m}} \\
		qx = \left(qx^m\right)^\frac{1}{m} \cdot q^{\frac{m - 1}{m}} \\
		(qx)^m = qx^m \cdot q^{m - 1} \\
		(qx)^m = (qx)^m\,.
	\end{gather*}
\end{remark}

\begin{lemma}
  \label{lem:root_prob}
  Let $\p{f}(X)$ be a random degree-$d$ polynomial over $\FF_p[X]$. Then the
  probability that $\p{f}(X)$ has roots in $\FF_p$ is at least $\infrac{1}{d!}$.
\end{lemma}
\begin{proof}
  First observe that there is $p^{d}$ canonical polynomials in $\FF_p[X]$.  Each
  of the polynomials may have up to $d$ roots. Consider polynomials which are
  reducible to polynomials of degree $1$, i.e.~polynomials that have all $d$
  roots. The roots can be picked in $\bar{C}^{p}_{d}$ ways, where
  $\bar{C}^{n}_{k}$ is the number of $k$-elements combinations with repetitions
  from $n$-element set. That is,
  \[
    \bar{C}^n_k = \binom{n + k - 1}{k}\,.
  \]
  Thus, the probability that a randomly picked polynomial has all $d$ roots is
  \begin{multline*}
    p^{-d} \cdot \bar{C}^p_d = p^{-d} \cdot \binom{p + d - 1}{d} =
    p^{-d} \cdot \frac{(p + d - 1)!}{(p + d - 1 - d)! \cdot d!} = \\
    p^{-d} \cdot \frac{(p + d - 1) \cdot \ldots \cdot p \cdot (p - 1)!}{(p - 1)!
      \cdot d!} = p^{-d} \cdot \frac{(p + d - 1)\cdot
      \ldots \cdot p}{d!} \\
    \geq p^{-d} \cdot {\frac{p^d}{d!}} = \frac{1}{d!}
  \end{multline*}
  \qed
\end{proof}

\subsection{Uber assumption}
\label{sec:uber_assumption}
We show security of our version of the uber assumption using the generic group
model as introduced by Shoup \cite{EC:Shoup97} where all group elements are
represented by random binary strings of length $\secpar$. That is, there are
random encodings $\xi_1, \xi_2, \xi_T$ which are injective functions from
$\ZZ_p^+$ to $\bin^{\secpar}$. We write
$\GRP_i = \smallset{\xi_i(x) \mid x \in \ZZ_p^+}$, for
$i \in \smallset{1, 2, T}$. For the sake of clarity  we denote by $\xi_{i, j}$
the $j$-th encoding in group $\GRP_i$.

Let
$\p{P}_i = \smallset{p_1, \ldots, p_{\tau_i}} \subset \FF_p[X_1, \ldots, X_n]$,
for $i \in \smallset{1, 2, T}, \tau_i, n \in \NN$, be sets of multivariate
polynomials. Denote by $\p{P}_i(x_1, \ldots, x_n)$ a set of evaluations of
polynomials in $\p{P_i}$ at $(x_1, \ldots, x_n)$. Denote by
$L_i = \smallset{(p_j, \xi_{i, j}) \mid j \leq \tau_i}$.

Let $\adv$ be an algorithm that is given encodings $\xi_{i, j_i}$ of polynomials
in $\p{P}_i$ for $i \in \smallset{1, 2, T}, j_i = \tau_i$. There is an oracle $\oracleo$
that allows to perform $\adv$ the following queries:
\begin{description}
\item[Group operations in $\GRP_1, \GRP_2, \GRP_T$:] On input
  $(\xi_{i, j}, \xi_{i, j'}, i, op)$, $j, j' \leq \tau_i$,
  $op \in \smallset{\msg{add}, \msg{sub}}$, $\oracleo$ sets $\tau'_i \gets \tau_i + 1$,
  computes
  $p_{i, \tau'_i} = p_{i, j}(x_1, \ldots, x_n) \pm p_{i, j'}(x_1, \ldots, x_n)$
  respectively to $op$. If there is an element  $p_{i, k} \in L_i$ such 
  that $p_{i, k} = p_{\tau'_i}$, then the oracle returns encoding of $p_{i,
    k}$. Otherwise it sets the encoding $\xi_{i, \tau'_i}$ to a new unused
  random string, adds $(p_{i, \tau'_i}, \xi_{i, \tau'_i})$ to $L_i$, and returns
  $\xi_{i, \tau'_i}$.
\item[Bilinear pairing:] On input $(\xi_{1, j}, \xi_{2, j'})$ the oracle sets
  $\tau' \gets \tau_T + 1$ and computes
  $r_{\tau'} \gets p_{i, j}(x_1, \ldots, x_n) \cdot p_{i, j'}(x_1, \ldots,
  x_n)$. If $r_{\tau'} \in L_T$ then return encoding found in the list $L_T$,
  else pick a new unused random string and set $\xi_{T, \tau'}$ to it. Return
  the encoding to the algorithm.
\end{description}

Given that, we are ready to show security of our variant of the Boneh et
al.~uber assumption. The proof goes similarly to the original proof given in
\cite{EC:BonBoyGoh05} with minor differences.

\begin{theorem}[Security of the uber assumption]
  \label{thm:uber_assumption}
  Let $\p{P}_i \in \FF_p[X_1, \ldots, X_n]^{m_i}$, for
  $i \in \smallset{1, 2, T}$ be $\tau_i$ tuples of $n$-variate polynomials over
  $\FF_p$ and let $\p{F} \in \FF_p[X_1, \ldots, X_n]^m$. Let
  $\xi_0, \xi_1, \xi_T$, $\GRP_1, \GRP_2, \GRP_T$ be as defined above. If
  polynomials $f \in \p{F}$ are pair-wise independent and are independent of
  $\p{P}_1, \p{P}_2, \p{P}_T$, then for any $\adv$ that makes up to $q$ queries to the
  GGM oracle holds:
  \begin{equation*}
    \begin{split}
     \left|\,
    \Pr\left[
    \adv\left(
      \begin{aligned}
        \xi_1(\p{P}_1(x_1, \ldots, x_n)), \\
        \xi_2(\p{P}_2(x_1, \ldots, x_n)), \\
        \xi_T(\p{P}_T(x_1, \ldots, x_n)), \\
        \xi_{1}(\p{F}_0), \xi_{1}(\p{F}_1)
      \end{aligned}
    \right) = b
    \, \left|\,
      \begin{aligned}
        x_1, \ldots, x_n, y_1, \ldots, y_m \sample \FF_p,\\
        b \sample \bin, \\
        \p{F}_b \gets \p{F}(x_1, \ldots, x_n),\\
        \p{F}_{1 - b} \gets (y_1, \ldots, y_m)
      \end{aligned}
    \right.  \right] - \frac{1}{2} \, \right| \\
     \leq \frac{d(q + m_1 + m_2 + m_T +
      m)^2 }{2p}
    \end{split}
  \end{equation*}
\end{theorem}
\begin{proof}
  Let $\cdv$ be a challenger that plays with $\adv$ in the following
  game. $\cdv$ maintains three lists
  \[
    L_i = \smallset{(p_j, \xi_{i, j}) \mid j \in \range{1}{\tau_i}},
  \]
  for $i \in \smallset{1, 2, T}$. Invariant $\tau$ states that
  $\tau_1 + \tau_2 + \tau_T = \tau + m_1 + m_2 + m$.

  Challenger $\cdv$ answers $\adv$'s oracle queries. However, it does it a bit
  differently that the oracle $\oracleo$ would:
  \begin{description}
  \item[Group operations in $\GRP_1, \GRP_2, \GRP_T$:] On input
    $(\xi_{i, j}, \xi_{i, j'}, i, op)$, $j, j' \leq \tau_i$,
    $op \in \smallset{\msg{add}, \msg{sub}}$, $\cdv$ sets
    $\tau' \gets \tau_i + 1$, computes
    $p_{i, \tau'}(X_1, \ldots, X_n) = p_{i, j}(X_1, \ldots, X_n) \pm p_{i,
      j'}(X_1, \ldots, X_n)$ respectively to $op$. If there is a polynomial
    $p_{i, k}(X_1, \ldots, X_n) \in L_i$ such that
    $p_{i, k}(X_1, \ldots, X_n) = p_{\tau'}(X_1, \ldots, X_n)$, then the
    challenger returns encoding of $p_{i, k}$. Otherwise it sets the encoding
    $\xi_{i, \tau'}$ to a new unused random string, adds
    $(p_{i, \tau'}, \xi_{i, \tau'})$ to $L_i$, and returns $\xi_{i, \tau'}$.
  \item[Bilinear pairing:] On input $(\xi_{1, j}, \xi_{2, j'})$ the challenger
    sets $\tau' \gets \tau_T + 1$ and computes
    $r_{\tau'}(X_1, \ldots, X_n) \gets p_{i, j}(X_1, \ldots, X_n) \cdot p_{i,
      j'}(X_1, \ldots, X_n)$. If $r_{\tau'}(X_1, \ldots, X_n) \in L_T$, $\cdv$
    returns encoding found in the list $L_T$. Else it picks a new unused random
    string and set $\xi_{T, \tau'}$ to it. Finally it returns the encoding to
    the algorithm.
\end{description}
  
After at most $q$ queries to the oracle, the adversary returns a bit $b'$. At
that point the challenger $\cdv$ chooses randomly $x_1, \ldots, x_n, y_1 \ldots, y_m$,
random bit $b$, and sets $X_i = x_i$, for $i \in \range{1}{n}$, and $Y_i = y_i$,
for $i \in \range{1}{m}$; furthermore, $\p{F}_b \gets \p{F}(x_1, \ldots, x_n)$
and $\p{F}_{1 - b} \gets (y_1, \ldots, y_m)$. Note that $\cdv$ simulates
perfectly unless the chosen values $x_1, \ldots, x_n, y_1, \ldots, y_m$ result
in equalities between polynomial evaluations that are not equalities between the
polynomials. That is, the simulation is perfect unless for some $i, j, j'$ holds
\[
  p_{i, j}(x_1, \ldots, x_n) - p_{i, j'}(x_1, \ldots, x_n) = 0,
  \]
  for $p_{i, j}(X_1, \ldots, X_n) \neq p_{i, j'}(X_1, \ldots, X_n)$.  Denote by
  $\bad$ an event that at least one of the three conditions holds. When $\bad$
  happens, the answer $\cdv$ gives to $\adv$ differs from an answer that a real
  oracle would give. We bound the probability that $\bad$ occurs in two steps.

  First we set $\p{F}_b = \p{F}(X_1, \ldots, X_n)$. Note that symbolic
  substitutions do not introduce any new equalities in $\GRP_1$. That is, if for
  all $j, j'$ holds $p_{1, j} \neq p_{1, j'}$, then $p_{1, j} \neq p_{1, j'}$
  even after setting $\p{F}_b = \p{F}(X_1, \ldots, X_n)$. This follows since all
  polynomials in $\p{F}$ are pairwise independent and $\p{F}$ independent on
  $\p{P}_1, \p{P}_2, \p{P}_T$. Indeed, $p_{1, j} - p_{1, j'}$ is a polynomial of
  the form
  \[
    \sum_{j = 1}^{m_1}a_j p_{1, j} + \sum_{j = 1}^{m} b_j f_j (X_1, \ldots, X_n),
  \]
  for some constants $a_j, b_j$. If the polynomial is non-zero, but setting
  $\p{F}_b = \p{F}(X_1, \ldots, X_n)$ makes this polynomial vanish, then some
  $f_k$ must be dependent on some $\p{P}_1, \p{F} \setminus \smallset{f_k}$.

  Now we set $X_1 \ldots, X_n, \p{F}_{1 - b}$ and bound probability that for
  some $i$ and $j, j'$ holds
  $(p_{i, j}(x_1, \ldots, x_n) - p_{i, j'}(x_1, \ldots, x_n) = 0$ for
  $p_{i, j} \neq p_{i, j'}$. By the construction, the maximum total degree of
  these polynomials is
  $d = \max(d_{\p{P}_1}+ d_{\p{P}_2}, d_{\p{P}_T}, d_{\p{F}})$, where $d_f$ is
  the total degree of some polynomial $f$ and for a set of polynomials
  $F = \smallset{f_1, \ldots, f_k}$, we write
  $d_F = \smallset{d_{f_1}, \ldots, d_{f_k}}$. Thus, for a given $j, j'$ probability that a random assignment to
  $X_1, \ldots, X_n, Y_1, \ldots, Y_n$ is a root of $p_{i, j} - p_{i, j'}$ is,
  by the Schwartz-Zippel lemma, bounded by $\infrac{d}{p}$, which is
  negligible. There is at most $2 \cdot {q + m_0 + m_1 + m  \choose 2}$ such
  pairs $p_{i, j}, p_{i, j'}$ we have that
  \[
    \prob{\bad} \leq  {q + m_0 + m_1 + m  \choose 2} \cdot \frac{2d}{p} \leq (q
    + m_0 + m_1 + m)^2 \frac{d}{p}.
  \]

  As noted, if $\bad$ does not occur then the simulation is perfect. Also the
  bit $b$ has been chosen independently on the $\adv$'s view, thus $\condprob{b
    = b'}{\neg \bad} = \infrac{1}{2}$. Hence,
  \[
    \begin{aligned}
      \prob{b = b'} & \leq \condprob{b = b'}{\neg \bad}(1 - \prob{\bad}) + \prob{\bad} =
      \frac{1}{2} + \frac{\prob{\bad}}{2} \\
      \prob{b = b'} & \geq \condprob{b = b'}{\neq \bad}(1 - \prob{\bad}) =
      \frac{1}{2} - \frac{\prob{\bad}}{2}.
    \end{aligned}
  \]
  Finally,
  \[
    \abs{\Pr[b = b'] - \frac{1}{2}} \leq \prob{\bad}/2 \leq (q
    + m_0 + m_1 + m)^2 \frac{d}{2p}
  \]
  as required.
\end{proof}

%\end{document}
  
\section{(Tight) simulation soundness of $\plonk$}
\begin{theorem}[Simulation soundness]
  Assume that $(\noofc + 2, 1)$-$\dlog$ is
  $\eps_\dlog(\secpar)$-hard, $\plonkprot$ is sound and $\ur{2}$ with security
  $\epss(\secpar)$ and $\epsur(\secpar)$ respectively. Then the probability that
  an algebraic, $\ppt$ adversary $\advss$ breaks simulation soundness of
  $\plonkprotfs$ is upper-bounded by
  \[
    \epsur(\secpar) + q_\ro^6 (\eps_{\dlog}(\secpar) + \epss(\secpar))\,,
  \]
  where $q_\ro$ is the total number of queries required by the adversary
  $\advss$.
\end{theorem}
\begin{proof}
  We proceed by contradiction. Suppose there exists a $\ppt$ adversary $\advss$
  that breaks simulation soundness with non-negligible probability
  \[
    \eps := \Pr \left[
      \begin{aligned}
        & \plonkprotfs.\verifier(\REL, \srs, \inp, \zkproof_{\advss}),\\
        & (\inp_{\advss}, \zkproof_{\advss}) \not\in Q,\\
        & \inp_\advss \not\in \LANG_\REL
      \end{aligned}
      \,\left|\, \vphantom{\begin{aligned}
            & \plonkprotfs.\verifier(\REL, \srs, \inp, \zkproof_{\advss}),\\
            & (\inp_{\advss}, \zkproof_{\advss}) \not\in Q,\\
            & \inp_\advss \not\in \LANG_\REL
          \end{aligned}}
        \begin{aligned}
          & \srs \gets \plonkprotfs.\kgen(\REL, \secparam)\\
          & (\inp_{\advss}, \zkproof_{\advss}) \gets \advss^{\simulator, \ro}
          (\REL, \srs),
        \end{aligned}
      \right.\right].
  \]

  In such case, we are able to build reductions $\rdvs$, $\rdvur$, $\rdvdlog$
  which using $\advss$ as a black-box, violate either the soundness, unique
  response properties of the underlying interactive protocol $\plonkprot$, or
  the $(\noofc + 2, 1)$-$\dlog$ assumption.

  In the following we denote by $\zkproof_{\advss}, \zkproof_{\simulator}$
  proofs returned by the adversary and the simulator respectively. We use
  $\zkproof[i]$ to denote prover's message in the $i$-th round of the proof,
  $\zkproof[i].\ch$ to denote the challenge that is given to the prover after
  $\zkproof[i]$, and $\zkproof\range{i}{j}$ to denote all messages of the proof
  including challenges between rounds $i$ and $j$.

  Without loss of generality, we assume that whenever the accepting proof
  contains a response to a challenge from a random oracle, we assume that the
  adversary queried the oracle to get it. It is straightforward to transform any
  adversary that violates this condition into an adversary that makes these
  additional queries to the random oracle and wins with the same probability.

  A crucial observation is that the adversary $\advss$ may have learned
  $\zkproof_{\advss}\range{1}{3}$ by querying the simulator on $\inp_{\advss}$
  or might have computed it itself. We denote the first event by $\event{E}$ and
  the second by $\nevent{E}$.
	%
  Additionally, we divide event $\nevent{E}$ into two disjunctive subevents:
  $\nevent{E}_0$ and $\nevent{E}_1$. Event $\nevent{E}_0$ considers a case when
  the final proof provided by the adversary $\advss$ is accepted by the
  idealised verification equation, i.e.~for that proof $\vereq_\zkproof(X) = 0$.
  Alternatively, event $\nevent{E}_1$ covers a case when for $\zkproof_\advss$
  it holds that $\vereq_\zkproof(\chi) = 0$, but $\vereq_\zkproof(X) \neq 0$, where $\chi$ is
  $\plonkprotfs$'s trapdoor.
	%
  As all these events are mutually exclusive and exhaustive, we have
  \[
    \eps = \prob{\advss \text{ wins}} = \prob{\advss \text{ wins}, \event{E}} +
    \prob{\advss \text{ wins}, \nevent{E}_0} + \prob{\advss \text{ wins},
      \nevent{E}_1}\,.
  \]

  Before analysing the events, we make the following observation. First of all,
  we allow reductions $\rdv_\dlog, \rdvur, \rdvs$ to simulate the random oracle
  and simulator for the adversary $\advss$. We argue that since the reductions
  in their simulation behaves as real random oracle or simulator would, the
  chances the adversary breaks simulation soundness does not change.

  Furthermore, we note that since $\advss$ is algebraic, it outputs a proof
  $\zkproof_\advss$ that can be written as
  \[
    \zkproof_\advss = \vec{M} \cdot (\underbrace{1 \| \chi \| \ldots \|
      \chi^{\noofc + 2}}_{\srs} \|
    \vec{\tilde{\zkproof}_{\simulator}}_1^{\top} \| \ldots \|
    \vec{\tilde{\zkproof}_{\simulator}}_{q_{\simulator}}^\top)^\top\,,
  \]
  where $\gone{1, \chi, \ldots, \chi^{\noofc + 2}}$ are all
  $\GRP_1$-elements from the SRS of $\plonkprotfs$, $\vec{M}$ is a matrix of
  coefficients output by $\advss$ aside the proof,
  $\vec{\tilde{\zkproof}_{\simulator}}_i$ denote all $\GRP_1$-elements from the
  simulated proof ${\zkproof_{\simulator}}_i$, and the adversary makes
  $q_\simulator$ queries to the simulator.  Since the reduction itself provides
  the simulated proofs, it knows a matrix $\vec{M'}$ such that
  \begin{equation}
    \label{eq:M_prim}
    \zkproof_\advss = \vec{M'} \cdot (1 \| \chi \| \ldots \| \chi^{\noofc + 2})^\top\,.
  \end{equation}
  We use this property when analysing the success probability of reductions
  $\rdvs$ and $\rdvdlog$.

  Also note that a proof $\zkproof$ could be accepted only if the verification
  equation $\vereq_\zkproof(\chi)$ holds. That is, the verifier plugs-in elements of
  $\zkproof$ into $\vereq_\zkproof(\chi)$ and checks whether it equals $0$. That is what
  is called a \emph{real check} in \cite{EPRINT:GabWilCio19}.  On the other hand
  there is an \emph{idealised check}, which verifies whether $\vereq_\zkproof(X) = 0$
  \emph{as a polynomial}---with proof elements being polynomials as well.

  \ncase{When $\event{E}$ happens} We assume that $\inp_{\advss}$ is submitted
  to the simulator $\simulator$.  We show how $\rdvur$ utilizes $\advss$, that
  makes use of $\inp_\advss, \zkproof_{\advss}\range{1}{3}$, to break the
  $\ur{2}$ property of $\plonkprot$.  This way we bound the probability
  $\prob{\adv \text{ wins}, \event{E}}$ by the probability of $\rdvur$ being
  able to win in the $\ur{2}$ game.

  Consider an algorithm $\rdvur$ that runs $\advss$ internally as a black-box:
  \begin{itemize}
  \item The reduction answers both queries to the simulator
    $\plonkprotfs.\simulator$ and to the random oracle. It also keeps lists $Q$,
    for the simulated proofs, and $Q_\ro$ for the random oracle queries.
  \item When $\advss$ outputs a fake proof $\zkproof_{\advss}$ for
    $\inp_\advss$, $\rdvur$ looks through lists $Q$ and $Q_\ro$ until it finds
    $\zkproof_{\simulator}\range{1}{3}$ such that
    $\zkproof_{\advss}\range{1}{2} = \zkproof_{\simulator}\range{1}{3}$ and a
    random oracle query $\zkproof_{\simulator}[3].\ch$ on
    $\zkproof_{\simulator}\range{1}{3}$.
  \item $\rdvur$ returns two proofs for $\inp_\advss$:
    \begin{align*}
      \zkproof_1 = (\zkproof_{\simulator}\range{1}{3}, \zkproof_{\simulator}[3].\ch, \zkproof_{\simulator}\range{4}{5})\\
      \zkproof_2 = (\zkproof_{\simulator}\range{1}{3}, \zkproof_{\simulator}[3].\ch, \zkproof_{\advss}\range{4}{5})
    \end{align*}
  \end{itemize}
  If $\zkproof_1 = \zkproof_2$, then $\advss$ fails to break special simulation
  extractability, as $\zkproof_2 \in Q$.  On the other hand, if the proofs are
  not equal, then $\rdvur$ breaks $\ur{2}$-ness of $\plonkprot$. Thus
  \[
    \prob{\advss \text{ wins}, \event{E}} \leq \epsur(\secpar).
  \]

  \ncase{When $\nevent{E}_0$ happens} In this case the reduction $\rdvs$ uses
  $\advss$ to break soundness of $\plonkprot$ with probability
  $\epss / q_{\ro}^6$, where $q_\ro$ is the number of total random oracle
  queries performed by the adversary or by $\rdvs$ on behalf of the
  simulator. As previously, $\rdvs$ runs $\advss$ internally and simulates its
  environment by answering to its queries to $\plonkprotfs.\simulator$ and
  $\ro$. The reduction works as follows:
  \begin{itemize}
  \item It guesses indices $i_1, \ldots, i_6$ such that random oracle queries
    $h_{i_1}, \ldots, h_{i_6}$ are the queries used in $\zkproof_\advss$. This
    is done with probability at least $1/q_{\ro}^6$ (since there are $6$
    challenges from the verifier in $\plonkprot$).
  \item On input $h$ for the $i$-th,
    $i \not\in \smallset{{i_1}, \ldots, {i_6}}$, random oracle query, $\rdvs$
    returns randomly picked $ y$, sets $\ro(h) = y $ and stores $(h, y)$ in
    $Q_\ro$ if $h$ is sent to $\ro$ the first time. If that is not the case,
    $\rdv$ finds $h$ in $Q_\ro$ and returns the corresponding $y$.
  \item On input $h_{i_j}$ for the $i_j$-th,
    $i_j \in \smallset{{i_1}, \ldots, {i_6}}$, random oracle query, $\rdvs$
    parses $h_{i_j}$ as a partial proof transcript $\zkproof[1..j]$ and runs
    $\plonkprot$ using $\zkproof[j]$ as a $\plonkprot.\prover$'s $j$-th message
    to $\plonkprot.\verifier$. The verifier responds with a challenge
    $\zkproof[j].\ch$. The reduction sets $\ro(h_{i_j}) = \zkproof[j].\ch$.
  \item On query $\inp_\simulator$ to $\simulator$ it runs a simulator
    $\plonkprotfs.\simulator$ internally and returns $\zkproof_\simulator$. If
    the random oracle query with input $\zkproof_\simulator[j]$,
    $1 \leq j \leq 2$, of the simulator is the $i_j$-th query, generate
    $\zkproof_\simulator[j].\ch$ by invoking $\plonkprot.\verifier$ on
    $\zkproof_\simulator[j]$ and programming
    $\ro(h_{i_j}) = \zkproof_\simulator[j].\ch$.
  \item Answers $\plonkprot.\verifier$'s challenge $\zkproof[j].\ch$ using the
    answer given by $\advss$, i.e.~$\zkproof_\advss[j + 1]$.
  \end{itemize}

  Assume that the $\plonkprot.\verifier$ accepts $\zkproof_\advss$. We consider
  a case when the idealised verification equation accepts. (Thus, the real
  verification accepts as well.) In that case $\rdvs$ extracts from $\vec{M'}$
  coefficients of $1, \chi, \ldots, \chi^{\noofc + 2}$ for
  polynomials $\p{a}(X), \p{b}(X)$, and $\p{c}(X)$ and reveals the witness
  $\wit_\advss$ (as it is encoded in theses polynomials' coefficients). If
  $\REL(\inp_\advss, \wit_\advss)$ holds then $\advss$ failed to break
  simulation-soundness of $\plonkprotfs$. On the other hand, if that is not the
  case, then $\rdvs$ breaks soundness of $\plonkprot$.
	%
  Since the reduction guesses queries $h_{i_1}, \ldots, h_{i_6}$ with
  probability $1/q_\ro^6$, then
  \[
    \prob{\rdvs \text{ wins}} = \prob{\advss \text{ wins}, h_{i_1}, \ldots,
      h_{i_6} \text{ are guessed correctly}, \nevent{E}_0}\,.
  \]
  Hence,
  \[
    \prob{\advss \text{ wins}, \nevent{E}_0} \leq q_\ro^6 \cdot \epsss(\secpar).
  \]

  \ncase{When $\nevent{E}_1$ happens} The reduction $\rdvdlog$ runs internally a
  protocol $\plonkprotfs$, which SRS is computed from the challenge
  $\gone{1, \chi, \ldots, \chi^{\noofc + 2}}, \gtwo{\chi}$ from the
  $(\noofc + 2, 1)$-$\dlog$ assumption challenger. Then it proceeds
  as $\rdvs$ does, except in the last part, when the adversary provided its
  proof $\zkproof_\advss$, $\rdvdlog$ uses the fact that the real verification
  equation holds, but the ideal verification equation does not to break the
  $\dlog$ assumption.

  Since $\vereq_\zkproof(X) \neq 0$, but $\vereq_\zkproof(\chi) = 0$ and $\rdvdlog$ knows
  $\vec{M'}$, as defined in \cref{eq:M_prim}, it can recreate all the
  polynomials submitted by $\advss$ as part of the proof and included in
  $\vereq_\zkproof(X)$. This way, it knows all coefficients of $\vereq_\zkproof(X)$. Thus it can
  factorize it and find its roots, one of them is the required $\chi$. Hence it
  holds, by the analogous analysis as in the previous case, that
  \[
    \prob{\advss \text{ wins}, \nevent{E}_1} \leq q_\ro^6 \cdot
    \eps_{\dlog}(\secpar).
  \]

  The proof is concluded by observing that the analysis of events
  $\event{E}, \nevent{E}_0, \nevent{E}_1$ gives
  \[
    \eps \leq \epsur(\secpar) + q_\ro^6 (\eps_{\dlog}(\secpar) +
    \epsss(\secpar))\,,
  \]
  hence $\eps$ is negligible if $\dlog$ is hard and $\plonkprot$ is sound and
  $\ur{2}$.  \qed
\end{proof}

\begin{theorem}[Simulation soundness]
  Assume that $\ps$ is sound and $\ur{k}$ with security $\epss(\secpar)$ and
  $\epsur(\secpar)$ respectively. Then the probability that a $\ppt$ adversary
  $\adv$ breaks simulation soundness of $\ps_{\fs}$ is upper-bounded by
  \[
    \epsur(\secpar) + q_\ro^\mu  \epss(\secpar)\,,
  \]
  where $q_\ro$ is the total number of queries made by the adversary
  $\adv$.
\end{theorem}

\begin{proof}
    The proof goes by game hoping. The games are controlled by an environment
  $\env$ that internally runs a simulation soundness adversary $\adv$,
  provides it with access to a random oracle and simulator, and when necessary
  rewinds it. The games differ by various breaking points, i.e.~points where the
  environment decides to abort the game.

  Denote by $\zkproof_{\adv}, \zkproof_{\simulator}$ proofs returned by the
  adversary and the simulator respectively. We use $\zkproof[i]$ to denote
  prover's message in the $i$-th round of the proof (counting from 1), i.e.~$(2i
  - 1)$-th message exchanged in the protocol. We use $\zkproof[i].\ch$ to denote the
  challenge that is given to the prover after $\zkproof[i]$, and
  $\zkproof[i..j]$ to denote all messages of the proof including challenges
  between rounds $i$ and $j$, but not challenge $\zkproof[j].\ch$. When it is
  not explicitly stated we denote the proven instance $\inp$ by $\zkproof[0]$
  (however, there is no challenge $\zkproof[0].\ch$).

  Without loss of generality, we assume that whenever the accepting proof
  contains a response to a challenge from a random oracle, then the
  adversary queried the oracle to get it. It is straightforward to transform any
  adversary that violates this condition into an adversary that makes these
  additional queries to the random oracle and wins with the same probability.

  \ngame{0} This is a simulation soundness game played between an adversary
  $\advss$ who has given access to a random oracle $\ro$ and simulator
  $\psfs.\simulator$. $\advss$ wins if it manages to produce an acceptable proof
  for a false statement. In the following game hops we upper-bound the
  probability that this happens.

  \ngame{1} This is identical to $\game{0}$ except that now the game is aborted
  if there is a simulated proof $\zkproof_\simulator$ for $\inp_{\adv}$ such
  that $(\inp_{\adv}, \zkproof_\simulator[1..k]) = (\inp_{\adv},
  \zkproof_{\adv}[1..k])$. That is, the adversary in its final proof
  reuses a part of a simulated proof it saw before and the proof is acceptable.
  Denote that event by $\event{\errur}$.

  \ncase{$\game{0} \mapsto \game{1}$} We have, \( \prob{\game{0} \land
    \nevent{\errur}} = \prob{\game{1} \land \nevent{\errur}} \) and, from the
  difference lemma, cf.~\cref{lem:difference_lemma},
  \[ \abs{\prob{\game{0}} - \prob{\game{1}}} \leq \prob{\event{\errur}}\,. \]
  Thus, to show that the transition from one game to another introduces only
  minor change in probability of $\adv$ winning it should be shown that
  $\prob{\event{\errur}}$ is small.

  We can assume that $\adv$ queried the simulator on the instance it wishes to
  output---$\inp_{\adv}$. We show a reduction $\rdvur$ that utilises $\adv$,
  who outputs a valid proof for $\inp_{\adv}$, to break the $\ur{k}$ property of
  $\ps$. Let $\rdvur$ run $\advse$ internally as a black-box:
  \begin{itemize}
	\item The reduction answers both queries to the simulator $\psfs.\simulator$
    and to the random oracle.  It also keeps lists $Q$, for the simulated
    proofs, and $Q_\ro$ for the random oracle queries.
  \item When $\adv$ makes a fake proof $\zkproof_{\adv}$ for $\inp_{\adv}$,
    $\rdvur$ looks through lists $Q$ and $Q_\ro$ until it finds
    $\zkproof_{\simulator}[0..k]$ such that
    $\zkproof_{\adv}[0..k] = \zkproof_{\simulator}[0..k]$ and a random oracle
    query $\zkproof_{\simulator}[k].\ch$ on $\zkproof_{\simulator}[0..k]$.
	\item $\rdvur$ returns two proofs for $\inp_{\adv}$:
	\begin{align*}
		\zkproof_1 = (\zkproof_{\simulator}[1..k],
		\zkproof_{\simulator}[k].\ch, \zkproof_{\simulator}[k + 1..\mu + 1])\\
		\zkproof_2 = (\zkproof_{\simulator}[1..k],
		\zkproof_{\simulator}[k].\ch, \zkproof_{\adv}[k + 1..\mu + 1])
	\end{align*}
	\end{itemize}  
	If $\zkproof_1 = \zkproof_2$, then $\adv$ fails to break simulation soundness,
  as $\zkproof_2 \in Q$. On the other hand, if the proofs are not equal, then
  $\rdvur$ breaks $\ur{k}$-ness of $\ps$. This happens only with negligible
  probability $\epsur$, hence \( \prob{\event{\errur}} \leq \epsur\,. \)
	
  \ngame{2} This is identical to $\game{1}$ except that now the environment
  aborts if the instance the adversary proves is not in the language.

  \ncase{$\game{1} \mapsto \game{2}$} 
  % STARTED REDUCTION TO NI SOUNDNESS:
  % We show that
  % $\abs{\prob{\game{1}} - \prob{\game{2}}} \leq \epss$. Consider a simulation
  % soundness adversary $\advss$ that does not break unique response property, yet it gives
  % an accepting proof for an invalid statement. We show that such an adversary
  % could be used to break soundness of $\ps$. Really, consider a soundness
  % adversary $\adv$. Since $\ps$ is HVZK $\adv$ can internally simulate $\advss$
  % and provide it with simulated proofs as it wishes. 
  %
  % REDUCTION TO INTERACTIVE SOUNDNESS:
  We show that
  $\abs{\prob{\game{1}} - \prob{\game{2}}} \leq q_{\ro}^{\mu} \cdot \epss(\secpar)$,
  where $\epss(\secpar)$ is the probability of breaking soundness of the underlying
  \emph{interactive} protocol $\ps$. Note that
  $\abs{\prob{\game{1}} - \prob{\game{2}}}$ is the probability that $\adv$
  outputs an acceptable proof for a false statement, which does not break the
  unique response property (proofs breaking that property have been excluded by
  $\game{1}$). Consider a soundness adversary $\adv'$ who initiates a proof with
  $\ps$'s verifier $\ps.\verifier$, internally runs $\adv$ and proceeds as
  follows:
  \begin{itemize}
  \item It guesses indices $i_1, \ldots, i_\mu$ such that random oracle queries
    $h_{i_1}, \ldots, h_{i_\mu}$ are the queries used in the $\zkproof_\adv$
    proof eventually output by $\adv$. This is done with probability at least
    $1/q_{\ro}^\mu$ (since there are $\mu$ challenges from the verifier in
    $\ps$).
  \item On input $h$ for the $i$-th,
    $i \not\in \smallset{{i_1}, \ldots, {i_\mu}}$, random oracle query, $\adv'$
    returns randomly picked $y$, sets $\ro(h) = y $ and stores $(h, y)$ in
    $Q_\ro$ if $h$ is sent to $\ro$ the first time. If that is not the case,
    $\adv$ finds $h$ in $Q_\ro$ and returns the corresponding $y$.
  \item On input $h_{i_j}$ for the $i_j$-th,
    $i_j \in \smallset{{i_1}, \ldots, {i_\mu}}$, random oracle query, $\adv'$
    parses $h_{i_j}$ as a partial proof transcript $\zkproof[1..j]$ and runs
    $\ps$ using $\zkproof[j]$ as a $\ps.\prover$'s $j$-th message to
    $\ps.\verifier$. The verifier responds with a challenge
    $\zkproof[j].\ch$. $\adv'$ sets $\ro(h_{i_j}) = \zkproof[j].\ch$. If we guessed the indices correctly we have that $h_{i_{j'}}$, for $j' \leq j$, parsed as $\zkproof[1..j']$ is a prefix of $\zkproof[1..j]$.
  \item On query $\inp_\simulator$ to $\simulator$, $\adv'$ runs the simulator
    $\ps.\simulator$ internally. Note that we require a simulator that only programs the random oracle for $j \geq k$.

    If the simulator makes a previously unanswered random
    oracle query with input $\zkproof_\simulator[1..j]$, $1 \leq j < k$, and this is the $i_j$-th query, it generates $\zkproof_\simulator[j].\ch$ by
    invoking $\ps.\verifier$ on $\zkproof_\simulator[j]$ and programs
    $\ro(h_{i_j}) = \zkproof_\simulator[j].\ch$.
    It returns $\zkproof_\simulator$.
  \item Answers $\ps.\verifier$'s final challenge $\zkproof[k].\ch$ using the
    answer given by $\adv$, i.e.~$\zkproof_\adv[\mu]$.
  \end{itemize}
  That is, $\adv'$ manages to break soundness of $\ps$ if $\adv$ manages to
  break simulation soundness without breaking the unique response property and
  $\adv'$ correctly guesses the indices of $\adv$ random oracle queries. This
  happens with probability upper-bounded by $\abs{\prob{\game{1}} -
    \prob{\game{2}}} \cdot \infrac{1}{q_\ro^{\mu}}$. Hence $\abs{\prob{\game{1}} -
    \prob{\game{2}}} \leq q_\ro^{\mu} \cdot \epss(\secpar)$.

  Note that in $\game{2}$ the adversary cannot win. Thus the probability
  that $\advss$ is successful is upper-bounded by
  $\epsur(\secpar) + q_{\ro}^{\mu} \cdot \epss(\secpar)$.  \qed
\end{proof}

\end{document}

